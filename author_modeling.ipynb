{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Modeling Pipeline\n",
    "\n",
    "This notebook implements a 5-stage pipeline for modeling an author's characteristic writing patterns through few-shot example curation.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "Rather than extracting explicit style rules, this approach models the author's **decision-making patterns** and **sensibility**, then curates exemplary passages for tacit transmission via few-shot learning.\n",
    "\n",
    "## Pipeline Stages\n",
    "\n",
    "1. **Stage 1: Analytical Mining** - Analyze sample texts through three lenses:\n",
    "   - Implied Author: Sensibility and stance\n",
    "   - Decision Patterns: Compositional choices\n",
    "   - Functional Texture: How surface features serve purpose\n",
    "\n",
    "2. **Stage 2: Cross-Text Synthesis** - Synthesize each dimension across multiple samples to identify stable patterns\n",
    "\n",
    "3. **Stage 3: Field Guide Construction** - Integrate all three syntheses into unified recognition criteria and density rubric\n",
    "\n",
    "4. **Stage 4: Passage Evaluation** - Evaluate passages from corpus using field guide (1-5 density rating)\n",
    "\n",
    "5. **Stage 5: Example Set Construction** - Curate 3-4 high-density passages that balance density, diversity, and complementarity\n",
    "\n",
    "## Note\n",
    "\n",
    "This pipeline is author-agnostic. The Russell corpus is used as a test case, but the same approach applies to any author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm==1.79.3 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.79.3)\n",
      "Requirement already satisfied: pydantic==2.7.4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (2.7.4)\n",
      "Requirement already satisfied: jinja2>=3.1.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (3.10.11)\n",
      "Requirement already satisfied: click in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.27.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.5.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.99.5 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.9.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (4.13.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jinja2>=3.1.0->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (4.0.2)\n",
      "Requirement already satisfied: anyio in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: certifi in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: sniffio in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=6.8.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.20.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (5.10.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (2023.12.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.20.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (0.9.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (4.64.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2022.4.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.27.1)\n",
      "Requirement already satisfied: exceptiongroup in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from anyio->httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.0.12)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (0.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from belletrist import (\n",
    "    LLM,\n",
    "    LLMConfig,\n",
    "    PromptMaker,\n",
    "    DataSampler,\n",
    "    ResultStore,\n",
    "    # Stage 1 configs\n",
    "    ImpliedAuthorConfig,\n",
    "    DecisionPatternConfig,\n",
    "    FunctionalTextureConfig,\n",
    "    # Stage 2 configs\n",
    "    ImpliedAuthorSynthesisConfig,\n",
    "    DecisionPatternSynthesisConfig,\n",
    "    TexturalSynthesisConfig,\n",
    "    # Stage 3 config\n",
    "    FieldGuideConstructionConfig,\n",
    "    # Stage 4 config\n",
    "    PassageEvaluationConfig,\n",
    "    # Stage 5 config\n",
    "    ExampleSetConstructionConfig,\n",
    "    # Utilities\n",
    "    extract_paragraph_windows,\n",
    "    extract_logical_sections,\n",
    "    get_full_sample_as_passage,\n",
    "    parse_passage_evaluation,\n",
    "    parse_example_set_selection,\n",
    "    validate_passage_evaluation,\n",
    "    validate_example_set_selection,\n",
    ")\n",
    "\n",
    "print(\"Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Base Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm_config = LLMConfig(\n",
    "    model=\"mistral/mistral-large-2411\",  # Change as needed\n",
    "    api_key=os.environ.get('MISTRAL_API_KEY'),\n",
    "    temperature=0.7\n",
    ")\n",
    "llm = LLM(llm_config)\n",
    "\n",
    "# Initialize prompt maker\n",
    "prompt_maker = PromptMaker()\n",
    "\n",
    "# Initialize data sampler (Russell corpus)\n",
    "data_dir = Path(\"data/russell\")\n",
    "sampler = DataSampler(data_dir)\n",
    "\n",
    "# Initialize result store\n",
    "store = ResultStore(\"russell_author_modeling.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.reset('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration: Select Samples for Analysis\n",
    "\n",
    "For Stages 1-3, we'll analyze 3-5 samples to extract stable cross-text patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will analyze 3 samples through Stages 1-3\n"
     ]
    }
   ],
   "source": [
    "# Select samples for Stage 1-3 analysis\n",
    "# These should be diverse, substantial passages (500-800 words recommended)\n",
    "ANALYSIS_SAMPLES = [\n",
    "    \"sample_001\",\n",
    "    \"sample_002\",\n",
    "    \"sample_003\",\n",
    "    # Add more as needed for richer cross-text synthesis\n",
    "]\n",
    "\n",
    "# For this demo, we'll generate samples if they don't exist\n",
    "# In production, you might use specific pre-selected samples\n",
    "NUM_SAMPLES = 3\n",
    "SAMPLE_PARAGRAPH_LENGTH = 5  # ~500-800 words\n",
    "\n",
    "print(f\"Will analyze {NUM_SAMPLES} samples through Stages 1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stage 1: Analytical Mining\n",
    "\n",
    "Run three specialized analyses on each sample:\n",
    "- **Implied Author**: What sensibility emerges from the prose?\n",
    "- **Decision Patterns**: What compositional choices are made at key junctures?\n",
    "- **Functional Texture**: How do surface features serve purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate or Load Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ sample_001 already gathered\n",
      "✓ sample_002 already gathered\n",
      "✓ sample_003 already gathered\n",
      "\n",
      "Sample 001 preview (first 600 chars):\n",
      "It must be remembered that even the \"industrious and conscientious\n",
      "worker\" receives less food than is required to maintain efficiency.\n",
      "\n",
      "Over the whole development of Russia and of Bolshevism since the\n",
      "October revolution there broods a tragic fatality. In spite of outward\n",
      "success the inner failure has proceeded by inevitable stages--stages\n",
      "which could, by sufficient acumen, have been foreseen from the first.\n",
      "By provoking the hostility of the outside world the Bolsheviks were\n",
      "forced to provoke the hostility of the peasants, and finally the\n",
      "hostility or utter apathy of the urban and industrial po...\n"
     ]
    }
   ],
   "source": [
    "# Generate samples if they don't exist\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    \n",
    "    # Skip if already saved\n",
    "    if store.get_sample(sample_id):\n",
    "        print(f\"✓ {sample_id} already gathered\")\n",
    "        continue\n",
    "        \n",
    "    # Generate new sample\n",
    "    segment = sampler.sample_segment(p_length=SAMPLE_PARAGRAPH_LENGTH)\n",
    "    store.save_segment(sample_id, segment)\n",
    "    print(f\"Generated {sample_id}\")\n",
    "\n",
    "# Display first sample for reference\n",
    "sample = store.get_sample(\"sample_001\")\n",
    "print(f\"\\nSample 001 preview (first 600 chars):\\n{sample['text'][:600]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stage 1A: Implied Author Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze each sample for implied author\nfor i in range(NUM_SAMPLES):\n    sample_id = f\"sample_{i+1:03d}\"\n    analyst_name = ImpliedAuthorConfig.analyst_name()\n    \n    # Check if analysis already exists (resume support)\n    if store.get_analysis(sample_id, analyst_name):\n        print(f\"{sample_id}: {analyst_name} analysis already complete\")\n        continue\n    \n    # Get sample text\n    sample = store.get_sample(sample_id)\n    \n    # Generate prompt\n    config = ImpliedAuthorConfig(text=sample['text'])\n    prompt = prompt_maker.render(config)\n    \n    # Run LLM\n    print(f\"{sample_id}: Running {analyst_name} analysis...\")\n    response = llm.complete(prompt)\n    \n    # Save analysis\n    store.save_analysis(\n        sample_id=sample_id,\n        analyst=analyst_name,\n        output=response.content,\n        model=response.model\n    )\n    print(f\"{sample_id}: {analyst_name} analysis complete\")\n\nprint(\"\\nStage 1A complete: All samples analyzed for implied author\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stage 1B: Decision Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze each sample for decision patterns\nfor i in range(NUM_SAMPLES):\n    sample_id = f\"sample_{i+1:03d}\"\n    analyst_name = DecisionPatternConfig.analyst_name()\n    \n    if store.get_analysis(sample_id, analyst_name):\n        print(f\"{sample_id}: {analyst_name} analysis already complete\")\n        continue\n    \n    sample = store.get_sample(sample_id)\n    config = DecisionPatternConfig(text=sample['text'])\n    prompt = prompt_maker.render(config)\n    \n    print(f\"{sample_id}: Running {analyst_name} analysis...\")\n    response = llm.complete(prompt)\n    \n    store.save_analysis(\n        sample_id=sample_id,\n        analyst=analyst_name,\n        output=response.content,\n        model=response.model\n    )\n    print(f\"{sample_id}: {analyst_name} analysis complete\")\n\nprint(\"\\nStage 1B complete: All samples analyzed for decision patterns\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stage 1C: Functional Texture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze each sample for functional texture\nfor i in range(NUM_SAMPLES):\n    sample_id = f\"sample_{i+1:03d}\"\n    analyst_name = FunctionalTextureConfig.analyst_name()\n    \n    if store.get_analysis(sample_id, analyst_name):\n        print(f\"{sample_id}: {analyst_name} analysis already complete\")\n        continue\n    \n    sample = store.get_sample(sample_id)\n    config = FunctionalTextureConfig(text=sample['text'])\n    prompt = prompt_maker.render(config)\n    \n    print(f\"{sample_id}: Running {analyst_name} analysis...\")\n    response = llm.complete(prompt)\n    \n    store.save_analysis(\n        sample_id=sample_id,\n        analyst=analyst_name,\n        output=response.content,\n        model=response.model\n    )\n    print(f\"{sample_id}: {analyst_name} analysis complete\")\n\nprint(\"\\nStage 1C complete: All samples analyzed for functional texture\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Stage 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_001: 3 analyses complete\n",
      "  - decision_pattern\n",
      "  - functional_texture\n",
      "  - implied_author\n",
      "sample_002: 3 analyses complete\n",
      "  - decision_pattern\n",
      "  - functional_texture\n",
      "  - implied_author\n",
      "sample_003: 3 analyses complete\n",
      "  - decision_pattern\n",
      "  - functional_texture\n",
      "  - implied_author\n",
      "\n",
      "--- Sample Implied Author Analysis (first 500 chars) ---\n",
      "## PART 1: DIMENSIONAL ANALYSIS\n",
      "\n",
      "### 1. RELATIONSHIP TO MATERIAL\n",
      "\n",
      "**Observation:**\n",
      "This author writes with a blend of mastery and critique, holding the subject matter with a critical distance while also showing a deep understanding of the historical and ideological complexities at play.\n",
      "\n",
      "**Evidence:**\n",
      "1. \"Over the whole development of Russia and of Bolshevism since the October revolution there broods a tragic fatality. In spite of outward success the inner failure has proceeded by inevitable sta...\n"
     ]
    }
   ],
   "source": [
    "# Display completion status\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "    print(f\"{sample_id}: {len(analyses)} analyses complete\")\n",
    "    for analyst_name in analyses.keys():\n",
    "        print(f\"  - {analyst_name}\")\n",
    "\n",
    "# Optionally display one analysis\n",
    "print(\"\\n--- Sample Implied Author Analysis (first 500 chars) ---\")\n",
    "sample, analyses = store.get_sample_with_analyses(\"sample_001\")\n",
    "implied_author = analyses.get(ImpliedAuthorConfig.analyst_name(), \"Not found\")\n",
    "print(implied_author[:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stage 2: Cross-Text Synthesis\n",
    "\n",
    "Synthesize each analytical dimension across samples to identify stable patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2A: Implied Author Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all implied author analyses\n",
    "implied_author_analyses = {}\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    analysis = store.get_analysis(sample_id, ImpliedAuthorConfig.analyst_name())\n",
    "    if analysis:\n",
    "        implied_author_analyses[sample_id] = analysis\n",
    "\n",
    "# Check if synthesis already exists\n",
    "synthesis_type = ImpliedAuthorSynthesisConfig.synthesis_type()\n",
    "existing = store.list_syntheses(synthesis_type)\n",
    "\n",
    "if existing:\n",
    "    print(f\"Implied author synthesis already exists: {existing[0]}\")\n",
    "    implied_author_synthesis_id = existing[0]\n",
    "else:\n",
    "    # Generate synthesis prompt\n",
    "    config = ImpliedAuthorSynthesisConfig(\n",
    "        implied_author_analyses=implied_author_analyses\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    # Run synthesis\n",
    "    print(\"Running implied author synthesis...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    # Save synthesis\n",
    "    sample_contributions = [\n",
    "        (sample_id, ImpliedAuthorConfig.analyst_name())\n",
    "        for sample_id in implied_author_analyses.keys()\n",
    "    ]\n",
    "    \n",
    "    implied_author_synthesis_id = store.save_synthesis(\n",
    "        synthesis_type=synthesis_type,\n",
    "        output=response.content,\n",
    "        model=response.model,\n",
    "        sample_contributions=sample_contributions,\n",
    "        config=config\n",
    "    )\n",
    "    print(f\"Synthesis saved: {implied_author_synthesis_id}\")\n",
    "\n",
    "# Display preview\n",
    "synth = store.get_synthesis(implied_author_synthesis_id)\n",
    "print(f\"\\nPreview (first 400 chars):\\n{synth['output'][:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2B: Decision Pattern Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all decision pattern analyses\n",
    "decision_pattern_analyses = {}\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    analysis = store.get_analysis(sample_id, DecisionPatternConfig.analyst_name())\n",
    "    if analysis:\n",
    "        decision_pattern_analyses[sample_id] = analysis\n",
    "\n",
    "# Check if synthesis already exists\n",
    "synthesis_type = DecisionPatternSynthesisConfig.synthesis_type()\n",
    "existing = store.list_syntheses(synthesis_type)\n",
    "\n",
    "if existing:\n",
    "    print(f\"Decision pattern synthesis already exists: {existing[0]}\")\n",
    "    decision_pattern_synthesis_id = existing[0]\n",
    "else:\n",
    "    config = DecisionPatternSynthesisConfig(\n",
    "        decision_pattern_analyses=decision_pattern_analyses\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    print(\"Running decision pattern synthesis...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    sample_contributions = [\n",
    "        (sample_id, DecisionPatternConfig.analyst_name())\n",
    "        for sample_id in decision_pattern_analyses.keys()\n",
    "    ]\n",
    "    \n",
    "    decision_pattern_synthesis_id = store.save_synthesis(\n",
    "        synthesis_type=synthesis_type,\n",
    "        output=response.content,\n",
    "        model=response.model,\n",
    "        sample_contributions=sample_contributions,\n",
    "        config=config\n",
    "    )\n",
    "    print(f\"Synthesis saved: {decision_pattern_synthesis_id}\")\n",
    "\n",
    "synth = store.get_synthesis(decision_pattern_synthesis_id)\n",
    "print(f\"\\nPreview (first 400 chars):\\n{synth['output'][:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2C: Textural Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all functional texture analyses\n",
    "textural_analyses = {}\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    analysis = store.get_analysis(sample_id, FunctionalTextureConfig.analyst_name())\n",
    "    if analysis:\n",
    "        textural_analyses[sample_id] = analysis\n",
    "\n",
    "# Check if synthesis already exists\n",
    "synthesis_type = TexturalSynthesisConfig.synthesis_type()\n",
    "existing = store.list_syntheses(synthesis_type)\n",
    "\n",
    "if existing:\n",
    "    print(f\"Textural synthesis already exists: {existing[0]}\")\n",
    "    textural_synthesis_id = existing[0]\n",
    "else:\n",
    "    config = TexturalSynthesisConfig(\n",
    "        textural_analyses=textural_analyses\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    print(\"Running textural synthesis...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    sample_contributions = [\n",
    "        (sample_id, FunctionalTextureConfig.analyst_name())\n",
    "        for sample_id in textural_analyses.keys()\n",
    "    ]\n",
    "    \n",
    "    textural_synthesis_id = store.save_synthesis(\n",
    "        synthesis_type=synthesis_type,\n",
    "        output=response.content,\n",
    "        model=response.model,\n",
    "        sample_contributions=sample_contributions,\n",
    "        config=config\n",
    "    )\n",
    "    print(f\"Synthesis saved: {textural_synthesis_id}\")\n",
    "\n",
    "synth = store.get_synthesis(textural_synthesis_id)\n",
    "print(f\"\\nPreview (first 400 chars):\\n{synth['output'][:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 3: Field Guide Construction\n",
    "\n",
    "Integrate all three Stage 2 syntheses into a unified recognition field guide with:\n",
    "- Unified sensibility description\n",
    "- Recognition criteria (questions for evaluation)\n",
    "- Density rubric (1-5 scale)\n",
    "- Master passage index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the three syntheses\n",
    "implied_synth = store.get_synthesis(implied_author_synthesis_id)\n",
    "decision_synth = store.get_synthesis(decision_pattern_synthesis_id)\n",
    "textural_synth = store.get_synthesis(textural_synthesis_id)\n",
    "\n",
    "# Check if field guide already exists\n",
    "synthesis_type = FieldGuideConstructionConfig.synthesis_type()\n",
    "existing = store.list_syntheses(synthesis_type)\n",
    "\n",
    "if existing:\n",
    "    print(f\"Field guide already exists: {existing[0]}\")\n",
    "    field_guide_id = existing[0]\n",
    "else:\n",
    "    # Generate field guide\n",
    "    config = FieldGuideConstructionConfig(\n",
    "        implied_author_synthesis=implied_synth['output'],\n",
    "        decision_pattern_synthesis=decision_synth['output'],\n",
    "        textural_synthesis=textural_synth['output']\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    print(\"Constructing field guide...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    # Save with parent linkage (no direct sample contributions)\n",
    "    field_guide_id = store.save_synthesis(\n",
    "        synthesis_type=synthesis_type,\n",
    "        output=response.content,\n",
    "        model=response.model,\n",
    "        sample_contributions=[],  # Inherits from parents\n",
    "        config=config,\n",
    "        parent_synthesis_id=implied_author_synthesis_id  # Link to one parent\n",
    "    )\n",
    "    print(f\"Field guide saved: {field_guide_id}\")\n",
    "\n",
    "# Display preview\n",
    "field_guide = store.get_synthesis(field_guide_id)\n",
    "print(f\"\\nField Guide Preview (first 500 chars):\\n{field_guide['output'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Corpus Mining Preparation\n",
    "\n",
    "Extract passages from the corpus for evaluation in Stage 4.\n",
    "\n",
    "We can use several strategies:\n",
    "- **Paragraph windows**: Overlapping sliding windows\n",
    "- **Logical sections**: Section-based extraction\n",
    "- **Full samples**: Evaluate entire samples\n",
    "- **Specific indices**: Passages flagged in Stage 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Extract paragraph windows from all samples\n",
    "passages_windows = extract_paragraph_windows(\n",
    "    store,\n",
    "    sample_ids=None,  # All samples\n",
    "    window_size=4,     # 4 paragraphs per window\n",
    "    overlap=2          # 2 paragraph overlap\n",
    ")\n",
    "\n",
    "print(f\"Extracted {len(passages_windows)} paragraph windows\")\n",
    "\n",
    "# Strategy 2: Extract logical sections\n",
    "passages_sections = extract_logical_sections(\n",
    "    store,\n",
    "    sample_ids=None,\n",
    "    min_length=200,  # Min 200 words\n",
    "    max_length=600   # Max 600 words\n",
    ")\n",
    "\n",
    "print(f\"Extracted {len(passages_sections)} logical sections\")\n",
    "\n",
    "# For this demo, we'll use paragraph windows\n",
    "# In production, you might combine strategies or use specific flagged passages\n",
    "passages_to_evaluate = passages_windows[:10]  # Limit for demo\n",
    "print(f\"\\nWill evaluate {len(passages_to_evaluate)} passages in Stage 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 4: Passage Evaluation\n",
    "\n",
    "Evaluate passages using the field guide to assign density ratings (1-5) and assess few-shot suitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get field guide for evaluation\n",
    "field_guide = store.get_synthesis(field_guide_id)\n",
    "field_guide_text = field_guide['output']\n",
    "\n",
    "# Evaluate each passage\n",
    "evaluation_results = []\n",
    "\n",
    "for idx, passage in enumerate(passages_to_evaluate):\n",
    "    print(f\"Evaluating passage {idx+1}/{len(passages_to_evaluate)}...\")\n",
    "    \n",
    "    # Create passage identifier\n",
    "    passage_id = f\"{passage['source_sample_id']}_para_{passage['paragraph_range']}\"\n",
    "    \n",
    "    # Check if evaluation already exists\n",
    "    # (In production, you'd check ResultStore for existing evaluations)\n",
    "    \n",
    "    # Generate evaluation prompt\n",
    "    config = PassageEvaluationConfig(\n",
    "        field_guide=field_guide_text,\n",
    "        source=passage_id,\n",
    "        passage=passage['text']\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    # Run evaluation\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    # Parse evaluation response\n",
    "    try:\n",
    "        parsed = parse_passage_evaluation(response.content)\n",
    "        \n",
    "        if validate_passage_evaluation(parsed):\n",
    "            # Save to ResultStore\n",
    "            eval_id = store.save_passage_evaluation(\n",
    "                sample_id=passage['source_sample_id'],\n",
    "                density_rating=parsed['density_rating'],\n",
    "                task_coverage=parsed.get('task_coverage', ''),\n",
    "                teaching_value=parsed.get('teaching_value', ''),\n",
    "                recommendation=parsed['recommendation'],\n",
    "                model=response.model,\n",
    "                field_guide_id=field_guide_id,\n",
    "                paragraph_range=passage['paragraph_range']\n",
    "            )\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                'eval_id': eval_id,\n",
    "                'passage_id': passage_id,\n",
    "                'rating': parsed['density_rating'],\n",
    "                'recommended': parsed['recommendation']\n",
    "            })\n",
    "            \n",
    "            print(f\"  → Rating: {parsed['density_rating']}, Recommended: {parsed['recommendation']}\")\n",
    "        else:\n",
    "            print(f\"  → Validation failed for passage {idx+1}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  → Error parsing evaluation: {e}\")\n",
    "\n",
    "print(f\"\\nStage 4 complete: Evaluated {len(evaluation_results)} passages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rating distribution\n",
    "rating_counts = {}\n",
    "for result in evaluation_results:\n",
    "    rating = result['rating']\n",
    "    rating_counts[rating] = rating_counts.get(rating, 0) + 1\n",
    "\n",
    "print(\"Rating Distribution:\")\n",
    "for rating in sorted(rating_counts.keys()):\n",
    "    print(f\"  Rating {rating}: {rating_counts[rating]} passages\")\n",
    "\n",
    "# Show high-density passages (4-5 rated)\n",
    "high_density = [r for r in evaluation_results if r['rating'] >= 4]\n",
    "print(f\"\\nHigh-density passages (4-5): {len(high_density)}\")\n",
    "for result in high_density:\n",
    "    print(f\"  {result['eval_id']}: Rating {result['rating']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stage 5: Example Set Construction\n",
    "\n",
    "Curate 3-4 high-density passages that balance:\n",
    "- **Density**: High ratings (4-5)\n",
    "- **Diversity**: Different compositional tasks/topics\n",
    "- **Complementarity**: Each adds something unique\n",
    "- **Coherence**: Together forming unified demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all passage evaluations for curation\n",
    "# (In production, query ResultStore for evaluations with ratings 4-5)\n",
    "high_density_evals = [r for r in evaluation_results if r['rating'] >= 4]\n",
    "\n",
    "if len(high_density_evals) < 3:\n",
    "    print(f\"Warning: Only {len(high_density_evals)} high-density passages found\")\n",
    "    print(\"Consider evaluating more passages or lowering density threshold\")\n",
    "else:\n",
    "    # Collect full evaluation texts for each high-density passage\n",
    "    passage_evaluations = []\n",
    "    for result in high_density_evals:\n",
    "        eval_data = store.get_passage_evaluation(result['eval_id'])\n",
    "        if eval_data:\n",
    "            # Format as evaluation text for the prompt\n",
    "            eval_text = f\"\"\"Passage ID: {result['eval_id']}\n",
    "Density Rating: {eval_data['density_rating']}\n",
    "Recommendation: {'YES' if eval_data['recommendation'] else 'NO'}\n",
    "\n",
    "Task Coverage:\n",
    "{eval_data.get('task_coverage', 'N/A')}\n",
    "\n",
    "Teaching Value:\n",
    "{eval_data.get('teaching_value', 'N/A')}\n",
    "\"\"\"\n",
    "            passage_evaluations.append(eval_text)\n",
    "    \n",
    "    # Generate example set curation prompt\n",
    "    config = ExampleSetConstructionConfig(\n",
    "        field_guide=field_guide_text,\n",
    "        passage_evaluations=passage_evaluations\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    # Run curation\n",
    "    print(f\"Curating example set from {len(passage_evaluations)} high-density passages...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    # Parse selected passages\n",
    "    try:\n",
    "        selected = parse_example_set_selection(response.content)\n",
    "        \n",
    "        if validate_example_set_selection(selected):\n",
    "            # Save example set to ResultStore\n",
    "            example_set_id = store.save_example_set(\n",
    "                field_guide_id=field_guide_id,\n",
    "                selection_rationale=response.content,  # Full curation response\n",
    "                model=response.model\n",
    "            )\n",
    "            \n",
    "            # Add member passages\n",
    "            for passage in selected:\n",
    "                store.add_example_set_member(\n",
    "                    example_set_id=example_set_id,\n",
    "                    evaluation_id=passage['passage_id'],\n",
    "                    unique_contribution=passage.get('unique_contribution', '')\n",
    "                )\n",
    "            \n",
    "            print(f\"\\nExample set saved: {example_set_id}\")\n",
    "            print(f\"Selected passages:\")\n",
    "            for p in selected:\n",
    "                print(f\"  - {p['passage_id']} (rating: {p.get('density_rating', 'N/A')})\")\n",
    "        else:\n",
    "            print(\"Validation failed: Example set does not meet criteria\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing example set selection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export & Inspection\n",
    "\n",
    "Export field guide and example sets to filesystem for use in generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"outputs/author_modeling\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export field guide\n",
    "field_guide_path = output_dir / f\"{field_guide_id}.txt\"\n",
    "store.export_synthesis(\n",
    "    synthesis_id=field_guide_id,\n",
    "    output_path=field_guide_path,\n",
    "    metadata_format='yaml'\n",
    ")\n",
    "print(f\"Field guide exported to: {field_guide_path}\")\n",
    "\n",
    "# Export example set (if exists)\n",
    "example_sets = store.list_example_sets()\n",
    "if example_sets:\n",
    "    for example_set_id in example_sets:\n",
    "        # Get example set with members\n",
    "        example_set = store.get_example_set_with_members(example_set_id)\n",
    "        \n",
    "        # Write to file\n",
    "        example_set_path = output_dir / f\"{example_set_id}.txt\"\n",
    "        with open(example_set_path, 'w') as f:\n",
    "            f.write(f\"# Example Set: {example_set_id}\\n\\n\")\n",
    "            f.write(f\"Field Guide: {example_set['field_guide_id']}\\n\")\n",
    "            f.write(f\"Created: {example_set['created_at']}\\n\")\n",
    "            f.write(f\"Model: {example_set['model']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Selection Rationale\\n\\n\")\n",
    "            f.write(example_set['selection_rationale'])\n",
    "            f.write(\"\\n\\n## Selected Passages\\n\\n\")\n",
    "            \n",
    "            for member in example_set['members']:\n",
    "                f.write(f\"### {member['evaluation_id']}\\n\\n\")\n",
    "                f.write(f\"**Unique Contribution:** {member['unique_contribution']}\\n\\n\")\n",
    "                \n",
    "                # Get actual passage text\n",
    "                eval_data = store.get_passage_evaluation(member['evaluation_id'])\n",
    "                if eval_data:\n",
    "                    sample = store.get_sample(eval_data['sample_id'])\n",
    "                    # Extract paragraph range from full sample\n",
    "                    # (simplified - in production, implement proper extraction)\n",
    "                    f.write(f\"**Source:** {eval_data['sample_id']}\\n\")\n",
    "                    f.write(f\"**Paragraph Range:** {eval_data.get('paragraph_range', 'N/A')}\\n\")\n",
    "                    f.write(f\"**Density Rating:** {eval_data['density_rating']}\\n\\n\")\n",
    "                f.write(\"---\\n\\n\")\n",
    "        \n",
    "        print(f\"Example set exported to: {example_set_path}\")\n",
    "else:\n",
    "    print(\"No example sets to export\")\n",
    "\n",
    "print(\"\\nExport complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Provenance Verification\n",
    "\n",
    "Verify full audit trail from samples through to example sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get field guide provenance\n",
    "print(\"Field Guide Provenance:\")\n",
    "provenance = store.get_synthesis_provenance(field_guide_id)\n",
    "pprint(provenance, depth=3)\n",
    "\n",
    "# Get example set provenance\n",
    "if example_sets:\n",
    "    print(f\"\\nExample Set {example_sets[0]} Provenance:\")\n",
    "    example_set = store.get_example_set_with_members(example_sets[0])\n",
    "    print(f\"  Field Guide: {example_set['field_guide_id']}\")\n",
    "    print(f\"  Members: {len(example_set['members'])}\")\n",
    "    for member in example_set['members']:\n",
    "        eval_data = store.get_passage_evaluation(member['evaluation_id'])\n",
    "        print(f\"    - {member['evaluation_id']} (from {eval_data['sample_id']})\")\n",
    "\n",
    "print(\"\\nProvenance verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented a complete author modeling pipeline:\n",
    "\n",
    "1. ✅ **Stage 1**: Analyzed samples through three lenses (implied author, decision patterns, functional texture)\n",
    "2. ✅ **Stage 2**: Synthesized each dimension across samples\n",
    "3. ✅ **Stage 3**: Constructed unified field guide with recognition criteria and rubric\n",
    "4. ✅ **Stage 4**: Evaluated passages using field guide (density ratings 1-5)\n",
    "5. ✅ **Stage 5**: Curated 3-4 high-density passages as few-shot example set\n",
    "\n",
    "**Next Steps:**\n",
    "- Use exported field guide for understanding author's patterns\n",
    "- Use example set for few-shot learning in generation tasks\n",
    "- Compare few-shot results against baseline methods (Stage 6 - future work)\n",
    "\n",
    "All results stored in `russell_author_modeling.db` with full provenance tracking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}