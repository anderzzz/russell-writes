{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author Modeling Pipeline\n",
    "\n",
    "This notebook implements a 5-stage pipeline for modeling an author's characteristic writing patterns through few-shot example curation.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "Rather than extracting explicit style rules, this approach models the author's **decision-making patterns** and **sensibility**, then curates exemplary passages for tacit transmission via few-shot learning.\n",
    "\n",
    "## Pipeline Stages\n",
    "\n",
    "1. **Stage 1: Analytical Mining** - Analyze sample texts through three lenses:\n",
    "   - Implied Author: Sensibility and stance\n",
    "   - Decision Patterns: Compositional choices\n",
    "   - Functional Texture: How surface features serve purpose\n",
    "\n",
    "2. **Stage 2: Cross-Text Synthesis** - Synthesize each dimension across multiple samples to identify stable patterns\n",
    "\n",
    "3. **Stage 3: Field Guide Construction** - Integrate all three syntheses into unified recognition criteria and density rubric\n",
    "\n",
    "4. **Stage 4: Passage Evaluation** - Evaluate passages from corpus using field guide (1-5 density rating)\n",
    "\n",
    "5. **Stage 5: Example Set Construction** - Curate 3-4 high-density passages that balance density, diversity, and complementarity\n",
    "\n",
    "## Note\n",
    "\n",
    "This pipeline is author-agnostic. The Russell corpus is used as a test case, but the same approach applies to any author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm==1.79.3 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.79.3)\n",
      "Requirement already satisfied: pydantic==2.7.4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (2.7.4)\n",
      "Requirement already satisfied: jinja2>=3.1.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (3.10.11)\n",
      "Requirement already satisfied: click in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.27.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.5.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.99.5 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.9.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (4.13.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jinja2>=3.1.0->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (4.0.2)\n",
      "Requirement already satisfied: anyio in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: certifi in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: sniffio in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=6.8.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.20.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (5.10.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (2023.12.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.20.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (0.9.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (4.64.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2022.4.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.27.1)\n",
      "Requirement already satisfied: exceptiongroup in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from anyio->httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.0.12)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (0.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from belletrist import (\n",
    "    LLM,\n",
    "    LLMConfig,\n",
    "    PromptMaker,\n",
    "    DataSampler,\n",
    "    ResultStore,\n",
    "    # Stage 1 configs\n",
    "    ImpliedAuthorConfig,\n",
    "    DecisionPatternConfig,\n",
    "    FunctionalTextureConfig,\n",
    "    # Stage 2 configs\n",
    "    ImpliedAuthorSynthesisConfig,\n",
    "    DecisionPatternSynthesisConfig,\n",
    "    TexturalSynthesisConfig,\n",
    "    # Stage 3 config\n",
    "    AuthorModelDefinitionConfig,\n",
    "    FieldGuideConstructionConfig,\n",
    "    # Stage 4 config\n",
    "    PassageEvaluationConfig,\n",
    "    # Stage 5 config\n",
    "    ExampleSetConstructionConfig,\n",
    "    # Utilities\n",
    "    extract_paragraph_windows,\n",
    "    extract_logical_sections,\n",
    "    get_full_sample_as_passage,\n",
    "    parse_passage_evaluation,\n",
    "    parse_example_set_selection,\n",
    "    validate_passage_evaluation,\n",
    "    validate_example_set_selection,\n",
    ")\n",
    "\n",
    "print(\"Dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Base Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm_config = LLMConfig(\n",
    "    model=\"mistral/mistral-large-2411\",  # Change as needed\n",
    "    api_key=os.environ.get('MISTRAL_API_KEY'),\n",
    "    temperature=0.7\n",
    ")\n",
    "llm = LLM(llm_config)\n",
    "\n",
    "# Initialize prompt maker\n",
    "prompt_maker = PromptMaker()\n",
    "\n",
    "# Initialize data sampler (Russell corpus)\n",
    "data_dir = Path(\"data/russell\")\n",
    "sampler = DataSampler(data_dir)\n",
    "\n",
    "# Initialize result store\n",
    "store = ResultStore(\"russell_author_modeling.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.reset('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration: Select Samples for Analysis\n",
    "\n",
    "For Stages 1-3, we'll analyze 3-5 samples to extract stable cross-text patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will analyze 5 samples through Stages 1-3\n"
     ]
    }
   ],
   "source": [
    "# Select samples for Stage 1-3 analysis\n",
    "# These should be diverse, substantial passages (500-800 words recommended)\n",
    "ANALYSIS_SAMPLES = [\n",
    "    \"sample_001\",\n",
    "    \"sample_002\",\n",
    "    \"sample_003\",\n",
    "    \"sample_004\",\n",
    "    \"sample_005\",\n",
    "    # Add more as needed for richer cross-text synthesis\n",
    "]\n",
    "\n",
    "# For this demo, we'll generate samples if they don't exist\n",
    "# In production, you might use specific pre-selected samples\n",
    "NUM_SAMPLES = 5\n",
    "SAMPLE_PARAGRAPH_LENGTH = 10\n",
    "\n",
    "print(f\"Will analyze {NUM_SAMPLES} samples through Stages 1-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stage 1: Analytical Mining\n",
    "\n",
    "Run three specialized analyses on each sample:\n",
    "- **Implied Author**: What sensibility emerges from the prose?\n",
    "- **Decision Patterns**: What compositional choices are made at key junctures?\n",
    "- **Functional Texture**: How do surface features serve purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate or Load Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ sample_001 already gathered\n",
      "✓ sample_002 already gathered\n",
      "✓ sample_003 already gathered\n",
      "✓ sample_004 already gathered\n",
      "✓ sample_005 already gathered\n",
      "\n",
      "Sample 001 preview:\n",
      "PART I\n",
      "\n",
      "THE PRESENT CONDITION OF RUSSIA\n",
      "\n",
      "I\n",
      "\n",
      "WHAT IS HOPED FROM BOLSHEVISM\n",
      "\n",
      "To understand Bolshevism it is not sufficient to know facts; it is\n",
      "necessary also to enter with sympathy or imagination into a new\n",
      "spirit. The chief thing that the Bolsheviks have done is to create a\n",
      "hope, or at any rate to make strong and widespread a hope which was\n",
      "formerly confined to a few. This aspect of the movement is as easy to\n",
      "grasp at a distance as it is in Russia--perhaps even easier, because\n",
      "in Russia present circumstances tend to obscure the view of the\n",
      "distant future. But the actual situation in Russia can only be\n",
      "understood superficially if we forget the hope which is the motive\n",
      "power of the whole. One might as well describe the Thebaid without\n",
      "mentioning that the hermits expected eternal bliss as the reward of\n",
      "their sacrifices here on earth.\n",
      "\n",
      "I cannot share the hopes of the Bolsheviks any more than those of the\n",
      "Egyptian anchorites; I regard both as tragic delusions, destined to\n",
      "bring upon the world centuries of darkness and futile violence. The\n",
      "principles of the Sermon on the Mount are admirable, but their effect\n",
      "upon average human nature was very different from what was intended.\n",
      "Those who followed Christ did not learn to love their enemies or to\n",
      "turn the other cheek. They learned instead to use the Inquisition and\n",
      "the stake, to subject the human intellect to the yoke of an ignorant\n",
      "and intolerant priesthood, to degrade art and extinguish science for a\n",
      "thousand years. These were the inevitable results, not of the\n",
      "teaching, but of fanatical belief in the teaching. The hopes which\n",
      "inspire Communism are, in the main, as admirable as those instilled by\n",
      "the Sermon on the Mount, but they are held as fanatically, and are\n",
      "likely to do as much harm. Cruelty lurks in our instincts, and\n",
      "fanaticism is a camouflage for cruelty. Fanatics are seldom genuinely\n",
      "humane, and those who sincerely dread cruelty will be slow to adopt a\n",
      "fanatical creed. I do not know whether Bolshevism can be prevented\n",
      "from acquiring universal power. But even if it cannot, I am persuaded\n",
      "that those who stand out against it, not from love of ancient\n",
      "injustice, but in the name of the free spirit of Man, will be the\n",
      "bearers of the seeds of progress, from which, when the world's\n",
      "gestation is accomplished, new life will be born.\n",
      "\n",
      "The war has left throughout Europe a mood of disillusionment and\n",
      "despair which calls aloud for a new religion, as the only force\n",
      "capable of giving men the energy to live vigorously. Bolshevism has\n",
      "supplied the new religion. It promises glorious things: an end of the\n",
      "injustice of rich and poor, an end of economic slavery, an end of war.\n",
      "It promises an end of the disunion of classes which poisons political\n",
      "life and threatens our industrial system with destruction. It promises\n",
      "an end to commercialism, that subtle falsehood that leads men to\n",
      "appraise everything by its money value, and to determine money value\n",
      "often merely by the caprices of idle plutocrats. It promises a world\n",
      "where all men and women shall be kept sane by work, and where all work\n",
      "shall be of value to the community, not only to a few wealthy\n",
      "vampires. It is to sweep away listlessness and pessimism and weariness\n",
      "and all the complicated miseries of those whose circumstances allow\n",
      "idleness and whose energies are not sufficient to force activity. In\n",
      "place of palaces and hovels, futile vice and useless misery, there is\n",
      "to be wholesome work, enough but not too much, all of it useful,\n",
      "performed by men and women who have no time for pessimism and no\n",
      "occasion for despair.\n",
      "\n",
      "The existing capitalist system is doomed. Its injustice is so glaring\n",
      "that only ignorance and tradition could lead wage-earners to tolerate\n",
      "it. As ignorance diminishes, tradition becomes weakened, and the war\n",
      "destroyed the hold upon men's minds of everything merely traditional.\n",
      "It may be that, through the influence of America, the capitalist\n",
      "system will linger for another fifty years; but it will grow\n",
      "continually weaker, and can never recover the position of easy\n",
      "dominance which it held in the nineteenth century. To attempt to\n",
      "bolster it up is a useless diversion of energies which might be\n",
      "expended upon building something new. Whether the new thing will be\n",
      "Bolshevism or something else, I do not know; whether it will be better\n",
      "or worse than capitalism, I do not know. But that a radically new\n",
      "order of society will emerge, I feel no doubt. And I also feel no\n",
      "doubt that the new order will be either some form of Socialism or a\n",
      "reversion to barbarism and petty war such as occurred during the\n",
      "barbarian invasion. If Bolshevism remains the only vigorous and\n",
      "effective competitor of capitalism, I believe that no form of\n",
      "Socialism will be realized, but only chaos and destruction. This\n",
      "belief, for which I shall give reasons later, is one of the grounds\n",
      "upon which I oppose Bolshevism. But to oppose it from the point of\n",
      "view of a supporter of capitalism would be, to my mind, utterly\n",
      "futile and against the movement of history in the present age.\n",
      "\n",
      "The effect of Bolshevism as a revolutionary hope is greater outside\n",
      "Russia than within the Soviet Republic. Grim realities have done much\n",
      "to kill hope among those who are subject to the dictatorship of\n",
      "Moscow. Yet even within Russia, the Communist party, in whose hands\n",
      "all political power is concentrated, still lives by hope, though the\n",
      "pressure of events has made the hope severe and stern and somewhat\n",
      "remote. It is this hope that leads to concentration upon the rising\n",
      "generation. Russian Communists often avow that there is little hope\n",
      "for those who are already adult, and that happiness can only come to\n",
      "the children who have grown up under the new regime and been moulded\n",
      "from the first to the group-mentality that Communism requires. It is\n",
      "only after the lapse of a generation that they hope to create a Russia\n",
      "that shall realize their vision.\n",
      "\n",
      "In the Western World, the hope inspired by Bolshevism is more\n",
      "immediate, less shot through with tragedy. Western Socialists who have\n",
      "visited Russia have seen fit to suppress the harsher features of the\n",
      "present regime, and have disseminated a belief among their followers\n",
      "that the millennium would be quickly realized there if there were no\n",
      "war and no blockade. Even those Socialists who are not Bolsheviks for\n",
      "their own country have mostly done very little to help men in\n",
      "appraising the merits or demerits of Bolshevik methods. By this lack\n",
      "of courage they have exposed Western Socialism to the danger of\n",
      "becoming Bolshevik through ignorance of the price that has to be paid\n",
      "and of the uncertainty as to whether the desired goal will be reached\n",
      "in the end. I believe that the West is capable of adopting less\n",
      "painful and more certain methods of reaching Socialism than those that\n",
      "have seemed necessary in Russia. And I believe that while some forms\n",
      "of Socialism are immeasurably better than capitalism, others are even\n",
      "worse. Among those that are worse I reckon the form which is being\n",
      "achieved in Russia, not only in itself, but as a more insuperable\n",
      "barrier to further progress....\n"
     ]
    }
   ],
   "source": [
    "# Generate samples if they don't exist\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    \n",
    "    # Skip if already saved\n",
    "    if store.get_sample(sample_id):\n",
    "        print(f\"✓ {sample_id} already gathered\")\n",
    "        continue\n",
    "        \n",
    "    # Generate new sample\n",
    "    segment = sampler.sample_segment(p_length=SAMPLE_PARAGRAPH_LENGTH)\n",
    "    store.save_segment(sample_id, segment)\n",
    "    print(f\"Generated {sample_id}\")\n",
    "\n",
    "# Display first sample for reference\n",
    "sample = store.get_sample(\"sample_001\")\n",
    "print(f\"\\nSample 001 preview:\\n{sample['text']}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stage 1A: Implied Author Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_001: Running implied_author analysis...\n",
      "sample_001: implied_author analysis complete\n",
      "sample_002: Running implied_author analysis...\n",
      "sample_002: implied_author analysis complete\n",
      "sample_003: Running implied_author analysis...\n",
      "sample_003: implied_author analysis complete\n",
      "sample_004: Running implied_author analysis...\n",
      "sample_004: implied_author analysis complete\n",
      "sample_005: Running implied_author analysis...\n",
      "sample_005: implied_author analysis complete\n",
      "\n",
      "Stage 1A complete: All samples analyzed for implied author\n"
     ]
    }
   ],
   "source": [
    "# Analyze each sample for implied author\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    analyst_name = ImpliedAuthorConfig.analyst_name()\n",
    "    \n",
    "    # Check if analysis already exists (resume support)\n",
    "    if store.get_analysis(sample_id, analyst_name):\n",
    "        print(f\"{sample_id}: {analyst_name} analysis already complete\")\n",
    "        continue\n",
    "    \n",
    "    # Get sample text\n",
    "    sample = store.get_sample(sample_id)\n",
    "    \n",
    "    # Generate prompt\n",
    "    config = ImpliedAuthorConfig(text=sample['text'])\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    # Run LLM\n",
    "    print(f\"{sample_id}: Running {analyst_name} analysis...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    # Save analysis\n",
    "    store.save_analysis(\n",
    "        sample_id=sample_id,\n",
    "        analyst=analyst_name,\n",
    "        output=response.content,\n",
    "        model=response.model\n",
    "    )\n",
    "    print(f\"{sample_id}: {analyst_name} analysis complete\")\n",
    "\n",
    "print(\"\\nStage 1A complete: All samples analyzed for implied author\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stage 1B: Decision Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_001: Running decision_pattern analysis...\n",
      "sample_001: decision_pattern analysis complete\n",
      "sample_002: Running decision_pattern analysis...\n",
      "sample_002: decision_pattern analysis complete\n",
      "sample_003: Running decision_pattern analysis...\n",
      "sample_003: decision_pattern analysis complete\n",
      "sample_004: Running decision_pattern analysis...\n",
      "sample_004: decision_pattern analysis complete\n",
      "sample_005: Running decision_pattern analysis...\n",
      "sample_005: decision_pattern analysis complete\n",
      "\n",
      "Stage 1B complete: All samples analyzed for decision patterns\n"
     ]
    }
   ],
   "source": [
    "# Analyze each sample for decision patterns\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    analyst_name = DecisionPatternConfig.analyst_name()\n",
    "    \n",
    "    if store.get_analysis(sample_id, analyst_name):\n",
    "        print(f\"{sample_id}: {analyst_name} analysis already complete\")\n",
    "        continue\n",
    "    \n",
    "    sample = store.get_sample(sample_id)\n",
    "    config = DecisionPatternConfig(text=sample['text'])\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    print(f\"{sample_id}: Running {analyst_name} analysis...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    store.save_analysis(\n",
    "        sample_id=sample_id,\n",
    "        analyst=analyst_name,\n",
    "        output=response.content,\n",
    "        model=response.model\n",
    "    )\n",
    "    print(f\"{sample_id}: {analyst_name} analysis complete\")\n",
    "\n",
    "print(\"\\nStage 1B complete: All samples analyzed for decision patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Stage 1C: Functional Texture Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_001: Running functional_texture analysis...\n",
      "sample_001: functional_texture analysis complete\n",
      "sample_002: Running functional_texture analysis...\n",
      "sample_002: functional_texture analysis complete\n",
      "sample_003: Running functional_texture analysis...\n",
      "sample_003: functional_texture analysis complete\n",
      "sample_004: Running functional_texture analysis...\n",
      "sample_004: functional_texture analysis complete\n",
      "sample_005: Running functional_texture analysis...\n",
      "sample_005: functional_texture analysis complete\n",
      "\n",
      "Stage 1C complete: All samples analyzed for functional texture\n"
     ]
    }
   ],
   "source": [
    "# Analyze each sample for functional texture\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    analyst_name = FunctionalTextureConfig.analyst_name()\n",
    "    \n",
    "    if store.get_analysis(sample_id, analyst_name):\n",
    "        print(f\"{sample_id}: {analyst_name} analysis already complete\")\n",
    "        continue\n",
    "    \n",
    "    sample = store.get_sample(sample_id)\n",
    "    config = FunctionalTextureConfig(text=sample['text'])\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    print(f\"{sample_id}: Running {analyst_name} analysis...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    store.save_analysis(\n",
    "        sample_id=sample_id,\n",
    "        analyst=analyst_name,\n",
    "        output=response.content,\n",
    "        model=response.model\n",
    "    )\n",
    "    print(f\"{sample_id}: {analyst_name} analysis complete\")\n",
    "\n",
    "print(\"\\nStage 1C complete: All samples analyzed for functional texture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Stage 1 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_001: 3 analyses complete\n",
      "  - decision_pattern\n",
      "  - functional_texture\n",
      "  - implied_author\n",
      "sample_002: 3 analyses complete\n",
      "  - decision_pattern\n",
      "  - functional_texture\n",
      "  - implied_author\n",
      "sample_003: 3 analyses complete\n",
      "  - decision_pattern\n",
      "  - functional_texture\n",
      "  - implied_author\n",
      "sample_004: 3 analyses complete\n",
      "  - decision_pattern\n",
      "  - functional_texture\n",
      "  - implied_author\n",
      "sample_005: 3 analyses complete\n",
      "  - decision_pattern\n",
      "  - functional_texture\n",
      "  - implied_author\n",
      "\n",
      "--- Sample Implied Author Analysis (first 500 chars) ---\n",
      "Total chars: 6370\n",
      "### PART 1: DIMENSIONAL ANALYSIS\n",
      "\n",
      "#### 1. RELATIONSHIP TO MATERIAL\n",
      "\n",
      "**Observation**: The author writes with a blend of curiosity and mastery, often seeking to unravel complexities and share insights. They approach the material with a keen intellectual interest, breaking down complex ideas into digestible parts.\n",
      "\n",
      "**Evidence**:\n",
      "1. \"The bent stick in water belongs here. People say it looks bent but is straight: this only means that it is straight to the touch, though bent to sight. There is no 'illusion,' but only a false inference, if we think that the stick would feel bent to the touch.\"\n",
      "2. \"It is a mistake to ask whether the 'thing' is duplicated when we see it double. The 'thing' is a whole system of 'sensibilia,' and it is only those visual 'sensibilia' which are data to the percipient that are duplicated.\"\n",
      "\n",
      "**Explanation**: These passages show the author's willingness to engage with complex phenomena and explain them in a way that clarifies misunderstandings. The use of precise language and logical reasoning indicates a mastery of the subject matter.\n",
      "\n",
      "#### 2. RELATIONSHIP TO READER\n",
      "\n",
      "**Observation**: The author imagines the reader as an intellectual equal, capable of following intricate arguments and logical deductions. There is a sense of intimacy, as if the author is sharing a personal journey of intellectual discovery.\n",
      "\n",
      "**Evidence**:\n",
      "1. \"It is a mistake to ask whether the 'thing' is duplicated when we see it double.\"\n",
      "2. \"Dreams might conceivably belong to this class, if they were jointed sufficiently neatly into waking life; but the chief instances are recurrent sensory hallucinations of the kind that lead to insanity.\"\n",
      "\n",
      "**Explanation**: The author assumes the reader is familiar with concepts like \"sensibilia\" and \"hallucinations,\" indicating a level of intellectual trust. The use of \"we\" and \"our\" creates a sense of shared exploration and understanding.\n",
      "\n",
      "#### 3. RELATIONSHIP TO UNCERTAINTY\n",
      "\n",
      "**Observation**: The author handles uncertainty with a balanced approach, acknowledging alternative views while maintaining a confident stance where evidence is strong. They are comfortable with provisional conclusions and the complexity of their subject matter.\n",
      "\n",
      "**Evidence**:\n",
      "1. \"It is a mistake to ask whether the 'thing' is duplicated when we see it double.\"\n",
      "2. \"There certainly are cases where (whatever psychological causes may contribute) the presence of physical causes also is very evident.\"\n",
      "\n",
      "**Explanation**: These passages show the author's willingness to acknowledge the limits of knowledge and the possibility of alternative explanations. They qualify their statements without undermining their overall confidence in their arguments.\n",
      "\n",
      "#### 4. RELATIONSHIP TO THE ACT OF WRITING\n",
      "\n",
      "**Observation**: The author takes pleasure in the act of writing, often employing wit and wordplay to convey complex ideas. They use language not just as a tool but as an aesthetic medium, enjoying the craft of prose.\n",
      "\n",
      "**Evidence**:\n",
      "1. \"The whole dream will be an appearance of the door banging, but owing to the peculiar condition of the body (especially the brain) during sleep, this appearance is not that expected to be produced by a door banging.\"\n",
      "2. \"The 'thing' is a whole system of 'sensibilia,' and it is only those visual 'sensibilia' which are data to the percipient that are duplicated.\"\n",
      "\n",
      "**Explanation**: The use of vivid imagery and precise, almost playful, language indicates an author who enjoys the process of writing. The prose is not merely instrumental but calls attention to itself through its craftsmanship.\n",
      "\n",
      "### PART 2: SIGNATURE MOMENTS\n",
      "\n",
      "#### Passage 1:\n",
      "\"The bent stick in water belongs here. People say it looks bent but is straight: this only means that it is straight to the touch, though bent to sight. There is no 'illusion,' but only a false inference, if we think that the stick would feel bent to the touch.\"\n",
      "\n",
      "**Explanation**: This passage is particularly revealing because it showcases the author's ability to break down a complex concept into understandable parts. It reflects their mastery of the subject matter and their desire to share this understanding with the reader.\n",
      "\n",
      "**Dimensions**: Relationship to Material, Relationship to Reader\n",
      "\n",
      "#### Passage 2:\n",
      "\"It is a mistake to ask whether the 'thing' is duplicated when we see it double. The 'thing' is a whole system of 'sensibilia,' and it is only those visual 'sensibilia' which are data to the percipient that are duplicated.\"\n",
      "\n",
      "**Explanation**: This passage highlights the author's intellectual curiosity and their ability to handle complex topics with clarity. It also shows their relationship to the reader, assuming a level of intellectual engagement.\n",
      "\n",
      "**Dimensions**: Relationship to Material, Relationship to Reader, Relationship to Uncertainty\n",
      "\n",
      "#### Passage 3:\n",
      "\"The whole dream will be an appearance of the door banging, but owing to the peculiar condition of the body (especially the brain) during sleep, this appearance is not that expected to be produced by a door banging.\"\n",
      "\n",
      "**Explanation**: This passage is revealing because it demonstrates the author's enjoyment of language and their ability to use vivid imagery to convey complex ideas. It also shows their comfort with uncertainty and provisional conclusions.\n",
      "\n",
      "**Dimensions**: Relationship to Uncertainty, Relationship to the Act of Writing\n",
      "\n",
      "### PART 3: SYNTHETIC PORTRAIT\n",
      "\n",
      "The implied author is a deeply intellectual and curious mind, driven by a passion for understanding the complexities of perception and reality. They approach their subject matter with a blend of mastery and curiosity, seeking to unravel intricate concepts and share their insights with the reader. Their prose is characterized by precision and clarity, often employing wit and wordplay to convey complex ideas. They imagine their reader as an intellectual equal, capable of following intricate arguments and appreciating the nuances of their thought process. The author is comfortable with uncertainty, acknowledging the limits of knowledge and the possibility of alternative explanations. Their writing is not merely instrumental but reflects a pleasure in the craft of prose, calling attention to itself through vivid imagery and precise language. In essence, the implied author is a thoughtful, intellectually rigorous, and aesthetically inclined mind, inviting the reader on a journey of discovery and understanding.\n"
     ]
    }
   ],
   "source": [
    "# Display completion status\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "    print(f\"{sample_id}: {len(analyses)} analyses complete\")\n",
    "    for analyst_name in analyses.keys():\n",
    "        print(f\"  - {analyst_name}\")\n",
    "\n",
    "# Optionally display one analysis\n",
    "print(\"\\n--- Sample Implied Author Analysis (first 500 chars) ---\")\n",
    "sample, analyses = store.get_sample_with_analyses(\"sample_005\")\n",
    "implied_author = analyses.get(ImpliedAuthorConfig.analyst_name(), \"Not found\")\n",
    "print(f\"Total chars: {len(implied_author)}\")\n",
    "print(implied_author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stage 2: Cross-Text Synthesis\n",
    "\n",
    "Synthesize each analytical dimension across samples to identify stable patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2A: Implied Author Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Implied author synthesis already exists: implied_author_synthesis_001\n",
      "\n",
      "Preview (first 400 chars):\n",
      "## PART 1: THE STABLE CORE\n",
      "\n",
      "### Fundamental Stance Toward Ideas and Inquiry\n",
      "\n",
      "**How does this author consistently engage with their subject matter?**\n",
      "The author consistently engages with their subject matter through a blend of curiosity, mastery, and critical distance. They display a deep intellectual interest, systematically exploring complex phenomena and seeking to dissolve them through logical ...\n"
     ]
    }
   ],
   "source": [
    "# Collect all implied author analyses\n",
    "implied_author_analyses = {}\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    analysis = store.get_analysis(sample_id, ImpliedAuthorConfig.analyst_name())\n",
    "    if analysis:\n",
    "        implied_author_analyses[sample_id] = analysis\n",
    "\n",
    "# Check if synthesis already exists\n",
    "synthesis_type = ImpliedAuthorSynthesisConfig.synthesis_type()\n",
    "existing = store.list_syntheses(synthesis_type)\n",
    "\n",
    "if existing:\n",
    "    print(f\"Implied author synthesis already exists: {existing[0]}\")\n",
    "    implied_author_synthesis_id = existing[0]\n",
    "else:\n",
    "    # Generate synthesis prompt\n",
    "    config = ImpliedAuthorSynthesisConfig(\n",
    "        implied_author_analyses=implied_author_analyses\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    # Run synthesis\n",
    "    print(\"Running implied author synthesis...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    # Save synthesis\n",
    "    sample_contributions = [\n",
    "        (sample_id, ImpliedAuthorConfig.analyst_name())\n",
    "        for sample_id in implied_author_analyses.keys()\n",
    "    ]\n",
    "    \n",
    "    implied_author_synthesis_id = store.save_synthesis(\n",
    "        synthesis_type=synthesis_type,\n",
    "        output=response.content,\n",
    "        model=response.model,\n",
    "        sample_contributions=sample_contributions,\n",
    "        config=config\n",
    "    )\n",
    "    print(f\"Synthesis saved: {implied_author_synthesis_id}\")\n",
    "\n",
    "# Display preview\n",
    "synth = store.get_synthesis(implied_author_synthesis_id)\n",
    "print(f\"\\nPreview (first 400 chars):\\n{synth['output'][:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2B: Decision Pattern Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running decision pattern synthesis...\n",
      "Synthesis saved: decision_pattern_synthesis_001\n",
      "\n",
      "Preview (first 400 chars):\n",
      "## PART 1: COMPOSITIONAL PROBLEM TAXONOMY\n",
      "\n",
      "Across all texts, the following types of compositional problems recur for this author:\n",
      "\n",
      "1. **Opening a text or major section**\n",
      "   - **Description**: The author often faces the challenge of introducing complex topics in a way that sets the tone and frames the discussion effectively. This involves selecting the right opening strategy, whether it's a philoso...\n"
     ]
    }
   ],
   "source": [
    "# Collect all decision pattern analyses\n",
    "decision_pattern_analyses = {}\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    analysis = store.get_analysis(sample_id, DecisionPatternConfig.analyst_name())\n",
    "    if analysis:\n",
    "        decision_pattern_analyses[sample_id] = analysis\n",
    "\n",
    "# Check if synthesis already exists\n",
    "synthesis_type = DecisionPatternSynthesisConfig.synthesis_type()\n",
    "existing = store.list_syntheses(synthesis_type)\n",
    "\n",
    "if existing:\n",
    "    print(f\"Decision pattern synthesis already exists: {existing[0]}\")\n",
    "    decision_pattern_synthesis_id = existing[0]\n",
    "else:\n",
    "    config = DecisionPatternSynthesisConfig(\n",
    "        decision_pattern_analyses=decision_pattern_analyses\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    print(\"Running decision pattern synthesis...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    sample_contributions = [\n",
    "        (sample_id, DecisionPatternConfig.analyst_name())\n",
    "        for sample_id in decision_pattern_analyses.keys()\n",
    "    ]\n",
    "    \n",
    "    decision_pattern_synthesis_id = store.save_synthesis(\n",
    "        synthesis_type=synthesis_type,\n",
    "        output=response.content,\n",
    "        model=response.model,\n",
    "        sample_contributions=sample_contributions,\n",
    "        config=config\n",
    "    )\n",
    "    print(f\"Synthesis saved: {decision_pattern_synthesis_id}\")\n",
    "\n",
    "synth = store.get_synthesis(decision_pattern_synthesis_id)\n",
    "print(f\"\\nPreview (first 400 chars):\\n{synth['output'][:400]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stage 2C: Textural Synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running textural synthesis...\n",
      "Synthesis saved: textural_synthesis_001\n",
      "\n",
      "Preview (first 400 chars):\n",
      "## SYNTHESIS OF TEXTURAL ANALYSES\n",
      "\n",
      "## PART 1: SENTENCE ARCHITECTURE\n",
      "\n",
      "### Structural Character\n",
      "\n",
      "This author's characteristic sentence is typically complex and varied, often employing long, compound-complex structures that are either front-loaded or back-loaded. The main clauses often precede a series of subordinate clauses that add layers of detail and qualification, creating a sense of progression...\n"
     ]
    }
   ],
   "source": [
    "# Collect all functional texture analyses\n",
    "textural_analyses = {}\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    analysis = store.get_analysis(sample_id, FunctionalTextureConfig.analyst_name())\n",
    "    if analysis:\n",
    "        textural_analyses[sample_id] = analysis\n",
    "\n",
    "# Check if synthesis already exists\n",
    "synthesis_type = TexturalSynthesisConfig.synthesis_type()\n",
    "existing = store.list_syntheses(synthesis_type)\n",
    "\n",
    "if existing:\n",
    "    print(f\"Textural synthesis already exists: {existing[0]}\")\n",
    "    textural_synthesis_id = existing[0]\n",
    "else:\n",
    "    config = TexturalSynthesisConfig(\n",
    "        textural_analyses=textural_analyses\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    print(\"Running textural synthesis...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    sample_contributions = [\n",
    "        (sample_id, FunctionalTextureConfig.analyst_name())\n",
    "        for sample_id in textural_analyses.keys()\n",
    "    ]\n",
    "    \n",
    "    textural_synthesis_id = store.save_synthesis(\n",
    "        synthesis_type=synthesis_type,\n",
    "        output=response.content,\n",
    "        model=response.model,\n",
    "        sample_contributions=sample_contributions,\n",
    "        config=config\n",
    "    )\n",
    "    print(f\"Synthesis saved: {textural_synthesis_id}\")\n",
    "\n",
    "synth = store.get_synthesis(textural_synthesis_id)\n",
    "print(f\"\\nPreview (first 400 chars):\\n{synth['output'][:400]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implied_synth = store.get_synthesis(implied_author_synthesis_id)\n",
    "decision_synth = store.get_synthesis(decision_pattern_synthesis_id)\n",
    "textural_synth = store.get_synthesis(textural_synthesis_id)\n",
    "\n",
    "# Check if author model definition already exists\n",
    "synthesis_type = AuthorModelDefinitionConfig.synthesis_type()\n",
    "existing = store.list_syntheses(synthesis_type)\n",
    "\n",
    "if existing:\n",
    "    print(f\"Author model definition already exists: {existing[0]}\")\n",
    "    author_model_id = existing[0]\n",
    "else:\n",
    "    # Generate author model definition\n",
    "    config = AuthorModelDefinitionConfig(\n",
    "        implied_author_synthesis=implied_synth['output'],\n",
    "        decision_pattern_synthesis=decision_synth['output'],\n",
    "        textural_synthesis=textural_synth['output']\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "\n",
    "    print(\"Constructing author model definition...\")\n",
    "    response = llm.complete(prompt)\n",
    "\n",
    "    # Save with parent linkage (no direct sample contributions)\n",
    "    author_model_id = store.save_synthesis(\n",
    "        synthesis_type=synthesis_type,\n",
    "        output=response.content,\n",
    "        model=response.model,\n",
    "        sample_contributions=[],  # Inherits from parents\n",
    "        config=config,\n",
    "        parent_synthesis_id=implied_author_synthesis_id  # Link to one parent\n",
    "    )\n",
    "    print(f\"Author model definition saved: {author_model_id}\")\n",
    "\n",
    "# Display preview\n",
    "author_model = store.get_synthesis(author_model_id)\n",
    "print(f\"\\nTotal chars of Author Model: {len(author_model['output'])}\")\n",
    "print(f\"\\nAuthor Model Preview (first 800 chars):\\n{author_model['output'][:800]}...\")\n",
    "\n",
    "# Export to filesystem for use in generation\n",
    "output_dir = Path(\"outputs/author_modeling\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "author_model_path = output_dir / f\"{author_model_id}.txt\"\n",
    "store.export_synthesis(\n",
    "    synthesis_id=author_model_id,\n",
    "    output_path=author_model_path,\n",
    "    metadata_format='yaml'\n",
    ")\n",
    "print(f\"\\nAuthor model exported to: {author_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Stage 3: Field Guide Construction\n",
    "\n",
    "Integrate all three Stage 2 syntheses into a unified recognition field guide with:\n",
    "- Unified sensibility description\n",
    "- Recognition criteria (questions for evaluation)\n",
    "- Density rubric (1-5 scale)\n",
    "- Master passage index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the three syntheses\n",
    "implied_synth = store.get_synthesis(implied_author_synthesis_id)\n",
    "decision_synth = store.get_synthesis(decision_pattern_synthesis_id)\n",
    "textural_synth = store.get_synthesis(textural_synthesis_id)\n",
    "\n",
    "# Check if field guide already exists\n",
    "synthesis_type = FieldGuideConstructionConfig.synthesis_type()\n",
    "existing = store.list_syntheses(synthesis_type)\n",
    "\n",
    "if existing:\n",
    "    print(f\"Field guide already exists: {existing[0]}\")\n",
    "    field_guide_id = existing[0]\n",
    "else:\n",
    "    # Generate field guide\n",
    "    config = FieldGuideConstructionConfig(\n",
    "        implied_author_synthesis=implied_synth['output'],\n",
    "        decision_pattern_synthesis=decision_synth['output'],\n",
    "        textural_synthesis=textural_synth['output']\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    print(\"Constructing field guide...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    # Save with parent linkage (no direct sample contributions)\n",
    "    field_guide_id = store.save_synthesis(\n",
    "        synthesis_type=synthesis_type,\n",
    "        output=response.content,\n",
    "        model=response.model,\n",
    "        sample_contributions=[],  # Inherits from parents\n",
    "        config=config,\n",
    "        parent_synthesis_id=implied_author_synthesis_id  # Link to one parent\n",
    "    )\n",
    "    print(f\"Field guide saved: {field_guide_id}\")\n",
    "\n",
    "# Display preview\n",
    "field_guide = store.get_synthesis(field_guide_id)\n",
    "print(f\"\\nTotal chars of Field Guide:{len(field_guide['output'])}\")\n",
    "print(f\"\\nField Guide Preview (first 500 chars):\\n{field_guide['output'][:500]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Corpus Mining Preparation\n",
    "\n",
    "Extract passages from the corpus for evaluation in Stage 4.\n",
    "\n",
    "We can use several strategies:\n",
    "- **Paragraph windows**: Overlapping sliding windows\n",
    "- **Logical sections**: Section-based extraction\n",
    "- **Full samples**: Evaluate entire samples\n",
    "- **Specific indices**: Passages flagged in Stage 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Extract paragraph windows (blind sliding window)\n",
    "passages_windows = extract_paragraph_windows(\n",
    "    store,\n",
    "    sample_ids=None,\n",
    "    window_size=4,\n",
    "    overlap=2\n",
    ")\n",
    "print(f\"Strategy 1 (Sliding Windows): {len(passages_windows)} passages\")\n",
    "\n",
    "# Strategy 2: Extract logical sections\n",
    "passages_sections = extract_logical_sections(\n",
    "    store,\n",
    "    sample_ids=None,\n",
    "    min_length=200,\n",
    "    max_length=600\n",
    ")\n",
    "print(f\"Strategy 2 (Logical Sections): {len(passages_sections)} passages\")\n",
    "\n",
    "# Strategy 3: Extract nominated passages from Stage 1-2 analyses\n",
    "# These are passages that analysts flagged as signature moments or high-density examples\n",
    "passages_nominated = []\n",
    "\n",
    "for i in range(NUM_SAMPLES):\n",
    "    sample_id = f\"sample_{i+1:03d}\"\n",
    "    \n",
    "    # Extract from each analysis type\n",
    "    for analyst_config, analysis_type in [\n",
    "        (ImpliedAuthorConfig, \"implied_author\"),\n",
    "        (DecisionPatternConfig, \"decision_pattern\"),\n",
    "        (FunctionalTextureConfig, \"functional_texture\")\n",
    "    ]:\n",
    "        analysis = store.get_analysis(sample_id, analyst_config.analyst_name())\n",
    "        if analysis:\n",
    "            try:\n",
    "                nominated = extract_nominated_passages_from_analysis(\n",
    "                    store=store,\n",
    "                    llm=llm,\n",
    "                    sample_id=sample_id,\n",
    "                    analysis_text=analysis,\n",
    "                    analysis_type=analysis_type,\n",
    "                    padding_before=2,\n",
    "                    padding_after=2,\n",
    "                    min_confidence=0.6\n",
    "                )\n",
    "                passages_nominated.extend(nominated)\n",
    "                print(f\"  {sample_id}/{analysis_type}: {len(nominated)} nominated passages\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Warning: Failed to extract from {sample_id}/{analysis_type}: {e}\")\n",
    "\n",
    "print(f\"\\nStrategy 3 (Nominated from Analyses): {len(passages_nominated)} passages\")\n",
    "\n",
    "# Choose evaluation strategy\n",
    "EVALUATION_STRATEGY = \"nominated_only\"  # Change to \"combined\" or \"all\"\n",
    "\n",
    "if EVALUATION_STRATEGY == \"nominated_only\":\n",
    "    passages_to_evaluate = passages_nominated\n",
    "elif EVALUATION_STRATEGY == \"combined\":\n",
    "    passages_to_evaluate = passages_nominated + passages_windows[:5]\n",
    "else:  # \"all\"\n",
    "    passages_to_evaluate = passages_nominated + passages_windows[:5] + passages_sections[:5]\n",
    "\n",
    "print(f\"\\nUsing strategy: {EVALUATION_STRATEGY}\")\n",
    "print(f\"Will evaluate {len(passages_to_evaluate)} passages in Stage 4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION: Test quote extraction on sample_001\n",
    "print(\"=== QUOTE EXTRACTION VALIDATION ===\\n\")\n",
    "\n",
    "sample_id = \"sample_001\"\n",
    "analysis = store.get_analysis(sample_id, \"implied_author\")\n",
    "\n",
    "if analysis:\n",
    "    from belletrist.models.author_modeling_models import QuoteExtractionConfig\n",
    "    \n",
    "    config = QuoteExtractionConfig(\n",
    "        analysis_text=analysis,\n",
    "        analysis_type=\"implied_author\"\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    response = llm.complete_json(prompt)\n",
    "    quote_data = json.loads(response.content)\n",
    "    \n",
    "    print(f\"Extracted {len(quote_data['quotes'])} quotes from implied_author analysis\")\n",
    "    \n",
    "    # Test fuzzy matching on first quote\n",
    "    if quote_data['quotes']:\n",
    "        first_quote = quote_data['quotes'][0]['quote_text']\n",
    "        print(f\"\\nTesting fuzzy match for first quote:\")\n",
    "        print(f\"  Quote (first 80 chars): {first_quote[:80]}...\")\n",
    "        \n",
    "        result = find_passage_by_quote(store, sample_id, first_quote)\n",
    "        \n",
    "        if result.found:\n",
    "            print(f\"  ✓ Match found!\")\n",
    "            print(f\"  Confidence: {result.match_confidence:.2f}\")\n",
    "            print(f\"  Core range: {result.core_range}\")\n",
    "            print(f\"  Full range (with padding): {result.full_range}\")\n",
    "        else:\n",
    "            print(\"  ✗ Match failed\")\n",
    "else:\n",
    "    print(f\"No implied_author analysis found for {sample_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 4: Passage Evaluation\n",
    "\n",
    "Evaluate passages using the field guide to assign density ratings (1-5) and assess few-shot suitability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get field guide for evaluation\n",
    "field_guide = store.get_synthesis(field_guide_id)\n",
    "field_guide_text = field_guide['output']\n",
    "\n",
    "# Evaluate each passage\n",
    "evaluation_results = []\n",
    "\n",
    "for idx, passage in enumerate(passages_to_evaluate):\n",
    "    print(f\"Evaluating passage {idx+1}/{len(passages_to_evaluate)}...\")\n",
    "    \n",
    "    # Create passage identifier\n",
    "    passage_id = f\"{passage['source_sample_id']}_para_{passage['paragraph_range']}\"\n",
    "    \n",
    "    # Check if evaluation already exists\n",
    "    # (In production, you'd check ResultStore for existing evaluations)\n",
    "    \n",
    "    # Generate evaluation prompt\n",
    "    config = PassageEvaluationConfig(\n",
    "        field_guide=field_guide_text,\n",
    "        source=passage_id,\n",
    "        passage=passage['text']\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    # Run evaluation\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    # Parse evaluation response\n",
    "    try:\n",
    "        parsed = parse_passage_evaluation(response.content)\n",
    "        \n",
    "        if validate_passage_evaluation(parsed):\n",
    "            # Save to ResultStore\n",
    "            eval_id = store.save_passage_evaluation(\n",
    "                sample_id=passage['source_sample_id'],\n",
    "                density_rating=parsed['density_rating'],\n",
    "                task_coverage=parsed.get('task_coverage', ''),\n",
    "                teaching_value=parsed.get('teaching_value', ''),\n",
    "                recommendation=parsed['recommendation'],\n",
    "                model=response.model,\n",
    "                field_guide_id=field_guide_id,\n",
    "                paragraph_range=passage['paragraph_range']\n",
    "            )\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                'eval_id': eval_id,\n",
    "                'passage_id': passage_id,\n",
    "                'rating': parsed['density_rating'],\n",
    "                'recommended': parsed['recommendation']\n",
    "            })\n",
    "            \n",
    "            print(f\"  → Rating: {parsed['density_rating']}, Recommended: {parsed['recommendation']}\")\n",
    "        else:\n",
    "            print(f\"  → Validation failed for passage {idx+1}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"  → Error parsing evaluation: {e}\")\n",
    "\n",
    "print(f\"\\nStage 4 complete: Evaluated {len(evaluation_results)} passages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rating distribution\n",
    "rating_counts = {}\n",
    "for result in evaluation_results:\n",
    "    rating = result['rating']\n",
    "    rating_counts[rating] = rating_counts.get(rating, 0) + 1\n",
    "\n",
    "print(\"Rating Distribution:\")\n",
    "for rating in sorted(rating_counts.keys()):\n",
    "    print(f\"  Rating {rating}: {rating_counts[rating]} passages\")\n",
    "\n",
    "# Show high-density passages (4-5 rated)\n",
    "high_density = [r for r in evaluation_results if r['rating'] >= 4]\n",
    "print(f\"\\nHigh-density passages (4-5): {len(high_density)}\")\n",
    "for result in high_density:\n",
    "    print(f\"  {result['eval_id']}: Rating {result['rating']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stage 5: Example Set Construction\n",
    "\n",
    "Curate 3-4 high-density passages that balance:\n",
    "- **Density**: High ratings (4-5)\n",
    "- **Diversity**: Different compositional tasks/topics\n",
    "- **Complementarity**: Each adds something unique\n",
    "- **Coherence**: Together forming unified demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all passage evaluations for curation\n",
    "# (In production, query ResultStore for evaluations with ratings 4-5)\n",
    "high_density_evals = [r for r in evaluation_results if r['rating'] >= 4]\n",
    "\n",
    "if len(high_density_evals) < 3:\n",
    "    print(f\"Warning: Only {len(high_density_evals)} high-density passages found\")\n",
    "    print(\"Consider evaluating more passages or lowering density threshold\")\n",
    "else:\n",
    "    # Collect full evaluation texts for each high-density passage\n",
    "    passage_evaluations = []\n",
    "    for result in high_density_evals:\n",
    "        eval_data = store.get_passage_evaluation(result['eval_id'])\n",
    "        if eval_data:\n",
    "            # Format as evaluation text for the prompt\n",
    "            eval_text = f\"\"\"Passage ID: {result['eval_id']}\n",
    "Density Rating: {eval_data['density_rating']}\n",
    "Recommendation: {'YES' if eval_data['recommendation'] else 'NO'}\n",
    "\n",
    "Task Coverage:\n",
    "{eval_data.get('task_coverage', 'N/A')}\n",
    "\n",
    "Teaching Value:\n",
    "{eval_data.get('teaching_value', 'N/A')}\n",
    "\"\"\"\n",
    "            passage_evaluations.append(eval_text)\n",
    "    \n",
    "    # Generate example set curation prompt\n",
    "    config = ExampleSetConstructionConfig(\n",
    "        field_guide=field_guide_text,\n",
    "        passage_evaluations=passage_evaluations\n",
    "    )\n",
    "    prompt = prompt_maker.render(config)\n",
    "    \n",
    "    # Run curation\n",
    "    print(f\"Curating example set from {len(passage_evaluations)} high-density passages...\")\n",
    "    response = llm.complete(prompt)\n",
    "    \n",
    "    # Parse selected passages\n",
    "    try:\n",
    "        selected = parse_example_set_selection(response.content)\n",
    "        \n",
    "        if validate_example_set_selection(selected):\n",
    "            # Save example set to ResultStore\n",
    "            example_set_id = store.save_example_set(\n",
    "                field_guide_id=field_guide_id,\n",
    "                selection_rationale=response.content,  # Full curation response\n",
    "                model=response.model\n",
    "            )\n",
    "            \n",
    "            # Add member passages\n",
    "            for passage in selected:\n",
    "                store.add_example_set_member(\n",
    "                    example_set_id=example_set_id,\n",
    "                    evaluation_id=passage['passage_id'],\n",
    "                    unique_contribution=passage.get('unique_contribution', '')\n",
    "                )\n",
    "            \n",
    "            print(f\"\\nExample set saved: {example_set_id}\")\n",
    "            print(f\"Selected passages:\")\n",
    "            for p in selected:\n",
    "                print(f\"  - {p['passage_id']} (rating: {p.get('density_rating', 'N/A')})\")\n",
    "        else:\n",
    "            print(\"Validation failed: Example set does not meet criteria\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing example set selection: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export & Inspection\n",
    "\n",
    "Export field guide and example sets to filesystem for use in generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"outputs/author_modeling\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Export field guide\n",
    "field_guide_path = output_dir / f\"{field_guide_id}.txt\"\n",
    "store.export_synthesis(\n",
    "    synthesis_id=field_guide_id,\n",
    "    output_path=field_guide_path,\n",
    "    metadata_format='yaml'\n",
    ")\n",
    "print(f\"Field guide exported to: {field_guide_path}\")\n",
    "\n",
    "# Export example set (if exists)\n",
    "example_sets = store.list_example_sets()\n",
    "if example_sets:\n",
    "    for example_set_id in example_sets:\n",
    "        # Get example set with members\n",
    "        example_set = store.get_example_set_with_members(example_set_id)\n",
    "        \n",
    "        # Write to file\n",
    "        example_set_path = output_dir / f\"{example_set_id}.txt\"\n",
    "        with open(example_set_path, 'w') as f:\n",
    "            f.write(f\"# Example Set: {example_set_id}\\n\\n\")\n",
    "            f.write(f\"Field Guide: {example_set['field_guide_id']}\\n\")\n",
    "            f.write(f\"Created: {example_set['created_at']}\\n\")\n",
    "            f.write(f\"Model: {example_set['model']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"## Selection Rationale\\n\\n\")\n",
    "            f.write(example_set['selection_rationale'])\n",
    "            f.write(\"\\n\\n## Selected Passages\\n\\n\")\n",
    "            \n",
    "            for member in example_set['members']:\n",
    "                f.write(f\"### {member['evaluation_id']}\\n\\n\")\n",
    "                f.write(f\"**Unique Contribution:** {member['unique_contribution']}\\n\\n\")\n",
    "                \n",
    "                # Get actual passage text\n",
    "                eval_data = store.get_passage_evaluation(member['evaluation_id'])\n",
    "                if eval_data:\n",
    "                    sample = store.get_sample(eval_data['sample_id'])\n",
    "                    # Extract paragraph range from full sample\n",
    "                    # (simplified - in production, implement proper extraction)\n",
    "                    f.write(f\"**Source:** {eval_data['sample_id']}\\n\")\n",
    "                    f.write(f\"**Paragraph Range:** {eval_data.get('paragraph_range', 'N/A')}\\n\")\n",
    "                    f.write(f\"**Density Rating:** {eval_data['density_rating']}\\n\\n\")\n",
    "                f.write(\"---\\n\\n\")\n",
    "        \n",
    "        print(f\"Example set exported to: {example_set_path}\")\n",
    "else:\n",
    "    print(\"No example sets to export\")\n",
    "\n",
    "print(\"\\nExport complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Provenance Verification\n",
    "\n",
    "Verify full audit trail from samples through to example sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get field guide provenance\n",
    "print(\"Field Guide Provenance:\")\n",
    "provenance = store.get_synthesis_provenance(field_guide_id)\n",
    "pprint(provenance, depth=3)\n",
    "\n",
    "# Get example set provenance\n",
    "if example_sets:\n",
    "    print(f\"\\nExample Set {example_sets[0]} Provenance:\")\n",
    "    example_set = store.get_example_set_with_members(example_sets[0])\n",
    "    print(f\"  Field Guide: {example_set['field_guide_id']}\")\n",
    "    print(f\"  Members: {len(example_set['members'])}\")\n",
    "    for member in example_set['members']:\n",
    "        eval_data = store.get_passage_evaluation(member['evaluation_id'])\n",
    "        print(f\"    - {member['evaluation_id']} (from {eval_data['sample_id']})\")\n",
    "\n",
    "print(\"\\nProvenance verification complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook implemented a complete author modeling pipeline:\n",
    "\n",
    "1. ✅ **Stage 1**: Analyzed samples through three lenses (implied author, decision patterns, functional texture)\n",
    "2. ✅ **Stage 2**: Synthesized each dimension across samples\n",
    "3. ✅ **Stage 3**: Constructed unified field guide with recognition criteria and rubric\n",
    "4. ✅ **Stage 4**: Evaluated passages using field guide (density ratings 1-5)\n",
    "5. ✅ **Stage 5**: Curated 3-4 high-density passages as few-shot example set\n",
    "\n",
    "**Next Steps:**\n",
    "- Use exported field guide for understanding author's patterns\n",
    "- Use example set for few-shot learning in generation tasks\n",
    "- Compare few-shot results against baseline methods (Stage 6 - future work)\n",
    "\n",
    "All results stored in `russell_author_modeling.db` with full provenance tracking."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
