{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce4c8b8",
   "metadata": {},
   "source": [
    "# Multi-Analyst Text Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates the full pipeline for analyzing text through multiple specialist lenses (rhetorician, syntactician, lexicologist, etc.) and synthesizing their observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c60182",
   "metadata": {},
   "source": [
    "## Installations and Preparations\n",
    "First, external modules are installed and ensured to be in working order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf10fd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:22.033495Z",
     "start_time": "2025-11-19T18:02:20.240014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm==1.79.3 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.79.3)\n",
      "Requirement already satisfied: pydantic==2.7.4 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.7.4)\n",
      "Requirement already satisfied: jinja2>=3.1.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: aiohttp>=3.10 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (3.13.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.99.5 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.22.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2>=3.1.0->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./venv/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.29.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./venv/lib/python3.11/site-packages (from tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.1.4)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: shellingham in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (0.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Optional: Install requirements if running in a fresh kernel\n",
    "# Uncomment if needed:\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Or install individual packages:\n",
    "# !pip install litellm pydantic jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27d2ef48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providers\n",
      "=========\n",
      "* openai\n",
      "* openai_like\n",
      "* bytez\n",
      "* xai\n",
      "* custom_openai\n",
      "* text-completion-openai\n",
      "* cohere\n",
      "* cohere_chat\n",
      "* clarifai\n",
      "* anthropic\n",
      "* anthropic_text\n",
      "* replicate\n",
      "* huggingface\n",
      "* together_ai\n",
      "* datarobot\n",
      "* openrouter\n",
      "* cometapi\n",
      "* vertex_ai\n",
      "* vertex_ai_beta\n",
      "* gemini\n",
      "* ai21\n",
      "* baseten\n",
      "* azure\n",
      "* azure_text\n",
      "* azure_ai\n",
      "* sagemaker\n",
      "* sagemaker_chat\n",
      "* bedrock\n",
      "* vllm\n",
      "* nlp_cloud\n",
      "* petals\n",
      "* oobabooga\n",
      "* ollama\n",
      "* ollama_chat\n",
      "* deepinfra\n",
      "* perplexity\n",
      "* mistral\n",
      "* groq\n",
      "* nvidia_nim\n",
      "* cerebras\n",
      "* baseten\n",
      "* ai21_chat\n",
      "* volcengine\n",
      "* codestral\n",
      "* text-completion-codestral\n",
      "* deepseek\n",
      "* sambanova\n",
      "* maritalk\n",
      "* cloudflare\n",
      "* fireworks_ai\n",
      "* friendliai\n",
      "* watsonx\n",
      "* watsonx_text\n",
      "* triton\n",
      "* predibase\n",
      "* databricks\n",
      "* empower\n",
      "* github\n",
      "* custom\n",
      "* litellm_proxy\n",
      "* hosted_vllm\n",
      "* llamafile\n",
      "* lm_studio\n",
      "* galadriel\n",
      "* gradient_ai\n",
      "* github_copilot\n",
      "* novita\n",
      "* meta_llama\n",
      "* featherless_ai\n",
      "* nscale\n",
      "* nebius\n",
      "* dashscope\n",
      "* moonshot\n",
      "* v0\n",
      "* heroku\n",
      "* oci\n",
      "* morph\n",
      "* lambda_ai\n",
      "* vercel_ai_gateway\n",
      "* wandb\n",
      "* ovhcloud\n",
      "* lemonade\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f809efe6",
   "metadata": {},
   "source": [
    "## Initialize Base Objects\n",
    "Set up connections to a Large Language Model provider via `litellm` model router. Also, setup up tools to retrieve text data to be part of the context window, that is, instruction prompts and texts to analyze. A basic result storage is also initialized.\n",
    "\n",
    "The LLM to use is set by the `model_string`, which is constructed as `<provider>/<model>`, the providers defined by the `litellm` package, see in particular `litellm.LITELLM_CHAT_PROVIDERS`. The API key to the provider should be stored in an environment variable with name defined in `model_provider_api_key_env_var`. Do **not** store the API key as a string variable directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8a53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_string = 'mistral/mistral-large-2411'\n",
    "model_provider_api_key_env_var = 'MISTRAL_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9284ffea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:29.927627Z",
     "start_time": "2025-11-19T18:02:29.858316Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from belletrist import LLM, LLMConfig, PromptMaker, DataSampler, ResultStore\n",
    "\n",
    "llm = LLM(LLMConfig(\n",
    "    model=model_string,\n",
    "    api_key=os.environ.get(model_provider_api_key_env_var)\n",
    "))\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler()\n",
    "store = ResultStore(Path(f\"{os.getcwd()}/belletrist_storage.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d46bfc",
   "metadata": {},
   "source": [
    "In case a clean run is done, the old contents of the database are discarded with a result store reset. Do not run the rest if content should be preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c244580",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e774277f",
   "metadata": {},
   "source": [
    "## Generate and Store Text Samples to be Analyzed\n",
    "\n",
    "A random text sample is taken from the data corpus and stored with full provenance (which file, which paragraphs). Each sample is an instance of `TextSegment`.\n",
    "\n",
    "The sample size is set by the variable `n_sample` and each sample comprises `m_paragraphs_per_sample` number of consecutive paragraphs.\n",
    "\n",
    "If non-random text samples are preferred, use the `get_paragraph_chunk` method of the `DataSampler` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9719fcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text source: /Users/andersohrn/PycharmProjects/russell_writes/data/russell/mysticism_and_logic_and_other_essays.txt\n",
      "Paragraph range: 365 - 369\n",
      "\n",
      "Consider for example the infinite divisibility of matter. In looking\n",
      "at a given thing and approaching it, one sense-datum will become\n",
      "several, and each of these will again divide. Thus _one_ appearance\n",
      "may represent _many_ things, and to this process there seems no end.\n",
      "Hence in the limit, when we approach indefinitely near to the thing\n",
      "there will be an indefinite number of units of matter corresponding to\n",
      "what, at a finite distance, is only one appearance. This is how\n",
      "infinite divisibility arises.\n",
      "\n",
      "The whole causal efficacy of a thing resides in its matter. This is in\n",
      "some sense an empirical fact, but it would be hard to state it\n",
      "precisely, because \"causal efficacy\" is difficult to define.\n",
      "\n",
      "What can be known empirically about the matter of a thing is only\n",
      "approximate, because we cannot get to know the appearances of the\n",
      "thing from very small distances, and cannot accurately infer the limit\n",
      "of these appearances. But it _is_ inferred _approximately_ by means of\n",
      "the appearances we can observe. It then turns out that these\n",
      "appearances can be exhibited by physics as a function of the matter\n",
      "in our immediate neighbourhood; e.g. the visual appearance of a\n",
      "distant object is a function of the light-waves that reach the eyes.\n",
      "This leads to confusions of thought, but offers no real difficulty.\n",
      "\n",
      "One appearance, of a visible object for example, is not sufficient to\n",
      "determine its other simultaneous appearances, although it goes a\n",
      "certain distance towards determining them. The determination of the\n",
      "hidden structure of a thing, so far as it is possible at all, can only\n",
      "be effected by means of elaborate dynamical inferences.\n"
     ]
    }
   ],
   "source": [
    "text_sample = sampler.sample_segment(p_length=4)\n",
    "print(f'Text source: {text_sample.file_path}')\n",
    "print(f'Paragraph range: {text_sample.paragraph_start} - {text_sample.paragraph_end}')\n",
    "print(f'\\n{text_sample.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "213df4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 1\n",
    "m_paragraphs_per_sample = 10\n",
    "\n",
    "for _ in range(n_sample):\n",
    "    sample_id = f'sample_{len(store.list_samples()) + 1:03d}'\n",
    "    segment = sampler.sample_segment(p_length=m_paragraphs_per_sample)\n",
    "    store.save_segment(sample_id, segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e64fe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys:\n",
      "============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sample_001']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sample keys:\\n============')\n",
    "store.list_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2055c7",
   "metadata": {},
   "source": [
    "## Construct the Analyst Agents and Analyze\n",
    "\n",
    "Send the text samples through each specialist analyst. Each produces an independent analysis from their domain expertise.\n",
    "\n",
    "**Prompt structure for caching optimization:**\n",
    "1. Preamble instruction (static)\n",
    "2. Analyst-specific template (static per analyst)\n",
    "3. Text to analyze (dynamic)\n",
    "\n",
    "Note that the execution of this can take time since it involves invoking LLMs. These are however independent analyses, and can therefore in principle be run in parallel, though the implementation below does not utilize that fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d4c490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist.models import (\n",
    "    PreambleInstructionConfig,\n",
    "    PreambleTextConfig,\n",
    "    RhetoricianConfig,\n",
    "    SyntacticianConfig,\n",
    "    LexicologistConfig,\n",
    "    InformationArchitectConfig,\n",
    "    EfficiencyAuditorConfig,\n",
    "    CrossPerspectiveIntegratorConfig,\n",
    ")\n",
    "ANALYSTS = [\"rhetorician\", \"syntactician\", \"lexicologist\", \"information_architect\", \"efficiency_auditor\"]\n",
    "ANALYST_CONFIGS = {\n",
    "    \"rhetorician\": RhetoricianConfig,\n",
    "    \"syntactician\": SyntacticianConfig,\n",
    "    \"lexicologist\": LexicologistConfig,\n",
    "    \"information_architect\": InformationArchitectConfig,\n",
    "    \"efficiency_auditor\": EfficiencyAuditorConfig,\n",
    "}\n",
    "\n",
    "def build_analyst_prompt(preamble_instruction: str, analyst_prompt: str, preamble_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to construct the full prompt for an analyst.\n",
    "    \n",
    "    \"\"\"\n",
    "    return f\"{preamble_instruction}\\n\\n{analyst_prompt}\\n\\n{preamble_text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b7e43e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 samples with 5 analysts each\n",
      "\n",
      "Sample: sample_001\n",
      "  Running rhetorician... ✓ (3612 chars)\n",
      "  Running syntactician... ✓ (8566 chars)\n",
      "  Running lexicologist... ✓ (5885 chars)\n",
      "  Running information_architect... ✓ (9118 chars)\n",
      "  Running efficiency_auditor... ✓ (5032 chars)\n",
      "\n",
      "All analyses complete for 1 samples\n"
     ]
    }
   ],
   "source": [
    "# Get all samples from the store\n",
    "all_samples = store.list_samples()\n",
    "print(f\"Processing {len(all_samples)} samples with {len(ANALYSTS)} analysts each\\n\")\n",
    "\n",
    "# Outer loop: iterate over each text sample\n",
    "for sample_id in all_samples:\n",
    "    print(f\"Sample: {sample_id}\")\n",
    "    \n",
    "    # Get the sample text\n",
    "    sample = store.get_sample(sample_id)\n",
    "    text = sample['text']\n",
    "    \n",
    "    # Build shared prompt components (reused across all analysts for this sample)\n",
    "    preamble_instruction = prompt_maker.render(PreambleInstructionConfig())\n",
    "    preamble_text = prompt_maker.render(PreambleTextConfig(text_to_analyze=text))\n",
    "    \n",
    "    # Inner loop: run each analyst on this sample\n",
    "    for analyst_name in ANALYSTS:\n",
    "        print(f\"  Running {analyst_name}...\", end=\" \")\n",
    "        \n",
    "        # Get analyst-specific prompt using the config class\n",
    "        analyst_config = ANALYST_CONFIGS[analyst_name]()\n",
    "        analyst_prompt = prompt_maker.render(analyst_config)\n",
    "        \n",
    "        # Build full prompt using helper function\n",
    "        full_prompt = build_analyst_prompt(preamble_instruction, analyst_prompt, preamble_text)\n",
    "        \n",
    "        # Run analysis\n",
    "        response = llm.complete(full_prompt)\n",
    "        store.save_analysis(sample_id, analyst_name, response.content, response.model)\n",
    "        \n",
    "        print(f\"✓ ({len(response.content)} chars)\")\n",
    "    \n",
    "    print()  # Blank line between samples\n",
    "\n",
    "print(f\"All analyses complete for {len(all_samples)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26552f6d",
   "metadata": {},
   "source": [
    "Then verify that analysis was run as expected and yielded analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75208dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete: True\n",
      "\n",
      "Sample: sample_001\n",
      "Source: File 0, paragraphs 9-19\n",
      "Analyses available: ['efficiency_auditor', 'information_architect', 'lexicologist', 'rhetorician', 'syntactician']\n",
      "\n",
      "--- Rhetorician Output (first 500 chars) ---\n",
      "Based on the provided text, here's a rhetorical analysis focusing on rhetorical strategy and stance:\n",
      "\n",
      "1. **WRITER'S POSITION**\n",
      "\n",
      "   - **Persona**: The writer emerges as an authority in the field, with a formal, academic tone. The use of specialized terms (_à priori_, _apodeictic_, _hypothetical_, _categorical_, etc.) and references to prominent figures (Kant, Bradley) and scholarly works (_Prolegomena_, _Pure Reason_) establishes their credentials.\n",
      "   - **Relationship to subject matter**: The wri\n"
     ]
    }
   ],
   "source": [
    "sample_id = 'sample_001'\n",
    "is_complete = store.is_complete(sample_id, ANALYSTS)\n",
    "print(f\"Analysis complete: {is_complete}\")\n",
    "\n",
    "# Retrieve sample and all analyses (both are now dicts)\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "print(f\"\\nSample: {sample['sample_id']}\")\n",
    "print(f\"Source: File {sample['file_index']}, paragraphs {sample['paragraph_start']}-{sample['paragraph_end']}\")\n",
    "print(f\"Analyses available: {list(analyses.keys())}\")\n",
    "\n",
    "# Examine one analysis\n",
    "print(f\"\\n--- Rhetorician Output (first 500 chars) ---\")\n",
    "print(analyses.get(\"rhetorician\", \"Not found\")[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad554fc",
   "metadata": {},
   "source": [
    "## Pattern Recognition (Cross-Perspective Integration)\n",
    "\n",
    "Synthesize all analyst perspectives to identify interactions, tensions, and load-bearing features. This is a per-text-cross-analyst transformation where a unit of writing patterns for each text sample is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "440bea59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_001']\n"
     ]
    }
   ],
   "source": [
    "samples_to_analyze = store.list_samples()[0:1]\n",
    "print(samples_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af5ee056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pattern_prompt_from_(text: str, analyses: dict):\n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "    analyst_info = {}\n",
    "    for analyst_name in ANALYSTS:\n",
    "        config_class = ANALYST_CONFIGS[analyst_name]\n",
    "        analyst_info[analyst_name] = {\n",
    "            'analysis': analyses[analyst_name],\n",
    "            'analyst_descr_short': config_class.description()\n",
    "        }\n",
    "\n",
    "    pattern_config = CrossPerspectiveIntegratorConfig(\n",
    "        original_text=sample['text'],\n",
    "        analysts=analyst_info\n",
    "    )\n",
    "    return prompt_maker.render(pattern_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e436bf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pattern recognizer for sample_001... ✓ (11791 chars)\n"
     ]
    }
   ],
   "source": [
    "for sample_id in samples_to_analyze:\n",
    "    print(f\"Running pattern recognizer for {sample_id}...\", end=\" \")\n",
    "    \n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "    pattern_prompt = build_pattern_prompt_from_(text=sample['text'], analyses=analyses)\n",
    "\n",
    "    pattern_response = llm.complete(pattern_prompt)\n",
    "    \n",
    "    # Store pattern recognition result in result store\n",
    "    store.save_analysis(\n",
    "        sample_id, \n",
    "        CrossPerspectiveIntegratorConfig.analyst_name(), \n",
    "        pattern_response.content, \n",
    "        pattern_response.model\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ ({len(pattern_response.content)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6078de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cross_perspective_integrator', 'efficiency_auditor', 'information_architect', 'lexicologist', 'rhetorician', 'syntactician'])\n",
      "\n",
      "--- Pattern Analyst Output (first 2000 chars) ---\n",
      "Not found\n"
     ]
    }
   ],
   "source": [
    "sample_id = 'sample_001'\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "print(analyses.keys())\n",
    "\n",
    "print(f\"\\n--- Pattern Analyst Output (first 2000 chars) ---\")\n",
    "print(analyses.get(\"pattern_recognizer\", \"Not found\")[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394634b5",
   "metadata": {},
   "source": "## Stage 2: Cross-Text Synthesis\n\nNow we synthesize patterns across multiple text analyses. This stage takes all the pattern recognition outputs (cross-perspective integrations from Stage 1) and identifies:\n- Recurring patterns across texts\n- Context-dependent variations\n- Hierarchies of importance\n- Generalizable principles\n\nThis produces a single synthesis document that IS stored in the ResultStore with auto-generated ID and full provenance tracking."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9173de",
   "metadata": {},
   "outputs": [],
   "source": "from belletrist.models import CrossTextSynthesizerConfig\n\n# Get all samples that have pattern recognition results\nall_samples = store.list_samples()\npattern_analyst = CrossPerspectiveIntegratorConfig.analyst_name()\n\n# Retrieve all pattern recognition analyses\nintegrated_analyses = {}\nfor sample_id in all_samples:\n    pattern_analysis = store.get_analysis(sample_id, pattern_analyst)\n    if pattern_analysis:\n        integrated_analyses[sample_id] = pattern_analysis\n    else:\n        print(f\"⚠ Sample {sample_id} missing pattern recognition analysis\")\n\nprint(f\"Found {len(integrated_analyses)} pattern recognition analyses\")\nprint(f\"Sample IDs: {list(integrated_analyses.keys())}\")\n\n# Only proceed if we have at least 2 (required for cross-text synthesis)\nif len(integrated_analyses) >= 2:\n    # Build cross-text synthesis config and prompt\n    cross_text_config = CrossTextSynthesizerConfig(\n        integrated_analyses=integrated_analyses\n    )\n    cross_text_prompt = prompt_maker.render(cross_text_config)\n    \n    # Run cross-text synthesis\n    print(\"\\nRunning cross-text synthesis...\", end=\" \")\n    cross_text_response = llm.complete(cross_text_prompt)\n    print(f\"✓ ({len(cross_text_response.content)} chars)\")\n    \n    # Save to ResultStore with auto-generated ID and full provenance\n    sample_contributions = [(sid, pattern_analyst) for sid in integrated_analyses.keys()]\n    cross_text_id = store.save_synthesis(\n        synthesis_type=cross_text_config.synthesis_type(),\n        output=cross_text_response.content,\n        model=cross_text_response.model,\n        sample_contributions=sample_contributions,\n        config=cross_text_config\n    )\n    \n    print(f\"Saved as: {cross_text_id}\")\n    \n    # Display first part\n    print(\"\\n--- Cross-Text Synthesis (first 1000 chars) ---\")\n    print(cross_text_response.content[:1000])\nelse:\n    print(f\"\\n⚠ Need at least 2 pattern recognition analyses for cross-text synthesis. Got {len(integrated_analyses)}.\")"
  },
  {
   "cell_type": "markdown",
   "id": "i1qrcrh4nic",
   "source": "## Stage 3: Synthesizer of Principles\n\nThe final stage converts the descriptive cross-text synthesis into actionable prescriptive writing principles. This generates a style guide that can be used to instruct an LLM to write in a similar style.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "qr0c2j7oc7j",
   "source": "from belletrist.models import SynthesizerOfPrinciplesConfig\n\n# Get the latest cross-text synthesis\ncross_text_syntheses = store.list_syntheses('cross_text_synthesis')\nif cross_text_syntheses:\n    # Use the most recent cross-text synthesis\n    latest_cross_text_id = cross_text_syntheses[-1]\n    cross_text_synthesis = store.get_synthesis(latest_cross_text_id)\n    \n    print(f\"Using cross-text synthesis: {latest_cross_text_id}\")\n    \n    # Build principles guide config and prompt\n    principles_config = SynthesizerOfPrinciplesConfig(\n        synthesis_document=cross_text_synthesis['output']\n    )\n    principles_prompt = prompt_maker.render(principles_config)\n    \n    # Run principles synthesis\n    print(\"Running principles synthesizer...\", end=\" \")\n    principles_response = llm.complete(principles_prompt)\n    print(f\"✓ ({len(principles_response.content)} chars)\")\n    \n    # Save to ResultStore with parent linkage (inherits provenance)\n    principles_id = store.save_synthesis(\n        synthesis_type=principles_config.synthesis_type(),\n        output=principles_response.content,\n        model=principles_response.model,\n        sample_contributions=[],  # Inherited from parent\n        config=principles_config,\n        parent_synthesis_id=latest_cross_text_id\n    )\n    \n    print(f\"Saved as: {principles_id}\")\n    \n    # Display first part\n    print(\"\\n--- Principles Guide (first 1000 chars) ---\")\n    print(principles_response.content[:1000])\nelse:\n    print(\"⚠ No cross-text synthesis found. Run Stage 2 first.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "uj0yzchfdee",
   "source": "## Querying Synthesis Metadata\n\nThe ResultStore tracks full provenance for all syntheses. Query metadata to understand what samples, analysts, and models contributed to each synthesis.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3he69dhwx7a",
   "source": "# List all syntheses\nprint(\"All Syntheses\")\nprint(\"=\" * 50)\nfor synth_type in ['cross_text_synthesis', 'principles_guide']:\n    syntheses = store.list_syntheses(synth_type)\n    print(f\"\\n{synth_type}: {len(syntheses)} found\")\n    for synth_id in syntheses:\n        print(f\"  - {synth_id}\")\n\n# Get detailed metadata for a synthesis\nif store.list_syntheses():\n    print(\"\\n\\nDetailed Metadata Example\")\n    print(\"=\" * 50)\n    \n    # Get first principles guide (or first cross-text if none)\n    principles = store.list_syntheses('principles_guide')\n    if principles:\n        synth_id = principles[0]\n    else:\n        synth_id = store.list_syntheses()[0]\n    \n    synth_with_meta = store.get_synthesis_with_metadata(synth_id)\n    \n    print(f\"\\nSynthesis ID: {synth_with_meta['synthesis_id']}\")\n    print(f\"Type: {synth_with_meta['type']}\")\n    print(f\"Model: {synth_with_meta['model']}\")\n    print(f\"Created: {synth_with_meta['created_at']}\")\n    print(f\"Parent: {synth_with_meta['parent_id']}\")\n    \n    if synth_with_meta.get('metadata'):\n        meta = synth_with_meta['metadata']\n        print(f\"\\nMetadata:\")\n        print(f\"  Samples: {meta['num_samples']}\")\n        print(f\"  Sample IDs: {meta['sample_ids']}\")\n        print(f\"  Model homogeneous: {meta['is_homogeneous_model']}\")\n        print(f\"  Models used: {meta['models_used']}\")\n\n# Get full provenance tree\nif store.list_syntheses('principles_guide'):\n    print(\"\\n\\nFull Provenance Tree\")\n    print(\"=\" * 50)\n    \n    principles_id = store.list_syntheses('principles_guide')[0]\n    provenance = store.get_synthesis_provenance(principles_id)\n    \n    print(f\"\\nPrinciples Guide: {provenance['synthesis']['synthesis_id']}\")\n    print(f\"  Created: {provenance['synthesis']['created_at']}\")\n    print(f\"  Model: {provenance['synthesis']['model']}\")\n    \n    if provenance['parent']:\n        parent = provenance['parent']\n        print(f\"\\n  Parent (Cross-Text): {parent['synthesis']['synthesis_id']}\")\n        print(f\"    Sample contributions: {len(parent['sample_contributions'])}\")\n        for sample_id, analyst in parent['sample_contributions'][:3]:\n            print(f\"      - {sample_id} / {analyst}\")\n        if len(parent['sample_contributions']) > 3:\n            print(f\"      ... and {len(parent['sample_contributions']) - 3} more\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4u09k0dvr03",
   "source": "## Exporting Syntheses to Filesystem\n\nExport final syntheses to text files with YAML metadata headers for consumption by other tools or LLMs.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mvynm0crvpk",
   "source": "# Create outputs directory\noutputs_dir = Path(\"outputs\")\noutputs_dir.mkdir(exist_ok=True)\n\n# Export cross-text synthesis\ncross_text_syntheses = store.list_syntheses('cross_text_synthesis')\nif cross_text_syntheses:\n    for synth_id in cross_text_syntheses:\n        output_path = outputs_dir / f\"{synth_id}.txt\"\n        store.export_synthesis(synth_id, output_path, metadata_format='yaml')\n        print(f\"Exported: {output_path}\")\n\n# Export principles guide\nprinciples_guides = store.list_syntheses('principles_guide')\nif principles_guides:\n    for synth_id in principles_guides:\n        output_path = outputs_dir / f\"{synth_id}.txt\"\n        store.export_synthesis(synth_id, output_path, metadata_format='yaml')\n        print(f\"Exported: {output_path}\")\n        \n        # Also create a special \"derived_style_instructions.txt\" for style_evaluation.ipynb\n        if synth_id == principles_guides[-1]:  # Use latest\n            instructions_path = outputs_dir / \"derived_style_instructions.txt\"\n            store.export_synthesis(synth_id, instructions_path, metadata_format='yaml')\n            print(f\"Exported for style evaluation: {instructions_path}\")\n\nprint(f\"\\nAll syntheses exported to {outputs_dir.absolute()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "87084aec",
   "metadata": {},
   "source": [
    "## Utilities: Working with Stored Samples\n",
    "\n",
    "Helper functions for browsing and managing stored results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all samples in the database\n",
    "all_samples = store.list_samples()\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "print(f\"Sample IDs: {all_samples}\")\n",
    "\n",
    "# Check completion status for each\n",
    "print(\"\\nCompletion status:\")\n",
    "for sid in all_samples:\n",
    "    complete = store.is_complete(sid, ANALYSTS)\n",
    "    status = \"✓\" if complete else \"✗\"\n",
    "    print(f\"  {status} {sid}\")\n",
    "\n",
    "# Close database connection when done\n",
    "# store.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}