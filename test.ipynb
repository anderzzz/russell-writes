{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea16adf",
   "metadata": {},
   "source": [
    "# Multi-Analyst Text Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates the full pipeline for analyzing text through multiple specialist lenses (rhetorician, syntactician, lexicologist, etc.) and synthesizing their observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ed5bd2",
   "metadata": {},
   "source": [
    "## Installations and Preparations\n",
    "First, external modules are installed and ensured to be in working order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104a7781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:22.033495Z",
     "start_time": "2025-11-19T18:02:20.240014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm==1.79.3 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.79.3)\n",
      "Requirement already satisfied: pydantic==2.7.4 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.7.4)\n",
      "Requirement already satisfied: jinja2>=3.1.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: aiohttp>=3.10 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (3.13.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.99.5 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.22.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2>=3.1.0->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./venv/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.29.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./venv/lib/python3.11/site-packages (from tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.1.4)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: shellingham in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (0.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Optional: Install requirements if running in a fresh kernel\n",
    "# Uncomment if needed:\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Or install individual packages:\n",
    "# !pip install litellm pydantic jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "723cf303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providers\n",
      "=========\n",
      "* openai\n",
      "* openai_like\n",
      "* bytez\n",
      "* xai\n",
      "* custom_openai\n",
      "* text-completion-openai\n",
      "* cohere\n",
      "* cohere_chat\n",
      "* clarifai\n",
      "* anthropic\n",
      "* anthropic_text\n",
      "* replicate\n",
      "* huggingface\n",
      "* together_ai\n",
      "* datarobot\n",
      "* openrouter\n",
      "* cometapi\n",
      "* vertex_ai\n",
      "* vertex_ai_beta\n",
      "* gemini\n",
      "* ai21\n",
      "* baseten\n",
      "* azure\n",
      "* azure_text\n",
      "* azure_ai\n",
      "* sagemaker\n",
      "* sagemaker_chat\n",
      "* bedrock\n",
      "* vllm\n",
      "* nlp_cloud\n",
      "* petals\n",
      "* oobabooga\n",
      "* ollama\n",
      "* ollama_chat\n",
      "* deepinfra\n",
      "* perplexity\n",
      "* mistral\n",
      "* groq\n",
      "* nvidia_nim\n",
      "* cerebras\n",
      "* baseten\n",
      "* ai21_chat\n",
      "* volcengine\n",
      "* codestral\n",
      "* text-completion-codestral\n",
      "* deepseek\n",
      "* sambanova\n",
      "* maritalk\n",
      "* cloudflare\n",
      "* fireworks_ai\n",
      "* friendliai\n",
      "* watsonx\n",
      "* watsonx_text\n",
      "* triton\n",
      "* predibase\n",
      "* databricks\n",
      "* empower\n",
      "* github\n",
      "* custom\n",
      "* litellm_proxy\n",
      "* hosted_vllm\n",
      "* llamafile\n",
      "* lm_studio\n",
      "* galadriel\n",
      "* gradient_ai\n",
      "* github_copilot\n",
      "* novita\n",
      "* meta_llama\n",
      "* featherless_ai\n",
      "* nscale\n",
      "* nebius\n",
      "* dashscope\n",
      "* moonshot\n",
      "* v0\n",
      "* heroku\n",
      "* oci\n",
      "* morph\n",
      "* lambda_ai\n",
      "* vercel_ai_gateway\n",
      "* wandb\n",
      "* ovhcloud\n",
      "* lemonade\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ef32a7",
   "metadata": {},
   "source": [
    "## Initialize Base Objects\n",
    "Set up connections to a Large Language Model provider via `litellm` model router. Also, setup up tools to retrieve text data to be part of the context window, that is, instruction prompts and texts to analyze. A basic result storage is also initialized.\n",
    "\n",
    "The LLM to use is set by the `model_string`, which is constructed as `<provider>/<model>`, the providers defined by the `litellm` package, see in particular `litellm.LITELLM_CHAT_PROVIDERS`. The API key to the provider should be stored in an environment variable with name defined in `model_provider_api_key_env_var`. Do **not** store the API key as a string variable directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "413c7287",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_string = 'mistral/mistral-large-2411'\n",
    "model_provider_api_key_env_var = 'MISTRAL_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d83e447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:29.927627Z",
     "start_time": "2025-11-19T18:02:29.858316Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from belletrist import LLM, LLMConfig, PromptMaker, DataSampler, ResultStore\n",
    "from belletrist.models import (\n",
    "    PreambleInstructionConfig,\n",
    "    PreambleTextConfig,\n",
    "    RhetoricianConfig,\n",
    "    SyntacticianConfig,\n",
    "    LexicologistConfig,\n",
    "    InformationArchitectConfig,\n",
    "    EfficiencyAuditorConfig,\n",
    "    PatternRecognizerTextConfig,\n",
    ")\n",
    "\n",
    "llm = LLM(LLMConfig(\n",
    "    model=model_string,\n",
    "    api_key=os.environ.get(model_provider_api_key_env_var)\n",
    "))\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler()\n",
    "store = ResultStore(Path(f\"{os.getcwd()}/belletrist_storage.db\"))\n",
    "store.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35d254",
   "metadata": {},
   "source": [
    "## Generate and Store Text Samples to be Analyzed\n",
    "\n",
    "A random text sample is taken from the data corpus and stored with full provenance (which file, which paragraphs). Each sample is an instance of `TextSegment`.\n",
    "\n",
    "The sample size is set by the variable `n_sample` and each sample comprises `m_paragraphs_per_sample` number of consecutive paragraphs.\n",
    "\n",
    "If non-random text samples are preferred, use the `get_paragraph_chunk` method of the `DataSampler` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1772ee71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text source: /Users/andersohrn/PycharmProjects/russell_writes/data/russell/mysticism_and_logic_and_other_essays.txt\n",
      "Paragraph range: 434 - 438\n",
      "\n",
      "And Bergson, who has rightly perceived that the law as stated by\n",
      "philosophers is worthless, nevertheless continues to suppose that it\n",
      "is used in science. Thus he says:--\n",
      "\n",
      "\"Now, it is argued, this law [the law of causality] means that every\n",
      "phenomenon is determined by its conditions, or, in other words, that\n",
      "the same causes produce the same effects.\"[37]\n",
      "\n",
      "And again:--\n",
      "\n",
      "\"We perceive physical phenomena, and these phenomena obey laws. This\n",
      "means: (1) That phenomena _a_, _b_, _c_, _d_, previously perceived,\n",
      "can occur again in the same shape; (2) that a certain phenomenon P,\n",
      "which appeared after the conditions _a_, _b_, _c_, _d_, and after\n",
      "these conditions only, will not fail to recur as soon as the same\n",
      "conditions are again present.\"[38]\n"
     ]
    }
   ],
   "source": [
    "text_sample = sampler.sample_segment(p_length=4)\n",
    "print(f'Text source: {text_sample.file_path}')\n",
    "print(f'Paragraph range: {text_sample.paragraph_start} - {text_sample.paragraph_end}')\n",
    "print(f'\\n{text_sample.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afc957a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 10\n",
    "m_paragraphs_per_sample = 10\n",
    "\n",
    "for _ in range(n_sample):\n",
    "    sample_id = f'sample_{len(store.list_samples()) + 1:03d}'\n",
    "    segment = sampler.sample_segment(p_length=m_paragraphs_per_sample)\n",
    "    store.save_segment(sample_id, segment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82670e",
   "metadata": {},
   "source": [
    "## Step 2: Run Multi-Analyst Pipeline\n",
    "\n",
    "Send the text through each specialist analyst. Each produces an independent analysis from their domain expertise.\n",
    "\n",
    "**Prompt structure for caching optimization:**\n",
    "1. Preamble instruction (static)\n",
    "2. Analyst-specific template (static per analyst)\n",
    "3. Text to analyze (dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98ee536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sample text\n",
    "sample = store.get_sample(sample_id)\n",
    "text = sample['text']  # Now returns dict, access via keys\n",
    "\n",
    "# Build shared prompt components (reused across all analysts)\n",
    "preamble_instruction = prompt_maker.render(PreambleInstructionConfig())\n",
    "preamble_text = prompt_maker.render(PreambleTextConfig(text_to_analyze=text))\n",
    "\n",
    "# --- RHETORICIAN ---\n",
    "print(\"Running rhetorician...\", end=\" \")\n",
    "rhetorician_prompt = prompt_maker.render(RhetoricianConfig())  # All sections enabled by default\n",
    "full_prompt = f\"{preamble_instruction}\\n\\n{rhetorician_prompt}\\n\\n{preamble_text}\"\n",
    "response = llm.complete(full_prompt)\n",
    "store.save_analysis(sample_id, \"rhetorician\", response.content, response.model)\n",
    "print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "# --- SYNTACTICIAN ---\n",
    "print(\"Running syntactician...\", end=\" \")\n",
    "syntactician_prompt = prompt_maker.render(SyntacticianConfig())\n",
    "full_prompt = f\"{preamble_instruction}\\n\\n{syntactician_prompt}\\n\\n{preamble_text}\"\n",
    "response = llm.complete(full_prompt)\n",
    "store.save_analysis(sample_id, \"syntactician\", response.content, response.model)\n",
    "print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "# --- LEXICOLOGIST ---\n",
    "print(\"Running lexicologist...\", end=\" \")\n",
    "lexicologist_prompt = prompt_maker.render(LexicologistConfig())\n",
    "full_prompt = f\"{preamble_instruction}\\n\\n{lexicologist_prompt}\\n\\n{preamble_text}\"\n",
    "response = llm.complete(full_prompt)\n",
    "store.save_analysis(sample_id, \"lexicologist\", response.content, response.model)\n",
    "print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "# --- INFORMATION ARCHITECT ---\n",
    "print(\"Running information_architect...\", end=\" \")\n",
    "info_arch_prompt = prompt_maker.render(InformationArchitectConfig())\n",
    "full_prompt = f\"{preamble_instruction}\\n\\n{info_arch_prompt}\\n\\n{preamble_text}\"\n",
    "response = llm.complete(full_prompt)\n",
    "store.save_analysis(sample_id, \"information_architect\", response.content, response.model)\n",
    "print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "# --- EFFICIENCY AUDITOR ---\n",
    "print(\"Running efficiency_auditor...\", end=\" \")\n",
    "efficiency_prompt = prompt_maker.render(EfficiencyAuditorConfig())\n",
    "full_prompt = f\"{preamble_instruction}\\n\\n{efficiency_prompt}\\n\\n{preamble_text}\"\n",
    "response = llm.complete(full_prompt)\n",
    "store.save_analysis(sample_id, \"efficiency_auditor\", response.content, response.model)\n",
    "print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "print(f\"\\nAll analyses complete for {sample_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078bd3eb",
   "metadata": {},
   "source": [
    "## Step 3: Retrieve and Examine Results\n",
    "\n",
    "Check what's been stored and verify all analyses are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b1cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of analyst types we're using\n",
    "ANALYSTS = [\"rhetorician\", \"syntactician\", \"lexicologist\", \"information_architect\", \"efficiency_auditor\"]\n",
    "\n",
    "# Check if all required analyses are present\n",
    "is_complete = store.is_complete(sample_id, ANALYSTS)\n",
    "print(f\"Analysis complete: {is_complete}\")\n",
    "\n",
    "# Retrieve sample and all analyses (both are now dicts)\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "print(f\"\\nSample: {sample['sample_id']}\")\n",
    "print(f\"Source: File {sample['file_index']}, paragraphs {sample['paragraph_start']}-{sample['paragraph_end']}\")\n",
    "print(f\"Analyses available: {list(analyses.keys())}\")\n",
    "\n",
    "# Examine one analysis\n",
    "print(f\"\\n--- Rhetorician Output (first 500 chars) ---\")\n",
    "print(analyses.get(\"rhetorician\", \"Not found\")[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381c141a",
   "metadata": {},
   "source": [
    "## Step 4: Pattern Recognition (Cross-Perspective Integration)\n",
    "\n",
    "Synthesize all analyst perspectives to identify interactions, tensions, and load-bearing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d21a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample and all analyses (both are dicts now)\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "# Format all analyst reports into a single string\n",
    "specialist_analyses = f\"\"\"**RHETORICIAN:**\n",
    "{analyses['rhetorician']}\n",
    "\n",
    "**SYNTACTICIAN:**\n",
    "{analyses['syntactician']}\n",
    "\n",
    "**LEXICOLOGIST:**\n",
    "{analyses['lexicologist']}\n",
    "\n",
    "**INFORMATION ARCHITECT:**\n",
    "{analyses['information_architect']}\n",
    "\n",
    "**EFFICIENCY AUDITOR:**\n",
    "{analyses['efficiency_auditor']}\n",
    "\"\"\"\n",
    "\n",
    "# Build pattern recognizer prompt using PromptMaker\n",
    "pattern_config = PatternRecognizerTextConfig(\n",
    "    original_text=sample['text'],  # Access dict with key\n",
    "    specialist_analyses=specialist_analyses\n",
    ")\n",
    "pattern_prompt = prompt_maker.render(pattern_config)\n",
    "\n",
    "# Get cross-perspective integration\n",
    "print(\"Running pattern recognizer...\", end=\" \")\n",
    "pattern_response = llm.complete(pattern_prompt)\n",
    "print(f\"✓ ({len(pattern_response.content)} chars)\")\n",
    "\n",
    "# Display first part of the synthesis\n",
    "print(\"\\n--- Pattern Recognition Output (first 1000 chars) ---\")\n",
    "print(pattern_response.content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c74117",
   "metadata": {},
   "source": [
    "## Utilities: Working with Stored Samples\n",
    "\n",
    "Helper functions for browsing and managing stored results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd71216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all samples in the database\n",
    "all_samples = store.list_samples()\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "print(f\"Sample IDs: {all_samples}\")\n",
    "\n",
    "# Check completion status for each\n",
    "print(\"\\nCompletion status:\")\n",
    "for sid in all_samples:\n",
    "    complete = store.is_complete(sid, ANALYSTS)\n",
    "    status = \"✓\" if complete else \"✗\"\n",
    "    print(f\"  {status} {sid}\")\n",
    "\n",
    "# Close database connection when done\n",
    "# store.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
