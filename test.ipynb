{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "968ea1e2",
   "metadata": {},
   "source": [
    "# Multi-Analyst Text Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates the full pipeline for analyzing text through multiple specialist lenses (rhetorician, syntactician, lexicologist, etc.) and synthesizing their observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8f4273",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:22.033495Z",
     "start_time": "2025-11-19T18:02:20.240014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm==1.79.3 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.79.3)\n",
      "Requirement already satisfied: pydantic==2.7.4 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.7.4)\n",
      "Requirement already satisfied: jinja2>=3.1.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: aiohttp>=3.10 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (3.13.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.99.5 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.22.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2>=3.1.0->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./venv/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.29.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./venv/lib/python3.11/site-packages (from tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.1.4)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: shellingham in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (0.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Optional: Install requirements if running in a fresh kernel\n",
    "# Uncomment if needed:\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Or install individual packages:\n",
    "# !pip install litellm pydantic jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2dd86e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providers\n",
      "=========\n",
      "* openai\n",
      "* openai_like\n",
      "* bytez\n",
      "* xai\n",
      "* custom_openai\n",
      "* text-completion-openai\n",
      "* cohere\n",
      "* cohere_chat\n",
      "* clarifai\n",
      "* anthropic\n",
      "* anthropic_text\n",
      "* replicate\n",
      "* huggingface\n",
      "* together_ai\n",
      "* datarobot\n",
      "* openrouter\n",
      "* cometapi\n",
      "* vertex_ai\n",
      "* vertex_ai_beta\n",
      "* gemini\n",
      "* ai21\n",
      "* baseten\n",
      "* azure\n",
      "* azure_text\n",
      "* azure_ai\n",
      "* sagemaker\n",
      "* sagemaker_chat\n",
      "* bedrock\n",
      "* vllm\n",
      "* nlp_cloud\n",
      "* petals\n",
      "* oobabooga\n",
      "* ollama\n",
      "* ollama_chat\n",
      "* deepinfra\n",
      "* perplexity\n",
      "* mistral\n",
      "* groq\n",
      "* nvidia_nim\n",
      "* cerebras\n",
      "* baseten\n",
      "* ai21_chat\n",
      "* volcengine\n",
      "* codestral\n",
      "* text-completion-codestral\n",
      "* deepseek\n",
      "* sambanova\n",
      "* maritalk\n",
      "* cloudflare\n",
      "* fireworks_ai\n",
      "* friendliai\n",
      "* watsonx\n",
      "* watsonx_text\n",
      "* triton\n",
      "* predibase\n",
      "* databricks\n",
      "* empower\n",
      "* github\n",
      "* custom\n",
      "* litellm_proxy\n",
      "* hosted_vllm\n",
      "* llamafile\n",
      "* lm_studio\n",
      "* galadriel\n",
      "* gradient_ai\n",
      "* github_copilot\n",
      "* novita\n",
      "* meta_llama\n",
      "* featherless_ai\n",
      "* nscale\n",
      "* nebius\n",
      "* dashscope\n",
      "* moonshot\n",
      "* v0\n",
      "* heroku\n",
      "* oci\n",
      "* morph\n",
      "* lambda_ai\n",
      "* vercel_ai_gateway\n",
      "* wandb\n",
      "* ovhcloud\n",
      "* lemonade\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c85480",
   "metadata": {},
   "source": [
    "## Initialize Base Objects\n",
    "Set up connections to a Large Language Model provider via `litellm` model router. Also, setup up tools to retrieve text data to be part of the context window, that is, instructions and texts to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66eb2231",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:29.927627Z",
     "start_time": "2025-11-19T18:02:29.858316Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from belletrist import LLM, LLMConfig, PromptMaker, DataSampler, ResultStore\n",
    "from belletrist.models import (\n",
    "    PreambleInstructionConfig,\n",
    "    PreambleTextConfig,\n",
    "    RhetoricianConfig,\n",
    "    SyntacticianConfig,\n",
    "    LexicologistConfig,\n",
    "    InformationArchitectConfig,\n",
    "    EfficiencyAuditorConfig,\n",
    "    PatternRecognizerTextConfig,\n",
    ")\n",
    "\n",
    "llm = LLM(LLMConfig(\n",
    "    model=\"mistral/mistral-large-2411\",\n",
    "    api_key=os.environ.get(\"MISTRAL_API_KEY\")\n",
    "))\n",
    "\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler()\n",
    "\n",
    "store = ResultStore(Path(f\"{os.getcwd()}/belletrist_storage.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559198d2",
   "metadata": {},
   "source": [
    "## Step 1: Generate and Store Sample\n",
    "\n",
    "Sample text from the data corpus and store it with full provenance (which file, which paragraphs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad68df05",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "table samples has 7 columns but 5 values were supplied",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m sample_id = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msample_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(store.list_samples())\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Store segment with automatic provenance extraction\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_segment\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSource: File \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegment.file_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegment.file_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/russell_writes/belletrist/result_store.py:152\u001b[39m, in \u001b[36mResultStore.save_segment\u001b[39m\u001b[34m(self, sample_id, segment)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34msave_segment\u001b[39m(\n\u001b[32m    138\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    139\u001b[39m     sample_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    140\u001b[39m     segment: \u001b[33m'\u001b[39m\u001b[33mTextSegment\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    141\u001b[39m ):\n\u001b[32m    142\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Save a TextSegment directly to the store.\u001b[39;00m\n\u001b[32m    143\u001b[39m \n\u001b[32m    144\u001b[39m \u001b[33;03m    Convenience method that extracts all fields from a TextSegment\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    150\u001b[39m \n\u001b[32m    151\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msave_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparagraph_start\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparagraph_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparagraph_end\u001b[49m\u001b[43m=\u001b[49m\u001b[43msegment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparagraph_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/russell_writes/belletrist/result_store.py:90\u001b[39m, in \u001b[36mResultStore.save_sample\u001b[39m\u001b[34m(self, sample_id, text, file_index, paragraph_start, paragraph_end)\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34msave_sample\u001b[39m(\n\u001b[32m     74\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     75\u001b[39m     sample_id: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     79\u001b[39m     paragraph_end: Optional[\u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     80\u001b[39m ):\n\u001b[32m     81\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Store a text sample with provenance information.\u001b[39;00m\n\u001b[32m     82\u001b[39m \n\u001b[32m     83\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     88\u001b[39m \u001b[33;03m        paragraph_end: Ending paragraph index (for slice)\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\"\"\u001b[39;49m\n\u001b[32m     91\u001b[39m \u001b[33;43m        INSERT OR REPLACE INTO samples\u001b[39;49m\n\u001b[32m     92\u001b[39m \u001b[33;43m        VALUES (?, ?, ?, ?, ?)\u001b[39;49m\n\u001b[32m     93\u001b[39m \u001b[33;43m    \u001b[39;49m\u001b[33;43m\"\"\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparagraph_start\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparagraph_end\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m.conn.commit()\n",
      "\u001b[31mOperationalError\u001b[39m: table samples has 7 columns but 5 values were supplied"
     ]
    }
   ],
   "source": [
    "# Option 1: Random sample (now with automatic provenance tracking!)\n",
    "segment = sampler.sample_segment(p_length=10)\n",
    "\n",
    "# Option 2: Specific file and paragraph range\n",
    "# segment = sampler.get_paragraph_chunk(file_index=0, paragraph_range=slice(10, 20))\n",
    "\n",
    "# Generate sample ID\n",
    "sample_id = f\"sample_{len(store.list_samples()) + 1:03d}\"\n",
    "\n",
    "# Store segment with automatic provenance extraction\n",
    "store.save_segment(sample_id, segment)\n",
    "\n",
    "print(f\"Created {sample_id}\")\n",
    "print(f\"Source: File {segment.file_index} ({segment.file_path.name})\")\n",
    "print(f\"Paragraphs: {segment.paragraph_start}-{segment.paragraph_end}\")\n",
    "print(f\"Text length: {len(segment.text)} characters\")\n",
    "print(f\"First 200 chars: {segment.text[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61031c7d",
   "metadata": {},
   "source": [
    "## Step 2: Run Multi-Analyst Pipeline\n",
    "\n",
    "Send the text through each specialist analyst. Each produces an independent analysis from their domain expertise.\n",
    "\n",
    "**Prompt structure for caching optimization:**\n",
    "1. Preamble instruction (static)\n",
    "2. Analyst-specific template (static per analyst)\n",
    "3. Text to analyze (dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52444b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sample text\n",
    "sample = store.get_sample(sample_id)\n",
    "text = sample['text']  # Now returns dict, access via keys\n",
    "\n",
    "# Build shared prompt components (reused across all analysts)\n",
    "preamble_instruction = prompt_maker.render(PreambleInstructionConfig())\n",
    "preamble_text = prompt_maker.render(PreambleTextConfig(text_to_analyze=text))\n",
    "\n",
    "# --- RHETORICIAN ---\n",
    "print(\"Running rhetorician...\", end=\" \")\n",
    "rhetorician_prompt = prompt_maker.render(RhetoricianConfig())  # All sections enabled by default\n",
    "full_prompt = f\"{preamble_instruction}\\n\\n{rhetorician_prompt}\\n\\n{preamble_text}\"\n",
    "response = llm.complete(full_prompt)\n",
    "store.save_analysis(sample_id, \"rhetorician\", response.content, response.model)\n",
    "print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "# --- SYNTACTICIAN ---\n",
    "print(\"Running syntactician...\", end=\" \")\n",
    "syntactician_prompt = prompt_maker.render(SyntacticianConfig())\n",
    "full_prompt = f\"{preamble_instruction}\\n\\n{syntactician_prompt}\\n\\n{preamble_text}\"\n",
    "response = llm.complete(full_prompt)\n",
    "store.save_analysis(sample_id, \"syntactician\", response.content, response.model)\n",
    "print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "# --- LEXICOLOGIST ---\n",
    "print(\"Running lexicologist...\", end=\" \")\n",
    "lexicologist_prompt = prompt_maker.render(LexicologistConfig())\n",
    "full_prompt = f\"{preamble_instruction}\\n\\n{lexicologist_prompt}\\n\\n{preamble_text}\"\n",
    "response = llm.complete(full_prompt)\n",
    "store.save_analysis(sample_id, \"lexicologist\", response.content, response.model)\n",
    "print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "# --- INFORMATION ARCHITECT ---\n",
    "print(\"Running information_architect...\", end=\" \")\n",
    "info_arch_prompt = prompt_maker.render(InformationArchitectConfig())\n",
    "full_prompt = f\"{preamble_instruction}\\n\\n{info_arch_prompt}\\n\\n{preamble_text}\"\n",
    "response = llm.complete(full_prompt)\n",
    "store.save_analysis(sample_id, \"information_architect\", response.content, response.model)\n",
    "print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "# --- EFFICIENCY AUDITOR ---\n",
    "print(\"Running efficiency_auditor...\", end=\" \")\n",
    "efficiency_prompt = prompt_maker.render(EfficiencyAuditorConfig())\n",
    "full_prompt = f\"{preamble_instruction}\\n\\n{efficiency_prompt}\\n\\n{preamble_text}\"\n",
    "response = llm.complete(full_prompt)\n",
    "store.save_analysis(sample_id, \"efficiency_auditor\", response.content, response.model)\n",
    "print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "print(f\"\\nAll analyses complete for {sample_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e0778f",
   "metadata": {},
   "source": [
    "## Step 3: Retrieve and Examine Results\n",
    "\n",
    "Check what's been stored and verify all analyses are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c109273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define list of analyst types we're using\n",
    "ANALYSTS = [\"rhetorician\", \"syntactician\", \"lexicologist\", \"information_architect\", \"efficiency_auditor\"]\n",
    "\n",
    "# Check if all required analyses are present\n",
    "is_complete = store.is_complete(sample_id, ANALYSTS)\n",
    "print(f\"Analysis complete: {is_complete}\")\n",
    "\n",
    "# Retrieve sample and all analyses (both are now dicts)\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "print(f\"\\nSample: {sample['sample_id']}\")\n",
    "print(f\"Source: File {sample['file_index']}, paragraphs {sample['paragraph_start']}-{sample['paragraph_end']}\")\n",
    "print(f\"Analyses available: {list(analyses.keys())}\")\n",
    "\n",
    "# Examine one analysis\n",
    "print(f\"\\n--- Rhetorician Output (first 500 chars) ---\")\n",
    "print(analyses.get(\"rhetorician\", \"Not found\")[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e8e31e",
   "metadata": {},
   "source": [
    "## Step 4: Pattern Recognition (Cross-Perspective Integration)\n",
    "\n",
    "Synthesize all analyst perspectives to identify interactions, tensions, and load-bearing features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbcba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample and all analyses (both are dicts now)\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "# Format all analyst reports into a single string\n",
    "specialist_analyses = f\"\"\"**RHETORICIAN:**\n",
    "{analyses['rhetorician']}\n",
    "\n",
    "**SYNTACTICIAN:**\n",
    "{analyses['syntactician']}\n",
    "\n",
    "**LEXICOLOGIST:**\n",
    "{analyses['lexicologist']}\n",
    "\n",
    "**INFORMATION ARCHITECT:**\n",
    "{analyses['information_architect']}\n",
    "\n",
    "**EFFICIENCY AUDITOR:**\n",
    "{analyses['efficiency_auditor']}\n",
    "\"\"\"\n",
    "\n",
    "# Build pattern recognizer prompt using PromptMaker\n",
    "pattern_config = PatternRecognizerTextConfig(\n",
    "    original_text=sample['text'],  # Access dict with key\n",
    "    specialist_analyses=specialist_analyses\n",
    ")\n",
    "pattern_prompt = prompt_maker.render(pattern_config)\n",
    "\n",
    "# Get cross-perspective integration\n",
    "print(\"Running pattern recognizer...\", end=\" \")\n",
    "pattern_response = llm.complete(pattern_prompt)\n",
    "print(f\"✓ ({len(pattern_response.content)} chars)\")\n",
    "\n",
    "# Display first part of the synthesis\n",
    "print(\"\\n--- Pattern Recognition Output (first 1000 chars) ---\")\n",
    "print(pattern_response.content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c6a10",
   "metadata": {},
   "source": [
    "## Utilities: Working with Stored Samples\n",
    "\n",
    "Helper functions for browsing and managing stored results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fedf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all samples in the database\n",
    "all_samples = store.list_samples()\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "print(f\"Sample IDs: {all_samples}\")\n",
    "\n",
    "# Check completion status for each\n",
    "print(\"\\nCompletion status:\")\n",
    "for sid in all_samples:\n",
    "    complete = store.is_complete(sid, ANALYSTS)\n",
    "    status = \"✓\" if complete else \"✗\"\n",
    "    print(f\"  {status} {sid}\")\n",
    "\n",
    "# Close database connection when done\n",
    "# store.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
