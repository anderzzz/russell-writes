{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fc49940",
   "metadata": {},
   "source": [
    "# Multi-Analyst Text Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates the full pipeline for analyzing text through multiple specialist lenses (rhetorician, syntactician, lexicologist, etc.) and synthesizing their observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf68fd8",
   "metadata": {},
   "source": [
    "## Installations and Preparations\n",
    "First, external modules are installed and ensured to be in working order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e425b2c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:22.033495Z",
     "start_time": "2025-11-19T18:02:20.240014Z"
    }
   },
   "outputs": [],
   "source": [
    "# Optional: Install requirements if running in a fresh kernel\n",
    "# Uncomment if needed:\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Or install individual packages:\n",
    "# !pip install litellm pydantic jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b250c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26c4f8",
   "metadata": {},
   "source": [
    "## Initialize Base Objects\n",
    "Set up connections to a Large Language Model provider via `litellm` model router. Also, setup up tools to retrieve text data to be part of the context window, that is, instruction prompts and texts to analyze. A basic result storage is also initialized.\n",
    "\n",
    "The LLM to use is set by the `model_string`, which is constructed as `<provider>/<model>`, the providers defined by the `litellm` package, see in particular `litellm.LITELLM_CHAT_PROVIDERS`. The API key to the provider should be stored in an environment variable with name defined in `model_provider_api_key_env_var`. Do **not** store the API key as a string variable directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c00dbe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_string = 'mistral/mistral-large-2411'\n",
    "model_provider_api_key_env_var = 'MISTRAL_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682641e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:29.927627Z",
     "start_time": "2025-11-19T18:02:29.858316Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from belletrist import LLM, LLMConfig, PromptMaker, DataSampler, ResultStore\n",
    "\n",
    "llm = LLM(LLMConfig(\n",
    "    model=model_string,\n",
    "    api_key=os.environ.get(model_provider_api_key_env_var)\n",
    "))\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler()\n",
    "store = ResultStore(Path(f\"{os.getcwd()}/belletrist_storage.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec04b9b",
   "metadata": {},
   "source": [
    "In case a clean run is done, the old contents of the database are discarded with a result store reset. Do not run the rest if content should be preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84ccd199",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdaea99",
   "metadata": {},
   "source": [
    "## Generate and Store Text Samples to be Analyzed\n",
    "\n",
    "A random text sample is taken from the data corpus and stored with full provenance (which file, which paragraphs). Each sample is an instance of `TextSegment`.\n",
    "\n",
    "The sample size is set by the variable `n_sample` and each sample comprises `m_paragraphs_per_sample` number of consecutive paragraphs.\n",
    "\n",
    "If non-random text samples are preferred, use the `get_paragraph_chunk` method of the `DataSampler` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5216bde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text source: /Users/andersohrn/PycharmProjects/russell_writes/data/russell/education_and_the_good_life.txt\n",
      "Paragraph range: 143 - 147\n",
      "\n",
      "But with the greater independence of walking and running there is\n",
      "apt to come also a new timidity. The new-born infant can easily be\n",
      "frightened; Dr. J. B. and Mrs. Watson found that the things which alarm\n",
      "it most are loud noises and the sensation of being dropped.[6] It\n",
      "is, however, so completely protected that it has little occasion for\n",
      "the rational exercise of fear; even in real dangers it is helpless,\n",
      "so that fear would not be of any use to it. During the second and\n",
      "third year, new fears develop. It is a moot point how far this is\n",
      "due to suggestion, and how far it is instinctive. The fact that the\n",
      "fears do not exist during the first year is not conclusive against\n",
      "their instinctive character, since an instinct may ripen at any\n",
      "age. Not even the most extreme Freudian would maintain that the sex\n",
      "instinct is mature at birth. Obviously children who can run about by\n",
      "themselves have more need of fear than infants that cannot walk; it\n",
      "would therefore not be surprising if the instinct of fear arose with\n",
      "the need. The question is of considerable educational importance. If\n",
      "all fears arise from suggestion, they can be prevented by the simple\n",
      "expedient of not showing fear or aversion before a child. If, on the\n",
      "other hand, some of them are instinctive, more elaborate methods will\n",
      "be required.\n",
      "\n",
      "Dr. Chalmers Mitchell, in his book “The Childhood of Animals”, gives\n",
      "a large number of observations and experiments to show that there is\n",
      "usually no inherited instinct of fear in young animals.[7] Except\n",
      "monkeys and a few birds, they view the age-long enemies of their\n",
      "species, such as snakes, without the slightest alarm, unless their\n",
      "parents have taught them to feel fear of these animals. Children well\n",
      "under a year old seem never to be afraid of animals. Dr. Watson taught\n",
      "one such child to be afraid of rats by repeatedly sounding a gong\n",
      "behind its head at the moment when he showed it a rat. The noise was\n",
      "terrifying, and the rat came to be so by association. But instinctive\n",
      "fear of animals seems quite unknown in the early months. Fear of the\n",
      "dark, also, seems never to occur in children who have not been exposed\n",
      "to the suggestion that the dark is terrifying. There are certainly\n",
      "very strong grounds for the view that most of the fears which we used\n",
      "to regard as instinctive are acquired, and would not arise if grown-up\n",
      "people did not create them.\n",
      "\n",
      "In order to get fresh light on this subject, I have observed my own\n",
      "children carefully; but as I could not always know what nurses and\n",
      "maids might have said to them, the interpretation of the facts was\n",
      "often doubtful. So far as I could judge, they bore out Dr. Watson’s\n",
      "views as to fear in the first year of life. In the second year, they\n",
      "showed no fear of animals, except that one of them, for a time, was\n",
      "afraid of horses. This, however, was apparently due to the fact that\n",
      "a horse had suddenly galloped past her with a very loud noise. She is\n",
      "still in her second year, and therefore for later observation I am\n",
      "dependent on the boy. Near the end of his second year, he had a new\n",
      "nurse who was generally timid and especially afraid of the dark. He\n",
      "quickly acquired her terrors (of which we were ignorant at first);\n",
      "he fled from dogs and cats, cowered in abject fear before a dark\n",
      "cupboard, wanted lights in every part of the room after dark, and was\n",
      "even afraid of his little sister the first time he saw her, thinking,\n",
      "apparently, that she was a strange animal of some unknown species.[8]\n",
      "All these fears might have been acquired from the timid nurse; in\n",
      "fact they gradually faded away after she was gone. There were other\n",
      "fears, however, which could not be accounted for in the same way,\n",
      "since they began before the nurse came, and were directed to objects\n",
      "which no grown-up person would find alarming. Chief of these was a\n",
      "fear of everything that moved in a surprising way, notably shadows and\n",
      "mechanical toys. After making this observation, I learned that fears\n",
      "of this sort are normal in childhood, and that there are strong reasons\n",
      "for regarding them as instinctive. The matter is discussed by William\n",
      "Stern in his “Psychology of Early Childhood”, p. 494 ff, under the\n",
      "heading “Fear of the Mysterious”. What he says is as follows:\n",
      "\n",
      "The special significance of this form of fear, particularly in early\n",
      "  childhood, has escaped the notice of the older school of child\n",
      "  psychologists; it has lately been established by Groos and by us.\n",
      "  “Fear of the unaccustomed seems to be more a part of primitive nature\n",
      "  than fear of a known danger” (Groos, p. 284). If the child meets\n",
      "  with anything that does not fit in with the familiar course of his\n",
      "  perception, three things are possible. Either the impression is so\n",
      "  alien that it is simply rejected as a foreign body, and consciousness\n",
      "  takes no notice of it. Or the interruption of the usual course of\n",
      "  perception is pronounced enough to attract attention but not so\n",
      "  violent as to effect disturbance; it is rather surprise, desire\n",
      "  for knowledge, the beginning of all thought, judgment, enquiry.\n",
      "  Or, lastly, the new suddenly breaks in upon the old with violent\n",
      "  intensity, throws familiar ideas into unexpected confusion without\n",
      "  a possibility of an immediate practical adjustment; then follows\n",
      "  a shock with a strong affective-tone of displeasure, the fear of\n",
      "  the mysterious (uncanny). Groos now has pointed out with keen\n",
      "  insight that this fear of the uncanny is also distinctly founded on\n",
      "  instinctive fear; it corresponds to a biological necessity which\n",
      "  works from one generation to the next.\n"
     ]
    }
   ],
   "source": [
    "text_sample = sampler.sample_segment(p_length=4)\n",
    "print(f'Text source: {text_sample.file_path}')\n",
    "print(f'Paragraph range: {text_sample.paragraph_start} - {text_sample.paragraph_end}')\n",
    "print(f'\\n{text_sample.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21c36d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 1\n",
    "m_paragraphs_per_sample = 10\n",
    "\n",
    "for _ in range(n_sample):\n",
    "    sample_id = f'sample_{len(store.list_samples()) + 1:03d}'\n",
    "    segment = sampler.sample_segment(p_length=m_paragraphs_per_sample)\n",
    "    store.save_segment(sample_id, segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f572b99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys:\n",
      "============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sample_001']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sample keys:\\n============')\n",
    "store.list_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6543e834",
   "metadata": {},
   "source": [
    "## Construct the Analyst Agents and Analyze\n",
    "\n",
    "Send the text samples through each specialist analyst. Each produces an independent analysis from their domain expertise.\n",
    "\n",
    "**Prompt structure for caching optimization:**\n",
    "1. Preamble instruction (static)\n",
    "2. Analyst-specific template (static per analyst)\n",
    "3. Text to analyze (dynamic)\n",
    "\n",
    "Note that the execution of this can take time since it involves invoking LLMs. These are however independent analyses, and can therefore in principle be run in parallel, though the implementation below does not utilize that fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "749ab972",
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist.models import (\n",
    "    PreambleInstructionConfig,\n",
    "    PreambleTextConfig,\n",
    "    RhetoricianConfig,\n",
    "    SyntacticianConfig,\n",
    "    LexicologistConfig,\n",
    "    InformationArchitectConfig,\n",
    "    EfficiencyAuditorConfig,\n",
    "    PatternRecognizerTextConfig,\n",
    ")\n",
    "ANALYSTS = [\"rhetorician\", \"syntactician\", \"lexicologist\", \"information_architect\", \"efficiency_auditor\"]\n",
    "ANALYST_CONFIGS = {\n",
    "    \"rhetorician\": RhetoricianConfig,\n",
    "    \"syntactician\": SyntacticianConfig,\n",
    "    \"lexicologist\": LexicologistConfig,\n",
    "    \"information_architect\": InformationArchitectConfig,\n",
    "    \"efficiency_auditor\": EfficiencyAuditorConfig,\n",
    "}\n",
    "\n",
    "def build_analyst_prompt(preamble_instruction: str, analyst_prompt: str, preamble_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to construct the full prompt for an analyst.\n",
    "    \n",
    "    \"\"\"\n",
    "    return f\"{preamble_instruction}\\n\\n{analyst_prompt}\\n\\n{preamble_text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2adf9b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 samples with 5 analysts each\n",
      "\n",
      "Sample: sample_001\n",
      "  Running rhetorician... ✓ (6056 chars)\n",
      "  Running syntactician... ✓ (11307 chars)\n",
      "  Running lexicologist... ✓ (5365 chars)\n",
      "  Running information_architect... ✓ (10960 chars)\n",
      "  Running efficiency_auditor... ✓ (5805 chars)\n",
      "\n",
      "All analyses complete for 1 samples\n"
     ]
    }
   ],
   "source": [
    "# Get all samples from the store\n",
    "all_samples = store.list_samples()\n",
    "print(f\"Processing {len(all_samples)} samples with {len(ANALYSTS)} analysts each\\n\")\n",
    "\n",
    "# Outer loop: iterate over each text sample\n",
    "for sample_id in all_samples:\n",
    "    print(f\"Sample: {sample_id}\")\n",
    "    \n",
    "    # Get the sample text\n",
    "    sample = store.get_sample(sample_id)\n",
    "    text = sample['text']\n",
    "    \n",
    "    # Build shared prompt components (reused across all analysts for this sample)\n",
    "    preamble_instruction = prompt_maker.render(PreambleInstructionConfig())\n",
    "    preamble_text = prompt_maker.render(PreambleTextConfig(text_to_analyze=text))\n",
    "    \n",
    "    # Inner loop: run each analyst on this sample\n",
    "    for analyst_name in ANALYSTS:\n",
    "        print(f\"  Running {analyst_name}...\", end=\" \")\n",
    "        \n",
    "        # Get analyst-specific prompt using the config class\n",
    "        analyst_config = ANALYST_CONFIGS[analyst_name]()\n",
    "        analyst_prompt = prompt_maker.render(analyst_config)\n",
    "        \n",
    "        # Build full prompt using helper function\n",
    "        full_prompt = build_analyst_prompt(preamble_instruction, analyst_prompt, preamble_text)\n",
    "        \n",
    "        # Run analysis\n",
    "        response = llm.complete(full_prompt)\n",
    "        store.save_analysis(sample_id, analyst_name, response.content, response.model)\n",
    "        \n",
    "        print(f\"✓ ({len(response.content)} chars)\")\n",
    "    \n",
    "    print()  # Blank line between samples\n",
    "\n",
    "print(f\"All analyses complete for {len(all_samples)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ec3aa",
   "metadata": {},
   "source": [
    "Then verify that analysis was run as expected and yielded analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9232bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete: True\n",
      "\n",
      "Sample: sample_001\n",
      "Source: File 2, paragraphs 354-364\n",
      "Analyses available: ['efficiency_auditor', 'information_architect', 'lexicologist', 'rhetorician', 'syntactician']\n",
      "\n",
      "--- Rhetorician Output (first 500 chars) ---\n",
      "### RHETORICAL STRATEGY AND STANCE\n",
      "\n",
      "#### 1. WRITER'S POSITION\n",
      "\n",
      "**Persona:**\n",
      "The writer presents an authoritative yet approachable persona. The text is written in a conversational tone, making it accessible and engaging. The writer appears as an expert who is also an explorer, inviting the reader to consider and participate in the educational strategies being discussed.\n",
      "\n",
      "**Relationship to Subject Matter:**\n",
      "The writer positions himself as an expert, drawing on a deep understanding of educational p\n"
     ]
    }
   ],
   "source": [
    "sample_id = 'sample_001'\n",
    "is_complete = store.is_complete(sample_id, ANALYSTS)\n",
    "print(f\"Analysis complete: {is_complete}\")\n",
    "\n",
    "# Retrieve sample and all analyses (both are now dicts)\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "print(f\"\\nSample: {sample['sample_id']}\")\n",
    "print(f\"Source: File {sample['file_index']}, paragraphs {sample['paragraph_start']}-{sample['paragraph_end']}\")\n",
    "print(f\"Analyses available: {list(analyses.keys())}\")\n",
    "\n",
    "# Examine one analysis\n",
    "print(f\"\\n--- Rhetorician Output (first 500 chars) ---\")\n",
    "print(analyses.get(\"rhetorician\", \"Not found\")[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aca908",
   "metadata": {},
   "source": [
    "## Pattern Recognition (Cross-Perspective Integration)\n",
    "\n",
    "Synthesize all analyst perspectives to identify interactions, tensions, and load-bearing features. This is a per-text-cross-analyst transformation where a unit of writing patterns for each text sample is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e9d7fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_001']\n"
     ]
    }
   ],
   "source": [
    "samples_to_analyze = store.list_samples()[0:1]\n",
    "print(samples_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77d27fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pattern_prompt_from_(text: str, analyses: dict):\n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "    analyst_info = {}\n",
    "    for analyst_name in ANALYSTS:\n",
    "        config_class = ANALYST_CONFIGS[analyst_name]\n",
    "        analyst_info[analyst_name] = {\n",
    "            'analysis': analyses[analyst_name],\n",
    "            'analyst_descr_short': config_class.description()\n",
    "        }\n",
    "\n",
    "    pattern_config = PatternRecognizerTextConfig(\n",
    "        original_text=sample['text'],\n",
    "        analysts=analyst_info\n",
    "    )\n",
    "    return prompt_maker.render(pattern_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a39fd35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pattern recognizer for sample_001... \n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "ServiceUnavailableError",
     "evalue": "litellm.ServiceUnavailableError: ServiceUnavailableError: MistralException - {\"object\":\"error\",\"message\":\"Service unavailable.\",\"type\":\"internal_server_error\",\"param\":null,\"code\":\"3803\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:196\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_sync_call\u001b[39m\u001b[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     response = \u001b[43msync_httpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[43msigned_json_body\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py:898\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/custom_httpx/http_handler.py:880\u001b[39m, in \u001b[36mHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[39m\n\u001b[32m    879\u001b[39m response = \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m880\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_models.py:758\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    757\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Server error '503 Service Unavailable' for url 'https://api.mistral.ai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/503",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/main.py:2176\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   2169\u001b[39m     api_base = (\n\u001b[32m   2170\u001b[39m         api_base\n\u001b[32m   2171\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m litellm.api_base\n\u001b[32m   2172\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m get_secret(\u001b[33m\"\u001b[39m\u001b[33mMISTRAL_API_BASE\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2173\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mhttps://api.mistral.ai/v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2174\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2176\u001b[39m     response = \u001b[43mbase_llm_http_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2183\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2184\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2188\u001b[39m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2195\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   2196\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model\n\u001b[32m   2197\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m custom_llm_provider == \u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2198\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m litellm.replicate_models\n\u001b[32m   2199\u001b[39m ):\n\u001b[32m   2200\u001b[39m     \u001b[38;5;66;03m# Setting the relevant API KEY for replicate, replicate defaults to using os.environ.get(\"REPLICATE_API_TOKEN\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:499\u001b[39m, in \u001b[36mBaseLLMHTTPHandler.completion\u001b[39m\u001b[34m(self, model, messages, api_base, custom_llm_provider, model_response, encoding, logging_obj, optional_params, timeout, litellm_params, acompletion, stream, fake_stream, api_key, headers, client, provider_config, shared_session)\u001b[39m\n\u001b[32m    497\u001b[39m     sync_httpx_client = client\n\u001b[32m--> \u001b[39m\u001b[32m499\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_common_sync_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43msync_httpx_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43msigned_json_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config.transform_response(\n\u001b[32m    511\u001b[39m     model=model,\n\u001b[32m    512\u001b[39m     raw_response=response,\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m     json_mode=json_mode,\n\u001b[32m    522\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:221\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_sync_call\u001b[39m\u001b[34m(self, sync_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:3569\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._handle_error\u001b[39m\u001b[34m(self, e, provider_config)\u001b[39m\n\u001b[32m   3563\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BaseLLMException(\n\u001b[32m   3564\u001b[39m         status_code=status_code,\n\u001b[32m   3565\u001b[39m         message=error_text,\n\u001b[32m   3566\u001b[39m         headers=error_headers,\n\u001b[32m   3567\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3569\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m provider_config.get_error_class(\n\u001b[32m   3570\u001b[39m     error_message=error_text,\n\u001b[32m   3571\u001b[39m     status_code=status_code,\n\u001b[32m   3572\u001b[39m     headers=error_headers,\n\u001b[32m   3573\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: {\"object\":\"error\",\"message\":\"Service unavailable.\",\"type\":\"internal_server_error\",\"param\":null,\"code\":\"3803\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mServiceUnavailableError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m sample, analyses = store.get_sample_with_analyses(sample_id)\n\u001b[32m      5\u001b[39m pattern_prompt = build_pattern_prompt_from_(text=sample[\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m], analyses=analyses)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m pattern_response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern_prompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Store pattern recognition result in result store\u001b[39;00m\n\u001b[32m     10\u001b[39m store.save_analysis(\n\u001b[32m     11\u001b[39m     sample_id, \n\u001b[32m     12\u001b[39m     PatternRecognizerTextConfig.analyst_name(), \n\u001b[32m     13\u001b[39m     pattern_response.content, \n\u001b[32m     14\u001b[39m     pattern_response.model\n\u001b[32m     15\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/russell_writes/belletrist/llm.py:80\u001b[39m, in \u001b[36mLLM.complete\u001b[39m\u001b[34m(self, prompt, system, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m request_params = \u001b[38;5;28mself\u001b[39m._build_request_params(messages, **kwargs)\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Execute the completion\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m raw_response = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Parse and return the response\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_response(raw_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/utils.py:1377\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1374\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1375\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1376\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/utils.py:1246\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1244\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1245\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1247\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1249\u001b[39m     kwargs=kwargs,\n\u001b[32m   1250\u001b[39m     call_type=call_type,\n\u001b[32m   1251\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/main.py:3770\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   3767\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3768\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3769\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3770\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3771\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3773\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3776\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2323\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2322\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2323\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2325\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:540\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m503\u001b[39m:\n\u001b[32m    539\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceUnavailableError(\n\u001b[32m    541\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mServiceUnavailableError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    542\u001b[39m         model=model,\n\u001b[32m    543\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    544\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    545\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    546\u001b[39m     )\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m504\u001b[39m:  \u001b[38;5;66;03m# gateway timeout error\u001b[39;00m\n\u001b[32m    548\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mServiceUnavailableError\u001b[39m: litellm.ServiceUnavailableError: ServiceUnavailableError: MistralException - {\"object\":\"error\",\"message\":\"Service unavailable.\",\"type\":\"internal_server_error\",\"param\":null,\"code\":\"3803\"}"
     ]
    }
   ],
   "source": [
    "for sample_id in samples_to_analyze:\n",
    "    print(f\"Running pattern recognizer for {sample_id}...\", end=\" \")\n",
    "    \n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "    pattern_prompt = build_pattern_prompt_from_(text=sample['text'], analyses=analyses)\n",
    "\n",
    "    pattern_response = llm.complete(pattern_prompt)\n",
    "    \n",
    "    # Store pattern recognition result in result store\n",
    "    store.save_analysis(\n",
    "        sample_id, \n",
    "        PatternRecognizerTextConfig.analyst_name(), \n",
    "        pattern_response.content, \n",
    "        pattern_response.model\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ ({len(pattern_response.content)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6fb07c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['efficiency_auditor', 'information_architect', 'lexicologist', 'rhetorician', 'syntactician'])\n",
      "\n",
      "--- Pattern Analyst Output (first 2000 chars) ---\n",
      "Not found\n"
     ]
    }
   ],
   "source": [
    "sample_id = 'sample_001'\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "print(analyses.keys())\n",
    "\n",
    "print(f\"\\n--- Pattern Analyst Output (first 2000 chars) ---\")\n",
    "print(analyses.get(\"pattern_recognizer\", \"Not found\")[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f6d5c",
   "metadata": {},
   "source": [
    "## Step 5: Cross-Text Synthesis (Cross-Analyst Pattern Recognition)\n",
    "\n",
    "Now we synthesize patterns across multiple text analyses. This stage takes all the pattern recognition outputs (cross-perspective integrations from Step 4) and identifies:\n",
    "- Recurring patterns across texts\n",
    "- Context-dependent variations\n",
    "- Hierarchies of importance\n",
    "- Generalizable principles\n",
    "\n",
    "This produces a single synthesis document that is NOT stored in the result store (since it spans multiple samples), but saved as a final output variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c69e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist.models import PatternRecognizerCrossAnalystConfig\n",
    "\n",
    "# Get all samples that have pattern recognition results\n",
    "all_samples = store.list_samples()\n",
    "pattern_analyst = PatternRecognizerTextConfig.analyst_name()\n",
    "\n",
    "# Retrieve all pattern recognition analyses\n",
    "integrated_analyses = {}\n",
    "for sample_id in all_samples:\n",
    "    pattern_analysis = store.get_analysis(sample_id, pattern_analyst)\n",
    "    if pattern_analysis:\n",
    "        integrated_analyses[sample_id] = pattern_analysis\n",
    "    else:\n",
    "        print(f\"⚠ Sample {sample_id} missing pattern recognition analysis\")\n",
    "\n",
    "print(f\"Found {len(integrated_analyses)} pattern recognition analyses\")\n",
    "print(f\"Sample IDs: {list(integrated_analyses.keys())}\")\n",
    "\n",
    "# Only proceed if we have at least 2 (required for cross-text synthesis)\n",
    "if len(integrated_analyses) >= 2:\n",
    "    # Build cross-analyst synthesis prompt\n",
    "    cross_analyst_config = PatternRecognizerCrossAnalystConfig(\n",
    "        integrated_analyses=integrated_analyses\n",
    "    )\n",
    "    cross_analyst_prompt = prompt_maker.render(cross_analyst_config)\n",
    "    \n",
    "    # Run cross-text synthesis\n",
    "    print(\"\\\\nRunning cross-text synthesis...\", end=\" \")\n",
    "    cross_analyst_response = llm.complete(cross_analyst_prompt)\n",
    "    print(f\"✓ ({len(cross_analyst_response.content)} chars)\")\n",
    "    \n",
    "    # Save to variable (not to result store - this is a final output)\n",
    "    cross_text_synthesis = cross_analyst_response.content\n",
    "    \n",
    "    # Display first part\n",
    "    print(\"\\\\n--- Cross-Text Synthesis (first 1000 chars) ---\")\n",
    "    print(cross_text_synthesis[:1000])\n",
    "else:\n",
    "    print(f\"\\\\n⚠ Need at least 2 pattern recognition analyses for cross-text synthesis. Got {len(integrated_analyses)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae7097d",
   "metadata": {},
   "source": [
    "## Utilities: Working with Stored Samples\n",
    "\n",
    "Helper functions for browsing and managing stored results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d254120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all samples in the database\n",
    "all_samples = store.list_samples()\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "print(f\"Sample IDs: {all_samples}\")\n",
    "\n",
    "# Check completion status for each\n",
    "print(\"\\nCompletion status:\")\n",
    "for sid in all_samples:\n",
    "    complete = store.is_complete(sid, ANALYSTS)\n",
    "    status = \"✓\" if complete else \"✗\"\n",
    "    print(f\"  {status} {sid}\")\n",
    "\n",
    "# Close database connection when done\n",
    "# store.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
