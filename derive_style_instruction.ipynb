{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d9bb386",
   "metadata": {},
   "source": [
    "# Multi-Analyst Text Analysis Pipeline\n",
    "\n",
    "In this notebook we demonstrate the full pipeline for analyzing text through multiple specialist lenses (rhetorician, syntactician, lexicologist, etc.) and synthesizing their observations into \n",
    "* a unified writing style analysis from the text samples,\n",
    "* an instruction of how to write in the analyzed style.\n",
    "\n",
    "The workflow is *agentic* in that it involves several distinct agents built on Large Language Models (LLMs), however, the order and relation between each agent is set, not dynamically derived.\n",
    "\n",
    "The analysis and its details are described in this blog post: **INSERT LINK**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480df68c",
   "metadata": {},
   "source": [
    "## Installations and Preparations\n",
    "External libraries are installed and tested to be in working order.\n",
    "\n",
    "Key dependencies:\n",
    "* LiteLLM (model router such that different LLM APIs can be readily employed)\n",
    "* Jinja (create prompts with variables and conditional logic)\n",
    "* Pydantic (create prompts from variables with validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762361b",
   "metadata": {},
   "source": [
    "**Install requirements.** Only needed if running in fresh kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ca03d63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:22.033495Z",
     "start_time": "2025-11-19T18:02:20.240014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm==1.79.3 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.79.3)\n",
      "Requirement already satisfied: pydantic==2.7.4 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.7.4)\n",
      "Requirement already satisfied: jinja2>=3.1.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: aiohttp>=3.10 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (3.13.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.99.5 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.22.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2>=3.1.0->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./venv/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.29.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./venv/lib/python3.11/site-packages (from tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.1.4)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: shellingham in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (0.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c95623",
   "metadata": {},
   "source": [
    "Check that LiteLLM was installed correctly. List the providers available via LiteLLM router."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f5e71c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providers\n",
      "=========\n",
      "* openai\n",
      "* openai_like\n",
      "* bytez\n",
      "* xai\n",
      "* custom_openai\n",
      "* text-completion-openai\n",
      "* cohere\n",
      "* cohere_chat\n",
      "* clarifai\n",
      "* anthropic\n",
      "* anthropic_text\n",
      "* replicate\n",
      "* huggingface\n",
      "* together_ai\n",
      "* datarobot\n",
      "* openrouter\n",
      "* cometapi\n",
      "* vertex_ai\n",
      "* vertex_ai_beta\n",
      "* gemini\n",
      "* ai21\n",
      "* baseten\n",
      "* azure\n",
      "* azure_text\n",
      "* azure_ai\n",
      "* sagemaker\n",
      "* sagemaker_chat\n",
      "* bedrock\n",
      "* vllm\n",
      "* nlp_cloud\n",
      "* petals\n",
      "* oobabooga\n",
      "* ollama\n",
      "* ollama_chat\n",
      "* deepinfra\n",
      "* perplexity\n",
      "* mistral\n",
      "* groq\n",
      "* nvidia_nim\n",
      "* cerebras\n",
      "* baseten\n",
      "* ai21_chat\n",
      "* volcengine\n",
      "* codestral\n",
      "* text-completion-codestral\n",
      "* deepseek\n",
      "* sambanova\n",
      "* maritalk\n",
      "* cloudflare\n",
      "* fireworks_ai\n",
      "* friendliai\n",
      "* watsonx\n",
      "* watsonx_text\n",
      "* triton\n",
      "* predibase\n",
      "* databricks\n",
      "* empower\n",
      "* github\n",
      "* custom\n",
      "* litellm_proxy\n",
      "* hosted_vllm\n",
      "* llamafile\n",
      "* lm_studio\n",
      "* galadriel\n",
      "* gradient_ai\n",
      "* github_copilot\n",
      "* novita\n",
      "* meta_llama\n",
      "* featherless_ai\n",
      "* nscale\n",
      "* nebius\n",
      "* dashscope\n",
      "* moonshot\n",
      "* v0\n",
      "* heroku\n",
      "* oci\n",
      "* morph\n",
      "* lambda_ai\n",
      "* vercel_ai_gateway\n",
      "* wandb\n",
      "* ovhcloud\n",
      "* lemonade\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b182504e",
   "metadata": {},
   "source": [
    "## Initialize Base Objects\n",
    "The base objects part of the current project library (`belletrist`) are initialized. They are:\n",
    "* `LLM`: the LLM object.\n",
    "* `LLMConfig`: the configuration of the LLM object, such as what model to use.\n",
    "* `PromptMaker`: generates prompts from templates and variables\n",
    "* `DataSampler`: retrieves and samples text at a source directory\n",
    "* `ResultStore`: simple database object to save intermediate and final outputs\n",
    "\n",
    "The LLM to use is set by the `model_string`, which is constructed as `<provider>/<model>`, the providers defined by the `litellm` package, see in particular `litellm.LITELLM_CHAT_PROVIDERS`. The API key to the provider should be stored in an environment variable with name defined in `model_provider_api_key_env_var`. You need to create that yourself for the provider of interest. \n",
    "\n",
    "Do **not** store the API key as a string variable directly in the notebook, you're at risk of exposing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2268e892",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_string = 'together_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput'\n",
    "model_provider_api_key_env_var = 'TOGETHER_AI_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45ffcc2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:29.927627Z",
     "start_time": "2025-11-19T18:02:29.858316Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from belletrist import LLM, LLMConfig, PromptMaker, DataSampler, ResultStore\n",
    "\n",
    "llm = LLM(LLMConfig(\n",
    "    model=model_string,\n",
    "    api_key=os.environ.get(model_provider_api_key_env_var)\n",
    "))\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler(\n",
    "    data_path=(Path(os.getcwd()) / \"data\" / \"russell\").resolve()\n",
    ")\n",
    "store = ResultStore(Path(f\"{os.getcwd()}/belletrist_storage.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f3231c",
   "metadata": {},
   "source": [
    "In case a clean run is wanted, the old contents of the database are discarded with a result store reset. Do **not** run this reset if content should be preserved from previous runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "811c5db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dfa12f6",
   "metadata": {},
   "source": [
    "## Generate and Store Text Samples to be Analyzed\n",
    "\n",
    "The `DataSampler` retrieves paragraphs from the corpus of text. The retrieval can be a random sample of consecutive paragraphs (via the method `sample_segment`) or a specific file and paragraph range (via the method `get_paragraph_chunk`).\n",
    "\n",
    "As illustration of the process, a random four-paragraph long segment is sampled below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c552d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text source: /Users/andersohrn/PycharmProjects/russell_writes/data/russell/education_and_the_good_life.txt\n",
      "Paragraph range: 88 - 92\n",
      "\n",
      "Questions of physical health, strictly speaking, lie outside the scope\n",
      "of this book, and must be left to medical practitioners. I shall touch\n",
      "on them only where they have psychological importance. But physical and\n",
      "mental are scarcely distinguishable in the first year of life. Moreover\n",
      "the educator in later years may find himself handicapped by purely\n",
      "physiological mistakes in handling the infant. We cannot therefore\n",
      "altogether avoid trespassing upon ground which does not of right belong\n",
      "to us.\n",
      "\n",
      "The new-born infant has reflexes and instincts, but no habits. Whatever\n",
      "habits it may have acquired in the womb are useless in its new\n",
      "situation: even breathing sometimes has to be taught, and some children\n",
      "die because they do not learn the lesson quickly enough. There is one\n",
      "well-developed instinct, the instinct of sucking; when the child is\n",
      "engaged in this occupation, it feels at home with its new environment.\n",
      "But the rest of its waking life is passed in a vague bewilderment,\n",
      "from which relief is found by sleeping most of the twenty-four hours.\n",
      "At the end of a fortnight, all this is changed. The child has acquired\n",
      "expectations from regularly recurring experiences. It is already a\n",
      "conservative--probably a more complete conservative than at any later\n",
      "time. Any novelty is met with resentment. If it could speak, it would\n",
      "say: “Do you suppose I am going to change the habits of a lifetime\n",
      "at my time of life?” The rapidity with which infants acquire habits\n",
      "is amazing. Every bad habit acquired is a barrier to better habits\n",
      "later; that is why the first formation of habits in early infancy is\n",
      "so important. If the first habits are good, endless trouble is saved\n",
      "later. Moreover habits acquired very early feel, in later life, just\n",
      "like instincts; they have the same profound grip. New contrary habits\n",
      "acquired afterwards cannot have the same force; for this reason, also,\n",
      "the first habits should be a matter of grave concern.\n",
      "\n",
      "Two considerations come in when we are considering habit-formation in\n",
      "infancy. The first and paramount consideration is health; the second\n",
      "is character. We want the child to become the sort of person that will\n",
      "be liked and will be able to cope with life successfully. Fortunately,\n",
      "health and character point in the same direction: what is good for one\n",
      "is good also for the other. It is character that specially concerns\n",
      "us in this book; but health requires the same practices. Thus we are\n",
      "not faced with the difficult alternative of a healthy scoundrel or a\n",
      "diseased saint.\n",
      "\n",
      "Every educated mother nowadays knows such simple facts as the\n",
      "importance of feeding the infant at regular intervals, not whenever it\n",
      "cries. This practice has arisen because it is better for the child’s\n",
      "digestion, which is an entirely sufficient reason. But it is also\n",
      "desirable from the point of view of moral education. Infants are far\n",
      "more cunning than grown-up people are apt to suppose; if they find that\n",
      "crying produces agreeable results, they will cry. When, in later life,\n",
      "a habit of complaining causes them to be disliked instead of petted,\n",
      "they feel surprised and resentful, and the world seems to them cold and\n",
      "unsympathetic. If, however, they grow up into charming women, they will\n",
      "still be petted when they are querulous, and the bad training begun\n",
      "in childhood will be intensified. The same thing is true of rich men.\n",
      "Unless the right methods are adopted in infancy, people in later life\n",
      "will be either discontented or grasping, according to the degree of\n",
      "their power. The right moment to begin the requisite moral training is\n",
      "the moment of birth, because then it can be begun without disappointing\n",
      "expectations. At any later time it will have to fight against contrary\n",
      "habits, and will therefore be met by resentful indignation.\n"
     ]
    }
   ],
   "source": [
    "text_sample = sampler.sample_segment(p_length=4)\n",
    "print(f'Text source: {text_sample.file_path}')\n",
    "print(f'Paragraph range: {text_sample.paragraph_start} - {text_sample.paragraph_end}')\n",
    "print(f'\\n{text_sample.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d940c2d0",
   "metadata": {},
   "source": [
    "A number of text samples are retrieved (**set `n_sample` to the desired number**) and each sample comprises a set number of paragraphs (**set `m_paragraphs_per_sample` to the desired number**). These text samples are stored in the project result stored with keys like `sample_001`, `sample_002` and so on; these keys are henceforth referring to specific text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9ff6112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thus it becomes evident that the real table, if there is one, is not the\n",
      "same as what we immediately experience by sight or touch or hearing. The\n",
      "real table, if there is one, is not _immediately_ known to us at all,\n",
      "but must be an inference from what is immediately known. Hence, two very\n",
      "difficult questions at once arise; namely, (1) Is there a real table at\n",
      "all? (2) If so, what sort of object can it be?\n",
      "\n",
      "It will help us in considering these questions to have a few simple\n",
      "terms of which the meaning is definite and clear. Let us give the name\n",
      "of 'sense-data' to the things that are immediately known in sensation:\n",
      "such things as colours, sounds, smells, hardnesses, roughnesses, and\n",
      "so on. We shall give the name 'sensation' to the experience of being\n",
      "immediately aware of these things. Thus, whenever we see a colour,\n",
      "we have a sensation _of_ the colour, but the colour itself is a\n",
      "sense-datum, not a sensation. The colour is that _of_ which we are\n",
      "immediately aware, and the awareness itself is the sensation. It is\n",
      "plain that if we are to know anything about the table, it must be\n",
      "by means of the sense-data--brown colour, oblong shape, smoothness,\n",
      "etc.--which we associate with the table; but, for the reasons which have\n",
      "been given, we cannot say that the table is the sense-data, or even\n",
      "that the sense-data are directly properties of the table. Thus a problem\n",
      "arises as to the relation of the sense-data to the real table, supposing\n",
      "there is such a thing.\n",
      "\n",
      "The real table, if it exists, we will call a 'physical object'. Thus\n",
      "we have to consider the relation of sense-data to physical objects.\n",
      "The collection of all physical objects is called 'matter'. Thus our two\n",
      "questions may be re-stated as follows: (1) Is there any such thing as\n",
      "matter? (2) If so, what is its nature?\n",
      "\n",
      "The philosopher who first brought prominently forward the reasons\n",
      "for regarding the immediate objects of our senses as not existing\n",
      "independently of us was Bishop Berkeley (1685-1753). His _Three\n",
      "Dialogues between Hylas and Philonous, in Opposition to Sceptics and\n",
      "Atheists_, undertake to prove that there is no such thing as matter at\n",
      "all, and that the world consists of nothing but minds and their ideas.\n",
      "Hylas has hitherto believed in matter, but he is no match for Philonous,\n",
      "who mercilessly drives him into contradictions and paradoxes, and makes\n",
      "his own denial of matter seem, in the end, as if it were almost common\n",
      "sense. The arguments employed are of very different value: some are\n",
      "important and sound, others are confused or quibbling. But Berkeley\n",
      "retains the merit of having shown that the existence of matter is\n",
      "capable of being denied without absurdity, and that if there are any\n",
      "things that exist independently of us they cannot be the immediate\n",
      "objects of our sensations.\n",
      "\n",
      "There are two different questions involved when we ask whether matter\n",
      "exists, and it is important to keep them clear. We commonly mean by\n",
      "'matter' something which is opposed to 'mind', something which we think\n",
      "of as occupying space and as radically incapable of any sort of thought\n",
      "or consciousness. It is chiefly in this sense that Berkeley denies\n",
      "matter; that is to say, he does not deny that the sense-data which we\n",
      "commonly take as signs of the existence of the table are really signs\n",
      "of the existence of _something_ independent of us, but he does deny\n",
      "that this something is non-mental, that it is neither mind nor ideas\n",
      "entertained by some mind. He admits that there must be something which\n",
      "continues to exist when we go out of the room or shut our eyes, and that\n",
      "what we call seeing the table does really give us reason for believing\n",
      "in something which persists even when we are not seeing it. But he\n",
      "thinks that this something cannot be radically different in nature from\n",
      "what we see, and cannot be independent of seeing altogether, though it\n",
      "must be independent of _our_ seeing. He is thus led to regard the 'real'\n",
      "table as an idea in the mind of God. Such an idea has the required\n",
      "permanence and independence of ourselves, without being--as matter would\n",
      "otherwise be--something quite unknowable, in the sense that we can only\n",
      "infer it, and can never be directly and immediately aware of it.\n"
     ]
    }
   ],
   "source": [
    "print(sampler.get_paragraph_chunk(2, slice(10,15)).text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2dc4bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 5\n",
    "m_paragraphs_per_sample = 5\n",
    "\n",
    "for _ in range(n_sample):\n",
    "    sample_id = f'sample_{len(store.list_samples()) + 1:03d}'\n",
    "    segment = sampler.sample_segment(p_length=m_paragraphs_per_sample)\n",
    "    store.save_segment(sample_id, segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cc55348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys:\n",
      "============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sample_001', 'sample_002', 'sample_003', 'sample_004', 'sample_005']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sample keys:\\n============')\n",
    "store.list_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944381e6",
   "metadata": {},
   "source": [
    "## Step 1: Construct the Analyst Agents and Analyze Text Samples\n",
    "\n",
    "Send the text samples through each specialist analyst. Each produces an independent analysis from their domain expertise.\n",
    "\n",
    "**Prompt structure for each analyst is:**\n",
    "1. Preamble instruction of task ahead \n",
    "2. Analyst-specific instruction template\n",
    "3. Text to analyze\n",
    "\n",
    "Note that the execution of this can take time since it involves invoking LLMs, once per analyst and text sample. These are however independent analyses, and can therefore *in principle* be run in parallel, though the implementation below does not utilize that fact.\n",
    "\n",
    "Note that the agents are distinguished by their prompts, which are obtained via prompt models defined within the `bellatrist` project library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ffc7767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist.models import (\n",
    "    PreambleInstructionConfig,\n",
    "    PreambleTextConfig,\n",
    "    RhetoricianConfig,\n",
    "    SyntacticianConfig,\n",
    "    LexicologistConfig,\n",
    "    InformationArchitectConfig,\n",
    "    EfficiencyAuditorConfig,\n",
    "    CrossPerspectiveIntegratorConfig,\n",
    ")\n",
    "ANALYSTS = [\"rhetorician\", \"syntactician\", \"lexicologist\", \"information_architect\", \"efficiency_auditor\"]\n",
    "ANALYST_CONFIGS = {\n",
    "    \"rhetorician\": RhetoricianConfig,\n",
    "    \"syntactician\": SyntacticianConfig,\n",
    "    \"lexicologist\": LexicologistConfig,\n",
    "    \"information_architect\": InformationArchitectConfig,\n",
    "    \"efficiency_auditor\": EfficiencyAuditorConfig,\n",
    "}\n",
    "\n",
    "def build_analyst_prompt(preamble_instruction: str, analyst_prompt: str, preamble_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to construct the full prompt for an analyst.\n",
    "    \n",
    "    \"\"\"\n",
    "    return f\"{preamble_instruction}\\n\\n{analyst_prompt}\\n\\n{preamble_text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f842dd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 samples with 5 analysts each\n",
      "\n",
      "Sample: sample_001\n",
      "  Running rhetorician... ✓ (9492 chars)\n",
      "  Running syntactician... ✓ (8694 chars)\n",
      "  Running lexicologist... ✓ (8726 chars)\n",
      "  Running information_architect... ✓ (8214 chars)\n",
      "  Running efficiency_auditor... ✓ (8800 chars)\n",
      "\n",
      "Sample: sample_002\n",
      "  Running rhetorician... ✓ (9531 chars)\n",
      "  Running syntactician... ✓ (8197 chars)\n",
      "  Running lexicologist... ✓ (8874 chars)\n",
      "  Running information_architect... ✓ (8900 chars)\n",
      "  Running efficiency_auditor... ✓ (8750 chars)\n",
      "\n",
      "Sample: sample_003\n",
      "  Running rhetorician... ✓ (9846 chars)\n",
      "  Running syntactician... ✓ (8630 chars)\n",
      "  Running lexicologist... ✓ (9161 chars)\n",
      "  Running information_architect... ✓ (8655 chars)\n",
      "  Running efficiency_auditor... ✓ (9000 chars)\n",
      "\n",
      "Sample: sample_004\n",
      "  Running rhetorician... ✓ (9749 chars)\n",
      "  Running syntactician... ✓ (8803 chars)\n",
      "  Running lexicologist... ✓ (9513 chars)\n",
      "  Running information_architect... ✓ (8599 chars)\n",
      "  Running efficiency_auditor... ✓ (9449 chars)\n",
      "\n",
      "Sample: sample_005\n",
      "  Running rhetorician... ✓ (9165 chars)\n",
      "  Running syntactician... ✓ (9071 chars)\n",
      "  Running lexicologist... ✓ (8318 chars)\n",
      "  Running information_architect... ✓ (8951 chars)\n",
      "  Running efficiency_auditor... ✓ (9401 chars)\n",
      "\n",
      "All analyses complete for 5 samples\n"
     ]
    }
   ],
   "source": [
    "# Get all samples from the store\n",
    "all_samples = store.list_samples()\n",
    "print(f\"Processing {len(all_samples)} samples with {len(ANALYSTS)} analysts each\\n\")\n",
    "\n",
    "# Outer loop: iterate over each text sample\n",
    "for sample_id in all_samples:\n",
    "    print(f\"Sample: {sample_id}\")\n",
    "    \n",
    "    # Get the sample text\n",
    "    sample = store.get_sample(sample_id)\n",
    "    text = sample['text']\n",
    "    \n",
    "    # Build shared prompt components (reused across all analysts for this sample)\n",
    "    preamble_instruction = prompt_maker.render(PreambleInstructionConfig())\n",
    "    preamble_text = prompt_maker.render(PreambleTextConfig(text_to_analyze=text))\n",
    "    \n",
    "    # Inner loop: run each analyst on this sample\n",
    "    for analyst_name in ANALYSTS:\n",
    "        print(f\"  Running {analyst_name}...\", end=\" \")\n",
    "        \n",
    "        # Get analyst-specific prompt using the config class\n",
    "        analyst_config = ANALYST_CONFIGS[analyst_name]()\n",
    "        analyst_prompt = prompt_maker.render(analyst_config)\n",
    "        full_prompt = build_analyst_prompt(preamble_instruction, analyst_prompt, preamble_text)\n",
    "        \n",
    "        # Run analysis and save result\n",
    "        response = llm.complete(full_prompt)\n",
    "        store.save_analysis(sample_id, analyst_name, response.content, response.model)\n",
    "        \n",
    "        print(f\"✓ ({len(response.content)} chars)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"All analyses complete for {len(all_samples)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf8b453",
   "metadata": {},
   "source": [
    "Verification that analysis was run as expected and yielded analysis results. Excerpt of one specific analysis retrieved from project database and printed for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d13758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete: True\n",
      "\n",
      "Sample: sample_001\n",
      "Source: File 2, paragraphs 32-37\n",
      "Analyses available: ['efficiency_auditor', 'information_architect', 'lexicologist', 'rhetorician', 'syntactician']\n",
      "\n",
      "--- Rhetorician Output (first 500 chars) ---\n",
      "### RHETORICAL STRATEGY AND STANCE ANALYSIS  \n",
      "**Text Segment:** Philosophical prose on the existence of an external world and other minds.\n",
      "\n",
      "---\n",
      "\n",
      "#### 1. **WRITER'S POSITION**\n",
      "\n",
      "- **Persona:** The writer adopts an **authoritative yet measured** persona—calm, reflective, and rigorously logical. The tone is that of a **philosopher as guide**, not a polemicist. There is a clear commitment to intellectual honesty and precision.\n",
      "\n",
      "  > Example: “In one sense it must be admitted that we can never prove th\n"
     ]
    }
   ],
   "source": [
    "sample_id = 'sample_001'\n",
    "is_complete = store.is_complete(sample_id, ANALYSTS)\n",
    "print(f\"Analysis complete: {is_complete}\")\n",
    "\n",
    "# Retrieve sample and all analyses (both are now dicts)\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "print(f\"\\nSample: {sample['sample_id']}\")\n",
    "print(f\"Source: File {sample['file_index']}, paragraphs {sample['paragraph_start']}-{sample['paragraph_end']}\")\n",
    "print(f\"Analyses available: {list(analyses.keys())}\")\n",
    "\n",
    "# Examine one analysis\n",
    "print(f\"\\n--- Rhetorician Output (first 500 chars) ---\")\n",
    "print(analyses.get(\"rhetorician\", \"Not found\")[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601bb693",
   "metadata": {},
   "source": [
    "## Step 2a: Pattern Recognition (Cross-Perspective Integration) per Text Sample\n",
    "\n",
    "Synthesize all analyst perspectives to identify interactions, tensions, and load-bearing features. This is a per-text-cross-analyst transformation. This looks to integrate multiple perspectives on each text sample and indirectly distill the information content in the assessments of the text samples. \n",
    "\n",
    "If only a subset of samples are to be analyzed, filter or slice the list `samples_to_analyze`, which is a list of sample IDs, as created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b40d1b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_001', 'sample_002', 'sample_003', 'sample_004', 'sample_005']\n"
     ]
    }
   ],
   "source": [
    "samples_to_analyze = store.list_samples()\n",
    "print(samples_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5be65f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pattern_prompt_from_(text: str, analyses: dict):\n",
    "    \"\"\"Convenience function to create the prompt, since the prompt depends on the kinds of analysts to integrate.\n",
    "    \n",
    "    \"\"\"\n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "    analyst_info = {}\n",
    "    for analyst_name in ANALYSTS:\n",
    "        config_class = ANALYST_CONFIGS[analyst_name]\n",
    "        analyst_info[analyst_name] = {\n",
    "            'analysis': analyses[analyst_name],\n",
    "            'analyst_descr_short': config_class.description()\n",
    "        }\n",
    "\n",
    "    pattern_config = CrossPerspectiveIntegratorConfig(\n",
    "        original_text=sample['text'],\n",
    "        analysts=analyst_info\n",
    "    )\n",
    "    return prompt_maker.render(pattern_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf8e67",
   "metadata": {},
   "source": [
    "Note that this loop can take time to execute, since LLMs are called. Each analysis is independent and can therefore in principle be parallelized, though the implementation below does not do that.\n",
    "\n",
    "Note also that the cross-perspective per-text result are stored in the result store, keyed on the sample ID and the analyst kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f784bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cross-Perspective Integrator agent for sample_001... ✓ (8788 chars)\n",
      "Running Cross-Perspective Integrator agent for sample_002... ✓ (8812 chars)\n",
      "Running Cross-Perspective Integrator agent for sample_003... ✓ (9405 chars)\n",
      "Running Cross-Perspective Integrator agent for sample_004... ✓ (9246 chars)\n",
      "Running Cross-Perspective Integrator agent for sample_005... ✓ (9321 chars)\n"
     ]
    }
   ],
   "source": [
    "for sample_id in samples_to_analyze:\n",
    "    print(f\"Running Cross-Perspective Integrator agent for {sample_id}...\", end=\" \")\n",
    "    \n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "    pattern_prompt = build_pattern_prompt_from_(text=sample['text'], analyses=analyses)\n",
    "\n",
    "    pattern_response = llm.complete(pattern_prompt)\n",
    "    \n",
    "    # Store pattern recognition result in result store\n",
    "    store.save_analysis(\n",
    "        sample_id, \n",
    "        CrossPerspectiveIntegratorConfig.analyst_name(), \n",
    "        pattern_response.content, \n",
    "        pattern_response.model\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ ({len(pattern_response.content)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69ba6f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cross_perspective_integrator', 'efficiency_auditor', 'information_architect', 'lexicologist', 'rhetorician', 'syntactician'])\n",
      "\n",
      "--- Pattern Analyst Output (first 2000 chars) ---\n",
      "# **CROSS-PERSPECTIVE INTEGRATION ANALYSIS**\n",
      "\n",
      "---\n",
      "\n",
      "## **I. EXTRACTED TECHNIQUES**\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Name: Core Claim Anchoring**  \n",
      "**Specification:** Place the main philosophical assertion in the first clause of the paragraph as a definitional or normative claim, using a copular structure (\"X is Y\") to establish an axiomatic premise. Follow immediately with elaboration, consequence, or contrast.  \n",
      "**Example from text:**  \n",
      "> \"The good which it concerns us to remember is the good which it lies in our power to create--the good in our own lives and in our attitude towards the world.\"  \n",
      "**Source observations:**  \n",
      "- *Information_architect*: \"Topic sentence: Explicit and initial... sets the moral-psychological frame.\"  \n",
      "- *Rhetorician*: \"Foundational ethical-psychological claim presented axiomatically.\"  \n",
      "- *Syntactician*: \"Main clause: 'The good ... is the good ...' — balanced structure with two parallel relative clauses.\"  \n",
      "**Confidence:** HIGH — multiple analysts converge on structural, rhetorical, and informational primacy of opening claims.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Name: Subordinate Consequence Stacking**  \n",
      "**Specification:** After establishing a core claim, attach a single relative clause (introduced by \"which\") that contains a concessive clause followed by multiple negative consequences in parallel structure. Embed 2–3 subordinate clauses within this structure, with depth up to three levels. Use cumulative coordination (\"can impair... and destroy...\") to amplify gravity.  \n",
      "**Example from text:**  \n",
      "> \"Insistence on belief in an external realisation of the good is a form of self-assertion, which, while it cannot secure the external good which it desires, can seriously impair the inward good which lies within our power, and destroy that reverence towards fact which constitutes both what is valuable in humility and what is fruitful in the scientific temper.\"  \n",
      "**Source observations:**  \n",
      "- *Syntactician*: \"One main clause, one major relative clause, and four embedded subordinate clau\n"
     ]
    }
   ],
   "source": [
    "sample_id = 'sample_005'\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "print(analyses.keys())\n",
    "\n",
    "print(f\"\\n--- Pattern Analyst Output (first 2000 chars) ---\")\n",
    "print(analyses.get(\"cross_perspective_integrator\", \"Not found\")[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e4ea2",
   "metadata": {},
   "source": [
    "## Stage 2b: Cross-Text Synthesis of Integrated Analyses\n",
    "\n",
    "Patterns that appear across multiple text analyses are synthesized. This stage takes all the cross-perspective integration outputs and identifies overaching techniques, complementary findings, and so on, in order to construct a highly specific conclusion on the techniques that are employed in the text samples. It attempts in other words a synthesis of all analysis, across perspectives and across text samples.\n",
    "\n",
    "This is a single document. In order to track provenance, the text samples and analyst types that went into its construction are gathered and included in the storage in the project database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f445dafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 cross-perspective integration results\n",
      "Sample IDs: ['sample_001', 'sample_002', 'sample_003', 'sample_004', 'sample_005']\n"
     ]
    }
   ],
   "source": [
    "from belletrist.models import CrossTextSynthesizerConfig\n",
    "\n",
    "# Get all samples that have cross-perspective integration results\n",
    "all_samples = store.list_samples()\n",
    "pattern_analyst = CrossPerspectiveIntegratorConfig.analyst_name()\n",
    "\n",
    "# Retrieve all pattern recognition analyses\n",
    "integrated_analyses = {}\n",
    "for sample_id in all_samples:\n",
    "    pattern_analysis = store.get_analysis(sample_id, pattern_analyst)\n",
    "    if pattern_analysis:\n",
    "        integrated_analyses[sample_id] = pattern_analysis\n",
    "    else:\n",
    "        print(f\"⚠ Sample {sample_id} missing cross-perspective integration results\")\n",
    "\n",
    "print(f\"Found {len(integrated_analyses)} cross-perspective integration results\")\n",
    "print(f\"Sample IDs: {list(integrated_analyses.keys())}\")\n",
    "\n",
    "if len(integrated_analyses) < 2:\n",
    "    print(f\"\\n⚠ Need at least 2 pattern recognition analyses for cross-text synthesis. Got {len(integrated_analyses)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59afbe0",
   "metadata": {},
   "source": [
    "This is where the analysis is run. This can take time, since it requires running an LLM, however, only one LLM call in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19412601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Cross-Text Synthesis... ✓ (8424 chars)\n",
      "Saved as: cross_text_synthesis_001\n"
     ]
    }
   ],
   "source": [
    "cross_text_config = CrossTextSynthesizerConfig(\n",
    "    integrated_analyses=integrated_analyses\n",
    ")\n",
    "cross_text_prompt = prompt_maker.render(cross_text_config)\n",
    "    \n",
    "print(\"Running Cross-Text Synthesis...\", end=\" \")\n",
    "cross_text_response = llm.complete(cross_text_prompt)\n",
    "print(f\"✓ ({len(cross_text_response.content)} chars)\")\n",
    "    \n",
    "# Save to ResultStore with auto-generated ID and full provenance\n",
    "sample_contributions = [(sid, pattern_analyst) for sid in integrated_analyses.keys()]\n",
    "cross_text_id = store.save_synthesis(\n",
    "    synthesis_type=cross_text_config.synthesis_type(),\n",
    "    output=cross_text_response.content,\n",
    "    model=cross_text_response.model,\n",
    "    sample_contributions=sample_contributions,\n",
    "    config=cross_text_config\n",
    ")\n",
    "print(f\"Saved as: {cross_text_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b77267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  # Clean up orphaned records caused by disabled foreign keys\n",
    "#  cursor = store.conn.execute(\"\"\"\n",
    "#      DELETE FROM synthesis_samples\n",
    "#      WHERE synthesis_id NOT IN (SELECT synthesis_id FROM syntheses)\n",
    "#  \"\"\")\n",
    "#  print(f\"Deleted {cursor.rowcount} orphaned synthesis_samples records\")#\n",
    "#\n",
    "#  cursor = store.conn.execute(\"\"\"\n",
    "#      DELETE FROM synthesis_metadata\n",
    "#      WHERE synthesis_id NOT IN (SELECT synthesis_id FROM syntheses)\n",
    "#  \"\"\")\n",
    "#  print(f\"Deleted {cursor.rowcount} orphaned synthesis_metadata records\")#\n",
    "#\n",
    "#  store.conn.commit()\n",
    "#\n",
    "#  # Verify cleanup\n",
    "#  orphans = store.conn.execute(\"\"\"\n",
    "#      SELECT COUNT(*) FROM synthesis_samples ss\n",
    "#      LEFT JOIN syntheses s ON ss.synthesis_id = s.synthesis_id\n",
    "#      WHERE s.synthesis_id IS NULL\n",
    "#  \"\"\").fetchone()[0]\n",
    "#  print(f\"Remaining orphaned records: {orphans}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7486344b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_contributions = [(sid, pattern_analyst) for sid in integrated_analyses.keys()]\n",
    "#cross_text_id = store.save_synthesis(\n",
    "#    synthesis_type=cross_text_config.synthesis_type(),\n",
    "#    output=cross_text_response.content,\n",
    "#    model=cross_text_response.model,\n",
    "#    sample_contributions=sample_contributions,\n",
    "#    config=cross_text_config\n",
    "#)\n",
    "#print(f\"Saved as: {cross_text_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "466df8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Text Synthesis (first 1000 chars) ---\n",
      "# **CROSS-TEXT SYNTHESIS: PROVISIONAL PRINCIPLES FOR PHILOSOPHICAL PROSE**\n",
      "\n",
      "---\n",
      "\n",
      "## **I. STABLE CORE PATTERNS**  \n",
      "*(Techniques appearing in ≥4 of 5 texts)*\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Main-Claim-First Clause Anchoring**  \n",
      "**Frequency**: 5/5  \n",
      "**Description**: Core assertions are placed in independent clauses at or near the beginning of sentences. When subordination occurs, the main clause (containing the primary claim) precedes or immediately follows the subordinate clause.  \n",
      "**Mechanical Specs**:  \n",
      "- Main clause: 6–15 words  \n",
      "- Subordinate clause: 15–35 words  \n",
      "- Core claim never embedded; always in main or initial clause  \n",
      "**Examples**:  \n",
      "> \"Other people are represented to me by certain sense-data...\"  \n",
      "> \"Reason is a harmonising, controlling force rather than a creative one.\"  \n",
      "> \"The first characteristic of two appearances... is continuity.\"  \n",
      "> \"We can now begin to understand one of the fundamental differences between physics and psychology.\"  \n",
      "> \"The good which it concerns us to remember is the good which it lies in our power to create...\"  \n",
      "\n",
      "**Centrality**: **Foundational**  \n",
      "**Construction Sequence**:  \n",
      "1. Identify core proposition.  \n",
      "2. Frame as subject-predicate in main clause.  \n",
      "3. Add qualifying clauses **after** or as fronted adverbials, never before.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Concrete-Abstract Alternation Pattern**  \n",
      "**Frequency**: 5/5  \n",
      "**Consolidated from**:  \n",
      "- Sample_001: Concrete-Abstract Alternation  \n",
      "- Sample_003: Semantic Anchoring via Concrete-Abstract Pairing  \n",
      "- Sample_004: Analogy-as-Anchoring  \n",
      "- Sample_005: Contrastive Ideal Framing (with grounding metaphors)  \n",
      "- Sample_002: Survival-Value Grounding  \n",
      "\n",
      "**Mechanical Specs**:  \n",
      "- Ratio: ~1:1 per major claim  \n",
      "- Abstract claim → immediate concrete illustration (6–20 words)  \n",
      "- Illustration uses Germanic-rooted words, sensory/domestic domain  \n",
      "**Examples**:  \n",
      "> \"...becomes utterly inexplicable when regarded as mere movements and changes of patches of colour, which are as incapable of hunger as a triangle is of playi\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Cross-Text Synthesis (first 1000 chars) ---\")\n",
    "print(cross_text_response.content[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b08b3a9",
   "metadata": {},
   "source": [
    "## Stage 3: Synthesize Prescriptive Writing Document\n",
    "\n",
    "The final stage converts the descriptive cross-text synthesis into actionable prescriptive writing principles. This generates a style guide that can be used to instruct an LLM to write in a similar style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de7cdc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cross-text synthesis: cross_text_synthesis_001\n"
     ]
    }
   ],
   "source": [
    "cross_text_syntheses = store.list_syntheses('cross_text_synthesis')\n",
    "cross_text_synthesis_to_analyze_id = cross_text_syntheses[-1]\n",
    "print(f\"Using cross-text synthesis: {cross_text_synthesis_to_analyze_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e18c050",
   "metadata": {},
   "source": [
    "The following step executes the LLM and can therefore take time. The result is stored for provenance tracking in the project database alongside relevant metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fbf5cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Synthesizer of Writing Style Principles... ✓ (9632 chars)\n",
      "Saved as: principles_guide_001\n"
     ]
    }
   ],
   "source": [
    "from belletrist.models import SynthesizerOfPrinciplesConfig\n",
    "\n",
    "# Build principles guide config and prompt\n",
    "cross_text_synthesis_to_analyze = store.get_synthesis(cross_text_synthesis_to_analyze_id)\n",
    "principles_config = SynthesizerOfPrinciplesConfig(\n",
    "    synthesis_document=cross_text_synthesis_to_analyze['output']\n",
    ")\n",
    "principles_prompt = prompt_maker.render(principles_config)\n",
    "    \n",
    "# Run principles synthesis\n",
    "print(\"Running Synthesizer of Writing Style Principles...\", end=\" \")\n",
    "principles_response = llm.complete(principles_prompt)\n",
    "print(f\"✓ ({len(principles_response.content)} chars)\")\n",
    "    \n",
    "# Save to ResultStore with parent linkage (inherits provenance)\n",
    "principles_id = store.save_synthesis(\n",
    "    synthesis_type=principles_config.synthesis_type(),\n",
    "    output=principles_response.content,\n",
    "    model=principles_response.model,\n",
    "    sample_contributions=[],  # Inherited from parent\n",
    "    config=principles_config,\n",
    "    parent_synthesis_id=cross_text_synthesis_to_analyze_id\n",
    ")\n",
    "print(f\"Saved as: {principles_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c41bb317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Principles Guide (first 2000 chars) ---\n",
      "# A GUIDE TO PHILOSOPHICAL CLARITY  \n",
      "## Principles Extracted from Pattern Analysis\n",
      "\n",
      "---\n",
      "\n",
      "### PART I: FOUNDATIONS  \n",
      "## **Core Principles**\n",
      "\n",
      "---\n",
      "\n",
      "**1. Anchor every claim in a simple, early main clause.**  \n",
      "Place your core assertion in a grammatically independent clause of 6–15 words, positioned at or near the beginning of the sentence. Do not bury the claim inside subordinate constructions or after lengthy qualifications. This creates immediate cognitive grounding, allowing the reader to process complexity *in relation to* a stable proposition.  \n",
      "> \"Other people are represented to me by certain sense-data...\"  \n",
      "> \"Reason is a harmonising, controlling force rather than a creative one.\"  \n",
      "> \"The first characteristic of two appearances... is continuity.\"  \n",
      "When qualifications are necessary, subordinate them—do not let them precede or enclose the main claim. This structure prevents ambiguity and maintains epistemic authority.  \n",
      "**Dependencies**: Enables controlled hedging, supports paragraph coherence, and makes complex syntax navigable. Without this, even precise ideas become cognitively unstable.\n",
      "\n",
      "---\n",
      "\n",
      "**2. Follow every abstract claim with a concrete illustration.**  \n",
      "After stating any abstract idea—epistemic, metaphysical, or normative—immediately ground it with a brief (6–20 word), sensory-based analogy from shared perceptual or lived experience. Use Germanic-rooted vocabulary and familiar domains (domestic, natural, bodily).  \n",
      "> \"...becomes utterly inexplicable when regarded as mere movements and changes of patches of colour, which are as incapable of hunger as a triangle is of playing football.\"  \n",
      "> \"The savage deceived by false friendship is likely to pay for his mistake with his life...\"  \n",
      "> \"Just as a photographic plate receives a different impression... so a brain receives a different impression...\"  \n",
      "This alternation prevents abstraction from floating free of understanding. The concrete instance acts as a perceptual anchor, making the abstract claim *thinkable*\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Principles Guide (first 2000 chars) ---\")\n",
    "print(principles_response.content[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32af6c8",
   "metadata": {},
   "source": [
    "## Querying Synthesis Metadata\n",
    "\n",
    "The ResultStore tracks full provenance for all syntheses. Query metadata to understand what samples, analysts, and models contributed to each synthesis.\n",
    "\n",
    "**This code is for convenience and not required to generate the writing instruction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e3bbd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Syntheses\n",
      "==================================================\n",
      "\n",
      "cross_text_synthesis: 1 found\n",
      "  - cross_text_synthesis_001\n",
      "\n",
      "principles_guide: 1 found\n",
      "  - principles_guide_001\n",
      "\n",
      "\n",
      "Detailed Metadata Example\n",
      "==================================================\n",
      "\n",
      "Synthesis ID: principles_guide_001\n",
      "Type: principles_guide\n",
      "Model: together_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput\n",
      "Created: 2025-11-25T18:08:35.705147\n",
      "Parent: cross_text_synthesis_001\n",
      "\n",
      "Metadata:\n",
      "  Samples: 0\n",
      "  Sample IDs: []\n",
      "  Model homogeneous: True\n",
      "  Models used: ['together_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput']\n",
      "\n",
      "\n",
      "Full Provenance Tree\n",
      "==================================================\n",
      "\n",
      "Principles Guide: principles_guide_001\n",
      "  Created: 2025-11-25T18:08:35.705147\n",
      "  Model: together_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput\n",
      "\n",
      "  Parent (Cross-Text): cross_text_synthesis_001\n",
      "    Sample contributions: 5\n",
      "      - sample_001 / cross_perspective_integrator\n",
      "      - sample_002 / cross_perspective_integrator\n",
      "      - sample_003 / cross_perspective_integrator\n",
      "      ... and 2 more\n"
     ]
    }
   ],
   "source": [
    "# List all syntheses\n",
    "print(\"All Syntheses\")\n",
    "print(\"=\" * 50)\n",
    "for synth_type in ['cross_text_synthesis', 'principles_guide']:\n",
    "    syntheses = store.list_syntheses(synth_type)\n",
    "    print(f\"\\n{synth_type}: {len(syntheses)} found\")\n",
    "    for synth_id in syntheses:\n",
    "        print(f\"  - {synth_id}\")\n",
    "\n",
    "# Get detailed metadata for a synthesis\n",
    "if store.list_syntheses():\n",
    "    print(\"\\n\\nDetailed Metadata Example\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get first principles guide (or first cross-text if none)\n",
    "    principles = store.list_syntheses('principles_guide')\n",
    "    if principles:\n",
    "        synth_id = principles[0]\n",
    "    else:\n",
    "        synth_id = store.list_syntheses()[0]\n",
    "    \n",
    "    synth_with_meta = store.get_synthesis_with_metadata(synth_id)\n",
    "    \n",
    "    print(f\"\\nSynthesis ID: {synth_with_meta['synthesis_id']}\")\n",
    "    print(f\"Type: {synth_with_meta['type']}\")\n",
    "    print(f\"Model: {synth_with_meta['model']}\")\n",
    "    print(f\"Created: {synth_with_meta['created_at']}\")\n",
    "    print(f\"Parent: {synth_with_meta['parent_id']}\")\n",
    "    \n",
    "    if synth_with_meta.get('metadata'):\n",
    "        meta = synth_with_meta['metadata']\n",
    "        print(f\"\\nMetadata:\")\n",
    "        print(f\"  Samples: {meta['num_samples']}\")\n",
    "        print(f\"  Sample IDs: {meta['sample_ids']}\")\n",
    "        print(f\"  Model homogeneous: {meta['is_homogeneous_model']}\")\n",
    "        print(f\"  Models used: {meta['models_used']}\")\n",
    "\n",
    "# Get full provenance tree\n",
    "if store.list_syntheses('principles_guide'):\n",
    "    print(\"\\n\\nFull Provenance Tree\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for p_id in store.list_syntheses('principles_guide'):\n",
    "        provenance = store.get_synthesis_provenance(p_id)\n",
    "    \n",
    "        print(f\"\\nPrinciples Guide: {provenance['synthesis']['synthesis_id']}\")\n",
    "        print(f\"  Created: {provenance['synthesis']['created_at']}\")\n",
    "        print(f\"  Model: {provenance['synthesis']['model']}\")\n",
    "    \n",
    "        if provenance['parent']:\n",
    "            parent = provenance['parent']\n",
    "            print(f\"\\n  Parent (Cross-Text): {parent['synthesis']['synthesis_id']}\")\n",
    "            print(f\"    Sample contributions: {len(parent['sample_contributions'])}\")\n",
    "            for sample_id, analyst in parent['sample_contributions'][:3]:\n",
    "                print(f\"      - {sample_id} / {analyst}\")\n",
    "            if len(parent['sample_contributions']) > 3:\n",
    "                print(f\"      ... and {len(parent['sample_contributions']) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc617e",
   "metadata": {},
   "source": [
    "## Exporting Syntheses to Filesystem\n",
    "\n",
    "Export final syntheses to text files with YAML metadata headers for consumption by other tools or LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80cf5a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: outputs/cross_text_synthesis_001.txt\n",
      "Exported: outputs/principles_guide_001.txt\n",
      "Exported for style evaluation: outputs/derived_style_instructions.txt\n",
      "\n",
      "All syntheses exported to /Users/andersohrn/PycharmProjects/russell_writes/outputs\n"
     ]
    }
   ],
   "source": [
    "# Create outputs directory\n",
    "outputs_dir = Path(\"outputs\")\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export cross-text synthesis\n",
    "cross_text_syntheses = store.list_syntheses('cross_text_synthesis')\n",
    "if cross_text_syntheses:\n",
    "    for synth_id in cross_text_syntheses:\n",
    "        output_path = outputs_dir / f\"{synth_id}.txt\"\n",
    "        store.export_synthesis(synth_id, output_path, metadata_format='yaml')\n",
    "        print(f\"Exported: {output_path}\")\n",
    "\n",
    "# Export principles guide\n",
    "principles_guides = store.list_syntheses('principles_guide')\n",
    "if principles_guides:\n",
    "    for synth_id in principles_guides:\n",
    "        output_path = outputs_dir / f\"{synth_id}.txt\"\n",
    "        store.export_synthesis(synth_id, output_path, metadata_format='yaml')\n",
    "        print(f\"Exported: {output_path}\")\n",
    "        \n",
    "        # Also create a special \"derived_style_instructions.txt\" for style_evaluation.ipynb\n",
    "        if synth_id == principles_guides[-1]:  # Use latest\n",
    "            instructions_path = outputs_dir / \"derived_style_instructions.txt\"\n",
    "            store.export_synthesis(synth_id, instructions_path, metadata_format='yaml')\n",
    "            print(f\"Exported for style evaluation: {instructions_path}\")\n",
    "\n",
    "print(f\"\\nAll syntheses exported to {outputs_dir.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
