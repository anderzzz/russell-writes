{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9655248a",
   "metadata": {},
   "source": [
    "# Multi-Analyst Text Analysis Pipeline\n",
    "\n",
    "This notebook demonstrates the full pipeline for analyzing text through multiple specialist lenses (rhetorician, syntactician, lexicologist, etc.) and synthesizing their observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b16aa",
   "metadata": {},
   "source": [
    "## Installations and Preparations\n",
    "First, external modules are installed and ensured to be in working order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd03a7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:22.033495Z",
     "start_time": "2025-11-19T18:02:20.240014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm==1.79.3 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (1.79.3)\n",
      "Requirement already satisfied: pydantic==2.7.4 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (2.7.4)\n",
      "Requirement already satisfied: jinja2>=3.1.0 in ./venv/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: aiohttp>=3.10 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (3.13.2)\n",
      "Requirement already satisfied: click in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (4.25.1)\n",
      "Requirement already satisfied: openai>=1.99.5 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in ./venv/lib/python3.11/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.22.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.11/site-packages (from jinja2>=3.1.0->-r requirements.txt (line 3)) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.22.0)\n",
      "Requirement already satisfied: anyio in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (4.11.0)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.0.9)\n",
      "Requirement already satisfied: idna in ./venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./venv/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.29.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in ./venv/lib/python3.11/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.32.5)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in ./venv/lib/python3.11/site-packages (from tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.1.4)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (6.0.3)\n",
      "Requirement already satisfied: shellingham in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in ./venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm==1.79.3->-r requirements.txt (line 1)) (0.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Optional: Install requirements if running in a fresh kernel\n",
    "# Uncomment if needed:\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Or install individual packages:\n",
    "# !pip install litellm pydantic jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a24f778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providers\n",
      "=========\n",
      "* openai\n",
      "* openai_like\n",
      "* bytez\n",
      "* xai\n",
      "* custom_openai\n",
      "* text-completion-openai\n",
      "* cohere\n",
      "* cohere_chat\n",
      "* clarifai\n",
      "* anthropic\n",
      "* anthropic_text\n",
      "* replicate\n",
      "* huggingface\n",
      "* together_ai\n",
      "* datarobot\n",
      "* openrouter\n",
      "* cometapi\n",
      "* vertex_ai\n",
      "* vertex_ai_beta\n",
      "* gemini\n",
      "* ai21\n",
      "* baseten\n",
      "* azure\n",
      "* azure_text\n",
      "* azure_ai\n",
      "* sagemaker\n",
      "* sagemaker_chat\n",
      "* bedrock\n",
      "* vllm\n",
      "* nlp_cloud\n",
      "* petals\n",
      "* oobabooga\n",
      "* ollama\n",
      "* ollama_chat\n",
      "* deepinfra\n",
      "* perplexity\n",
      "* mistral\n",
      "* groq\n",
      "* nvidia_nim\n",
      "* cerebras\n",
      "* baseten\n",
      "* ai21_chat\n",
      "* volcengine\n",
      "* codestral\n",
      "* text-completion-codestral\n",
      "* deepseek\n",
      "* sambanova\n",
      "* maritalk\n",
      "* cloudflare\n",
      "* fireworks_ai\n",
      "* friendliai\n",
      "* watsonx\n",
      "* watsonx_text\n",
      "* triton\n",
      "* predibase\n",
      "* databricks\n",
      "* empower\n",
      "* github\n",
      "* custom\n",
      "* litellm_proxy\n",
      "* hosted_vllm\n",
      "* llamafile\n",
      "* lm_studio\n",
      "* galadriel\n",
      "* gradient_ai\n",
      "* github_copilot\n",
      "* novita\n",
      "* meta_llama\n",
      "* featherless_ai\n",
      "* nscale\n",
      "* nebius\n",
      "* dashscope\n",
      "* moonshot\n",
      "* v0\n",
      "* heroku\n",
      "* oci\n",
      "* morph\n",
      "* lambda_ai\n",
      "* vercel_ai_gateway\n",
      "* wandb\n",
      "* ovhcloud\n",
      "* lemonade\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa6720c",
   "metadata": {},
   "source": [
    "## Initialize Base Objects\n",
    "Set up connections to a Large Language Model provider via `litellm` model router. Also, setup up tools to retrieve text data to be part of the context window, that is, instruction prompts and texts to analyze. A basic result storage is also initialized.\n",
    "\n",
    "The LLM to use is set by the `model_string`, which is constructed as `<provider>/<model>`, the providers defined by the `litellm` package, see in particular `litellm.LITELLM_CHAT_PROVIDERS`. The API key to the provider should be stored in an environment variable with name defined in `model_provider_api_key_env_var`. Do **not** store the API key as a string variable directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76e0efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_string = 'mistral/mistral-large-2411'\n",
    "model_provider_api_key_env_var = 'MISTRAL_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2507394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:29.927627Z",
     "start_time": "2025-11-19T18:02:29.858316Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from belletrist import LLM, LLMConfig, PromptMaker, DataSampler, ResultStore\n",
    "\n",
    "llm = LLM(LLMConfig(\n",
    "    model=model_string,\n",
    "    api_key=os.environ.get(model_provider_api_key_env_var)\n",
    "))\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler()\n",
    "store = ResultStore(Path(f\"{os.getcwd()}/belletrist_storage.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d23dd",
   "metadata": {},
   "source": [
    "In case a clean run is done, the old contents of the database are discarded with a result store reset. Do not run the rest if content should be preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f94b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59b6a7",
   "metadata": {},
   "source": [
    "## Generate and Store Text Samples to be Analyzed\n",
    "\n",
    "A random text sample is taken from the data corpus and stored with full provenance (which file, which paragraphs). Each sample is an instance of `TextSegment`.\n",
    "\n",
    "The sample size is set by the variable `n_sample` and each sample comprises `m_paragraphs_per_sample` number of consecutive paragraphs.\n",
    "\n",
    "If non-random text samples are preferred, use the `get_paragraph_chunk` method of the `DataSampler` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e1e4571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text source: /Users/andersohrn/PycharmProjects/russell_writes/data/russell/education_and_the_good_life.txt\n",
      "Paragraph range: 418 - 422\n",
      "\n",
      "[18] Although Miss McMillan is American, I understand that the\n",
      "importance of nursery-schools is even less appreciated in America than\n",
      "in England. As, however, there are not the financial difficulties which\n",
      "exist in Europe, it may be hoped that the movement will soon become\n",
      "wide-spread in the United States. There is no mention of it in O’Shea’s\n",
      "book, though the need of it is evident from his remarks on p. 182.\n",
      "\n",
      "[19] See Montessori, “The Montessori Method” (Heinemann, 1912), p. 42\n",
      "ff.\n",
      "\n",
      "[20] O’Shea, p. 386.\n",
      "\n",
      "[21] Are we to infer that culture consists in carrying a hip-flask? The\n",
      "definition seems applicable.\n"
     ]
    }
   ],
   "source": [
    "text_sample = sampler.sample_segment(p_length=4)\n",
    "print(f'Text source: {text_sample.file_path}')\n",
    "print(f'Paragraph range: {text_sample.paragraph_start} - {text_sample.paragraph_end}')\n",
    "print(f'\\n{text_sample.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81d8548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 2\n",
    "m_paragraphs_per_sample = 5\n",
    "\n",
    "for _ in range(n_sample):\n",
    "    sample_id = f'sample_{len(store.list_samples()) + 1:03d}'\n",
    "    segment = sampler.sample_segment(p_length=m_paragraphs_per_sample)\n",
    "    store.save_segment(sample_id, segment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6df1f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys:\n",
      "============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sample_001', 'sample_002']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sample keys:\\n============')\n",
    "store.list_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e684b",
   "metadata": {},
   "source": [
    "## Step 1: Construct the Analyst Agents and Analyze Text Samples\n",
    "\n",
    "Send the text samples through each specialist analyst. Each produces an independent analysis from their domain expertise.\n",
    "\n",
    "**Prompt structure for caching optimization:**\n",
    "1. Preamble instruction (static)\n",
    "2. Analyst-specific template (static per analyst)\n",
    "3. Text to analyze (dynamic)\n",
    "\n",
    "Note that the execution of this can take time since it involves invoking LLMs. These are however independent analyses, and can therefore in principle be run in parallel, though the implementation below does not utilize that fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be6fadb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist.models import (\n",
    "    PreambleInstructionConfig,\n",
    "    PreambleTextConfig,\n",
    "    RhetoricianConfig,\n",
    "    SyntacticianConfig,\n",
    "    LexicologistConfig,\n",
    "    InformationArchitectConfig,\n",
    "    EfficiencyAuditorConfig,\n",
    "    CrossPerspectiveIntegratorConfig,\n",
    ")\n",
    "ANALYSTS = [\"rhetorician\", \"syntactician\", \"lexicologist\", \"information_architect\", \"efficiency_auditor\"]\n",
    "ANALYST_CONFIGS = {\n",
    "    \"rhetorician\": RhetoricianConfig,\n",
    "    \"syntactician\": SyntacticianConfig,\n",
    "    \"lexicologist\": LexicologistConfig,\n",
    "    \"information_architect\": InformationArchitectConfig,\n",
    "    \"efficiency_auditor\": EfficiencyAuditorConfig,\n",
    "}\n",
    "\n",
    "def build_analyst_prompt(preamble_instruction: str, analyst_prompt: str, preamble_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to construct the full prompt for an analyst.\n",
    "    \n",
    "    \"\"\"\n",
    "    return f\"{preamble_instruction}\\n\\n{analyst_prompt}\\n\\n{preamble_text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b2f43fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2 samples with 5 analysts each\n",
      "\n",
      "Sample: sample_001\n",
      "  Running rhetorician... ✓ (5429 chars)\n",
      "  Running syntactician... ✓ (7609 chars)\n",
      "  Running lexicologist... ✓ (7375 chars)\n",
      "  Running information_architect... ✓ (7518 chars)\n",
      "  Running efficiency_auditor... ✓ (6641 chars)\n",
      "\n",
      "Sample: sample_002\n",
      "  Running rhetorician... ✓ (3019 chars)\n",
      "  Running syntactician... ✓ (6240 chars)\n",
      "  Running lexicologist... ✓ (5410 chars)\n",
      "  Running information_architect... ✓ (4649 chars)\n",
      "  Running efficiency_auditor... ✓ (7724 chars)\n",
      "\n",
      "All analyses complete for 2 samples\n"
     ]
    }
   ],
   "source": [
    "# Get all samples from the store\n",
    "all_samples = store.list_samples()\n",
    "print(f\"Processing {len(all_samples)} samples with {len(ANALYSTS)} analysts each\\n\")\n",
    "\n",
    "# Outer loop: iterate over each text sample\n",
    "for sample_id in all_samples:\n",
    "    print(f\"Sample: {sample_id}\")\n",
    "    \n",
    "    # Get the sample text\n",
    "    sample = store.get_sample(sample_id)\n",
    "    text = sample['text']\n",
    "    \n",
    "    # Build shared prompt components (reused across all analysts for this sample)\n",
    "    preamble_instruction = prompt_maker.render(PreambleInstructionConfig())\n",
    "    preamble_text = prompt_maker.render(PreambleTextConfig(text_to_analyze=text))\n",
    "    \n",
    "    # Inner loop: run each analyst on this sample\n",
    "    for analyst_name in ANALYSTS:\n",
    "        print(f\"  Running {analyst_name}...\", end=\" \")\n",
    "        \n",
    "        # Get analyst-specific prompt using the config class\n",
    "        analyst_config = ANALYST_CONFIGS[analyst_name]()\n",
    "        analyst_prompt = prompt_maker.render(analyst_config)\n",
    "        \n",
    "        # Build full prompt using helper function\n",
    "        full_prompt = build_analyst_prompt(preamble_instruction, analyst_prompt, preamble_text)\n",
    "        \n",
    "        # Run analysis\n",
    "        response = llm.complete(full_prompt)\n",
    "        store.save_analysis(sample_id, analyst_name, response.content, response.model)\n",
    "        \n",
    "        print(f\"✓ ({len(response.content)} chars)\")\n",
    "    \n",
    "    print()  # Blank line between samples\n",
    "\n",
    "print(f\"All analyses complete for {len(all_samples)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5001a3fe",
   "metadata": {},
   "source": [
    "Then verify that analysis was run as expected and yielded analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c84f66af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete: True\n",
      "\n",
      "Sample: sample_001\n",
      "Source: File 0, paragraphs 268-273\n",
      "Analyses available: ['efficiency_auditor', 'information_architect', 'lexicologist', 'rhetorician', 'syntactician']\n",
      "\n",
      "--- Rhetorician Output (first 500 chars) ---\n",
      "### Rhetorical Strategy and Stance Analysis\n",
      "\n",
      "#### 1. WRITER'S POSITION\n",
      "\n",
      "**Persona:**\n",
      "- The writer adopts an authoritative and scholarly persona. The use of specialized terminology and references to authorities like Bosanquet, Erdmann, Riemann, and Helmholtz indicates a high degree of expertise.\n",
      "- The text is formal and academic, suggesting a detached and analytical stance rather than a conversational one.\n",
      "\n",
      "**Relationship to Subject Matter:**\n",
      "- The writer positions themselves as an expert in the \n"
     ]
    }
   ],
   "source": [
    "sample_id = 'sample_001'\n",
    "is_complete = store.is_complete(sample_id, ANALYSTS)\n",
    "print(f\"Analysis complete: {is_complete}\")\n",
    "\n",
    "# Retrieve sample and all analyses (both are now dicts)\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "print(f\"\\nSample: {sample['sample_id']}\")\n",
    "print(f\"Source: File {sample['file_index']}, paragraphs {sample['paragraph_start']}-{sample['paragraph_end']}\")\n",
    "print(f\"Analyses available: {list(analyses.keys())}\")\n",
    "\n",
    "# Examine one analysis\n",
    "print(f\"\\n--- Rhetorician Output (first 500 chars) ---\")\n",
    "print(analyses.get(\"rhetorician\", \"Not found\")[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b793ed2",
   "metadata": {},
   "source": [
    "## Step 2a: Pattern Recognition (Cross-Perspective Integration) per Text Sample\n",
    "\n",
    "Synthesize all analyst perspectives to identify interactions, tensions, and load-bearing features. This is a per-text-cross-analyst transformation where a unit of writing patterns for each text sample is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2615d0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_001', 'sample_002']\n"
     ]
    }
   ],
   "source": [
    "samples_to_analyze = store.list_samples()\n",
    "print(samples_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4dfabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pattern_prompt_from_(text: str, analyses: dict):\n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "    analyst_info = {}\n",
    "    for analyst_name in ANALYSTS:\n",
    "        config_class = ANALYST_CONFIGS[analyst_name]\n",
    "        analyst_info[analyst_name] = {\n",
    "            'analysis': analyses[analyst_name],\n",
    "            'analyst_descr_short': config_class.description()\n",
    "        }\n",
    "\n",
    "    pattern_config = CrossPerspectiveIntegratorConfig(\n",
    "        original_text=sample['text'],\n",
    "        analysts=analyst_info\n",
    "    )\n",
    "    return prompt_maker.render(pattern_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "930626af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pattern recognizer for sample_001... ✓ (8104 chars)\n",
      "Running pattern recognizer for sample_002... ✓ (13416 chars)\n"
     ]
    }
   ],
   "source": [
    "for sample_id in samples_to_analyze:\n",
    "    print(f\"Running pattern recognizer for {sample_id}...\", end=\" \")\n",
    "    \n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "    pattern_prompt = build_pattern_prompt_from_(text=sample['text'], analyses=analyses)\n",
    "\n",
    "    pattern_response = llm.complete(pattern_prompt)\n",
    "    \n",
    "    # Store pattern recognition result in result store\n",
    "    store.save_analysis(\n",
    "        sample_id, \n",
    "        CrossPerspectiveIntegratorConfig.analyst_name(), \n",
    "        pattern_response.content, \n",
    "        pattern_response.model\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ ({len(pattern_response.content)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd294b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cross_perspective_integrator', 'efficiency_auditor', 'information_architect', 'lexicologist', 'rhetorician', 'syntactician'])\n",
      "\n",
      "--- Pattern Analyst Output (first 2000 chars) ---\n",
      "## INTEGRATED ANALYSIS\n",
      "\n",
      "### I. PRIMARY INTERACTIONS\n",
      "\n",
      "#### 1. Syntactic Structure and Lexical Precision\n",
      "\n",
      "**Observation:**\n",
      "The syntactician notes the text’s use of complex and compound-complex sentences, which create space for lexical precision.\n",
      "\n",
      "**Quote:**\n",
      "- **Rhetorician**: \"The writer adopts an authoritative and scholarly persona.\"\n",
      "- **Syntactician**: \"The text employs a mix of simple, compound, complex, and compound-complex sentences, indicating a sophisticated level of syntactic complexity.\"\n",
      "\n",
      "**Mechanism:**\n",
      "The complex syntactic structures allow for the inclusion of detailed and precise lexical choices. The use of subordinate clauses and participial phrases provides a framework for introducing technical terms and detailed explanations.\n",
      "\n",
      "**Assessment:**\n",
      "This interaction is central to the text's effect. The syntactic complexity supports the lexical precision, which is crucial for conveying the nuanced arguments presented.\n",
      "\n",
      "#### 2. Rhythm and Logical Flow\n",
      "\n",
      "**Observation:**\n",
      "The information architect notes that the text builds linearly within paragraphs, while the syntactician highlights the use of long, intricate sentences.\n",
      "\n",
      "**Quote:**\n",
      "- **Information Architect**: \"The paragraph builds linearly, with each sentence adding a layer of explanation or evidence to support the initial claim.\"\n",
      "- **Syntactician**: \"The sentences are generally long and intricate, which can be challenging for readability but also adds depth to the content.\"\n",
      "\n",
      "**Mechanism:**\n",
      "The linear build within paragraphs, combined with the intricate sentence structures, creates a rhythm that supports the logical flow of arguments. The complexity of the sentences matches the depth of the logical reasoning.\n",
      "\n",
      "**Assessment:**\n",
      "This interaction is central. The rhythm created by the sentence structures supports the logical flow, making the text intellectually dense but coherent.\n",
      "\n",
      "#### 3. Economy in Syntax and Lexical Density\n",
      "\n",
      "**Observation:**\n",
      "The efficiency auditor notes that some phrases can be cut without loss of\n"
     ]
    }
   ],
   "source": [
    "sample_id = 'sample_001'\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "print(analyses.keys())\n",
    "\n",
    "print(f\"\\n--- Pattern Analyst Output (first 2000 chars) ---\")\n",
    "print(analyses.get(\"cross_perspective_integrator\", \"Not found\")[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb242db",
   "metadata": {},
   "source": [
    "## Stage 2b: Cross-Text Synthesis of Each Integrated Analysis\n",
    "\n",
    "Now we synthesize patterns across multiple text analyses. This stage takes all the pattern recognition outputs (cross-perspective integrations from Stage 1) and identifies:\n",
    "- Recurring patterns across texts\n",
    "- Context-dependent variations\n",
    "- Hierarchies of importance\n",
    "- Generalizable principles\n",
    "\n",
    "This produces a single synthesis document that IS stored in the ResultStore with auto-generated ID and full provenance tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb5ea577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 pattern recognition analyses\n",
      "Sample IDs: ['sample_001', 'sample_002']\n",
      "\n",
      "Running cross-text synthesis... ✓ (11956 chars)\n",
      "Saved as: cross_text_synthesis_001\n",
      "\n",
      "--- Cross-Text Synthesis (first 1000 chars) ---\n",
      "## SYNTHESIZED ANALYSIS\n",
      "\n",
      "### I. Stable Core Patterns\n",
      "\n",
      "#### 1. Syntactic Complexity Supporting Lexical Precision\n",
      "\n",
      "- **Frequency:** 2/2\n",
      "- **Pattern:** The use of complex and compound-complex sentences to create space for lexical precision and detailed explanations.\n",
      "- **Quotes:**\n",
      "  - Sample 001: \"The text employs a mix of simple, compound, complex, and compound-complex sentences, indicating a sophisticated level of syntactic complexity.\"\n",
      "  - Sample 002: \"The text demonstrates a mix of simple and complex sentence structures, with a notable variation in sentence length.\"\n",
      "- **Centrality:** Foundational\n",
      "\n",
      "#### 2. Rhythm Supporting Logical Flow\n",
      "\n",
      "- **Frequency:** 2/2\n",
      "- **Pattern:** Variation in sentence length and structure creates a rhythm that supports the logical progression of arguments and explanations.\n",
      "- **Quotes:**\n",
      "  - Sample 001: \"The paragraph builds linearly, with each sentence adding a layer of explanation or evidence to support the initial claim.\"\n",
      "  - Sample 002: \"The text exhibits a\n"
     ]
    }
   ],
   "source": [
    "from belletrist.models import CrossTextSynthesizerConfig\n",
    "\n",
    "# Get all samples that have pattern recognition results\n",
    "all_samples = store.list_samples()\n",
    "pattern_analyst = CrossPerspectiveIntegratorConfig.analyst_name()\n",
    "\n",
    "# Retrieve all pattern recognition analyses\n",
    "integrated_analyses = {}\n",
    "for sample_id in all_samples:\n",
    "    pattern_analysis = store.get_analysis(sample_id, pattern_analyst)\n",
    "    if pattern_analysis:\n",
    "        integrated_analyses[sample_id] = pattern_analysis\n",
    "    else:\n",
    "        print(f\"⚠ Sample {sample_id} missing pattern recognition analysis\")\n",
    "\n",
    "print(f\"Found {len(integrated_analyses)} pattern recognition analyses\")\n",
    "print(f\"Sample IDs: {list(integrated_analyses.keys())}\")\n",
    "\n",
    "# Only proceed if we have at least 2 (required for cross-text synthesis)\n",
    "if len(integrated_analyses) >= 2:\n",
    "    # Build cross-text synthesis config and prompt\n",
    "    cross_text_config = CrossTextSynthesizerConfig(\n",
    "        integrated_analyses=integrated_analyses\n",
    "    )\n",
    "    cross_text_prompt = prompt_maker.render(cross_text_config)\n",
    "    \n",
    "    # Run cross-text synthesis\n",
    "    print(\"\\nRunning cross-text synthesis...\", end=\" \")\n",
    "    cross_text_response = llm.complete(cross_text_prompt)\n",
    "    print(f\"✓ ({len(cross_text_response.content)} chars)\")\n",
    "    \n",
    "    # Save to ResultStore with auto-generated ID and full provenance\n",
    "    sample_contributions = [(sid, pattern_analyst) for sid in integrated_analyses.keys()]\n",
    "    cross_text_id = store.save_synthesis(\n",
    "        synthesis_type=cross_text_config.synthesis_type(),\n",
    "        output=cross_text_response.content,\n",
    "        model=cross_text_response.model,\n",
    "        sample_contributions=sample_contributions,\n",
    "        config=cross_text_config\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved as: {cross_text_id}\")\n",
    "    \n",
    "    # Display first part\n",
    "    print(\"\\n--- Cross-Text Synthesis (first 1000 chars) ---\")\n",
    "    print(cross_text_response.content[:1000])\n",
    "else:\n",
    "    print(f\"\\n⚠ Need at least 2 pattern recognition analyses for cross-text synthesis. Got {len(integrated_analyses)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca17cf",
   "metadata": {},
   "source": [
    "## Stage 3: Synthesizer of Principles\n",
    "\n",
    "The final stage converts the descriptive cross-text synthesis into actionable prescriptive writing principles. This generates a style guide that can be used to instruct an LLM to write in a similar style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21fa11a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cross-text synthesis: cross_text_synthesis_001\n",
      "Running principles synthesizer... ✓ (16853 chars)\n",
      "Saved as: principles_guide_001\n",
      "\n",
      "--- Principles Guide (first 1000 chars) ---\n",
      "# A GUIDE TO CLEAR AND EFFECTIVE PROSE\n",
      "## Principles Extracted from Pattern Analysis\n",
      "\n",
      "### PART I: FOUNDATIONS\n",
      "**Core Principles**\n",
      "\n",
      "#### 1. Use Syntactic Complexity to Support Lexical Precision\n",
      "**Mechanism:** Complex sentence structures create space for detailed and precise explanations. This allows for nuanced arguments and accurate technical information without overwhelming the reader.\n",
      "**Grounded in Evidence:**\n",
      "- Sample 001: \"The text employs a mix of simple, compound, complex, and compound-complex sentences.\"\n",
      "- Sample 002: \"The text demonstrates a mix of simple and complex sentence structures.\"\n",
      "\n",
      "**Dependencies:**\n",
      "- Enables detailed explanations and logical flow.\n",
      "- Without this, the text would lack depth and coherence.\n",
      "\n",
      "#### 2. Vary Sentence Length and Structure to Create Rhythm\n",
      "**Mechanism:** Variation in sentence length and structure creates a rhythm that supports the logical progression of arguments and explanations. This keeps the reader engaged and ensures coherence.\n",
      "**Grounded i\n"
     ]
    }
   ],
   "source": [
    "from belletrist.models import SynthesizerOfPrinciplesConfig\n",
    "\n",
    "# Get the latest cross-text synthesis\n",
    "cross_text_syntheses = store.list_syntheses('cross_text_synthesis')\n",
    "if cross_text_syntheses:\n",
    "    # Use the most recent cross-text synthesis\n",
    "    latest_cross_text_id = cross_text_syntheses[-1]\n",
    "    cross_text_synthesis = store.get_synthesis(latest_cross_text_id)\n",
    "    \n",
    "    print(f\"Using cross-text synthesis: {latest_cross_text_id}\")\n",
    "    \n",
    "    # Build principles guide config and prompt\n",
    "    principles_config = SynthesizerOfPrinciplesConfig(\n",
    "        synthesis_document=cross_text_synthesis['output']\n",
    "    )\n",
    "    principles_prompt = prompt_maker.render(principles_config)\n",
    "    \n",
    "    # Run principles synthesis\n",
    "    print(\"Running principles synthesizer...\", end=\" \")\n",
    "    principles_response = llm.complete(principles_prompt)\n",
    "    print(f\"✓ ({len(principles_response.content)} chars)\")\n",
    "    \n",
    "    # Save to ResultStore with parent linkage (inherits provenance)\n",
    "    principles_id = store.save_synthesis(\n",
    "        synthesis_type=principles_config.synthesis_type(),\n",
    "        output=principles_response.content,\n",
    "        model=principles_response.model,\n",
    "        sample_contributions=[],  # Inherited from parent\n",
    "        config=principles_config,\n",
    "        parent_synthesis_id=latest_cross_text_id\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved as: {principles_id}\")\n",
    "    \n",
    "    # Display first part\n",
    "    print(\"\\n--- Principles Guide (first 1000 chars) ---\")\n",
    "    print(principles_response.content[:1000])\n",
    "else:\n",
    "    print(\"⚠ No cross-text synthesis found. Run Stage 2 first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ef528c",
   "metadata": {},
   "source": [
    "## Querying Synthesis Metadata\n",
    "\n",
    "The ResultStore tracks full provenance for all syntheses. Query metadata to understand what samples, analysts, and models contributed to each synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc401734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Syntheses\n",
      "==================================================\n",
      "\n",
      "cross_text_synthesis: 1 found\n",
      "  - cross_text_synthesis_001\n",
      "\n",
      "principles_guide: 1 found\n",
      "  - principles_guide_001\n",
      "\n",
      "\n",
      "Detailed Metadata Example\n",
      "==================================================\n",
      "\n",
      "Synthesis ID: principles_guide_001\n",
      "Type: principles_guide\n",
      "Model: mistral-large-2411\n",
      "Created: 2025-11-24T15:40:32.461919\n",
      "Parent: cross_text_synthesis_001\n",
      "\n",
      "Metadata:\n",
      "  Samples: 0\n",
      "  Sample IDs: []\n",
      "  Model homogeneous: True\n",
      "  Models used: ['mistral-large-2411']\n",
      "\n",
      "\n",
      "Full Provenance Tree\n",
      "==================================================\n",
      "\n",
      "Principles Guide: principles_guide_001\n",
      "  Created: 2025-11-24T15:40:32.461919\n",
      "  Model: mistral-large-2411\n",
      "\n",
      "  Parent (Cross-Text): cross_text_synthesis_001\n",
      "    Sample contributions: 2\n",
      "      - sample_001 / cross_perspective_integrator\n",
      "      - sample_002 / cross_perspective_integrator\n"
     ]
    }
   ],
   "source": [
    "# List all syntheses\n",
    "print(\"All Syntheses\")\n",
    "print(\"=\" * 50)\n",
    "for synth_type in ['cross_text_synthesis', 'principles_guide']:\n",
    "    syntheses = store.list_syntheses(synth_type)\n",
    "    print(f\"\\n{synth_type}: {len(syntheses)} found\")\n",
    "    for synth_id in syntheses:\n",
    "        print(f\"  - {synth_id}\")\n",
    "\n",
    "# Get detailed metadata for a synthesis\n",
    "if store.list_syntheses():\n",
    "    print(\"\\n\\nDetailed Metadata Example\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get first principles guide (or first cross-text if none)\n",
    "    principles = store.list_syntheses('principles_guide')\n",
    "    if principles:\n",
    "        synth_id = principles[0]\n",
    "    else:\n",
    "        synth_id = store.list_syntheses()[0]\n",
    "    \n",
    "    synth_with_meta = store.get_synthesis_with_metadata(synth_id)\n",
    "    \n",
    "    print(f\"\\nSynthesis ID: {synth_with_meta['synthesis_id']}\")\n",
    "    print(f\"Type: {synth_with_meta['type']}\")\n",
    "    print(f\"Model: {synth_with_meta['model']}\")\n",
    "    print(f\"Created: {synth_with_meta['created_at']}\")\n",
    "    print(f\"Parent: {synth_with_meta['parent_id']}\")\n",
    "    \n",
    "    if synth_with_meta.get('metadata'):\n",
    "        meta = synth_with_meta['metadata']\n",
    "        print(f\"\\nMetadata:\")\n",
    "        print(f\"  Samples: {meta['num_samples']}\")\n",
    "        print(f\"  Sample IDs: {meta['sample_ids']}\")\n",
    "        print(f\"  Model homogeneous: {meta['is_homogeneous_model']}\")\n",
    "        print(f\"  Models used: {meta['models_used']}\")\n",
    "\n",
    "# Get full provenance tree\n",
    "if store.list_syntheses('principles_guide'):\n",
    "    print(\"\\n\\nFull Provenance Tree\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    principles_id = store.list_syntheses('principles_guide')[0]\n",
    "    provenance = store.get_synthesis_provenance(principles_id)\n",
    "    \n",
    "    print(f\"\\nPrinciples Guide: {provenance['synthesis']['synthesis_id']}\")\n",
    "    print(f\"  Created: {provenance['synthesis']['created_at']}\")\n",
    "    print(f\"  Model: {provenance['synthesis']['model']}\")\n",
    "    \n",
    "    if provenance['parent']:\n",
    "        parent = provenance['parent']\n",
    "        print(f\"\\n  Parent (Cross-Text): {parent['synthesis']['synthesis_id']}\")\n",
    "        print(f\"    Sample contributions: {len(parent['sample_contributions'])}\")\n",
    "        for sample_id, analyst in parent['sample_contributions'][:3]:\n",
    "            print(f\"      - {sample_id} / {analyst}\")\n",
    "        if len(parent['sample_contributions']) > 3:\n",
    "            print(f\"      ... and {len(parent['sample_contributions']) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e5741",
   "metadata": {},
   "source": [
    "## Exporting Syntheses to Filesystem\n",
    "\n",
    "Export final syntheses to text files with YAML metadata headers for consumption by other tools or LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9f103dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: outputs/cross_text_synthesis_001.txt\n",
      "Exported: outputs/principles_guide_001.txt\n",
      "Exported for style evaluation: outputs/derived_style_instructions.txt\n",
      "\n",
      "All syntheses exported to /Users/andersohrn/PycharmProjects/russell_writes/outputs\n"
     ]
    }
   ],
   "source": [
    "# Create outputs directory\n",
    "outputs_dir = Path(\"outputs\")\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export cross-text synthesis\n",
    "cross_text_syntheses = store.list_syntheses('cross_text_synthesis')\n",
    "if cross_text_syntheses:\n",
    "    for synth_id in cross_text_syntheses:\n",
    "        output_path = outputs_dir / f\"{synth_id}.txt\"\n",
    "        store.export_synthesis(synth_id, output_path, metadata_format='yaml')\n",
    "        print(f\"Exported: {output_path}\")\n",
    "\n",
    "# Export principles guide\n",
    "principles_guides = store.list_syntheses('principles_guide')\n",
    "if principles_guides:\n",
    "    for synth_id in principles_guides:\n",
    "        output_path = outputs_dir / f\"{synth_id}.txt\"\n",
    "        store.export_synthesis(synth_id, output_path, metadata_format='yaml')\n",
    "        print(f\"Exported: {output_path}\")\n",
    "        \n",
    "        # Also create a special \"derived_style_instructions.txt\" for style_evaluation.ipynb\n",
    "        if synth_id == principles_guides[-1]:  # Use latest\n",
    "            instructions_path = outputs_dir / \"derived_style_instructions.txt\"\n",
    "            store.export_synthesis(synth_id, instructions_path, metadata_format='yaml')\n",
    "            print(f\"Exported for style evaluation: {instructions_path}\")\n",
    "\n",
    "print(f\"\\nAll syntheses exported to {outputs_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99a4c3",
   "metadata": {},
   "source": [
    "## Utilities: Working with Stored Samples\n",
    "\n",
    "Helper functions for browsing and managing stored results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af4ce0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all samples in the database\n",
    "all_samples = store.list_samples()\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "print(f\"Sample IDs: {all_samples}\")\n",
    "\n",
    "# Check completion status for each\n",
    "print(\"\\nCompletion status:\")\n",
    "for sid in all_samples:\n",
    "    complete = store.is_complete(sid, ANALYSTS)\n",
    "    status = \"✓\" if complete else \"✗\"\n",
    "    print(f\"  {status} {sid}\")\n",
    "\n",
    "# Close database connection when done\n",
    "# store.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
