{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e30cf20",
   "metadata": {},
   "source": [
    "# Multi-Analyst Text Analysis Pipeline\n",
    "\n",
    "In this notebook we demonstrate the full pipeline for analyzing text through multiple specialist lenses (rhetorician, syntactician, lexicologist, etc.) and synthesizing their observations into \n",
    "* a unified writing style analysis from the text samples,\n",
    "* an instruction of how to write in the analyzed style.\n",
    "\n",
    "The workflow is *agentic* in that it involves several distinct agents built on Large Language Models (LLMs), however, the order and relation between each agent is set, not dynamically derived.\n",
    "\n",
    "The analysis and its details are described in this blog post: **INSERT LINK**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6001aa",
   "metadata": {},
   "source": [
    "## Installations and Preparations\n",
    "External libraries are installed and tested to be in working order.\n",
    "\n",
    "Key dependencies:\n",
    "* LiteLLM (model router such that different LLM APIs can be readily employed)\n",
    "* Jinja (create prompts with variables and conditional logic)\n",
    "* Pydantic (create prompts from variables with validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de28f946",
   "metadata": {},
   "source": [
    "**Install requirements.** Only needed if running in fresh kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6bf55b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:22.033495Z",
     "start_time": "2025-11-19T18:02:20.240014Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527ee15",
   "metadata": {},
   "source": [
    "Check that LiteLLM was installed correctly. List the providers available via LiteLLM router."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc6c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0952017d",
   "metadata": {},
   "source": [
    "## Initialize Base Objects\n",
    "The base objects part of the current project library (`belletrist`) are initialized. They are:\n",
    "* `LLM`: the LLM object.\n",
    "* `LLMConfig`: the configuration of the LLM object, such as what model to use.\n",
    "* `PromptMaker`: generates prompts from templates and variables\n",
    "* `DataSampler`: retrieves and samples text at a source directory\n",
    "* `ResultStore`: simple database object to save intermediate and final outputs\n",
    "\n",
    "The LLM to use is set by the `model_string`, which is constructed as `<provider>/<model>`, the providers defined by the `litellm` package, see in particular `litellm.LITELLM_CHAT_PROVIDERS`. The API key to the provider should be stored in an environment variable with name defined in `model_provider_api_key_env_var`. You need to create that yourself for the provider of interest. \n",
    "\n",
    "Do **not** store the API key as a string variable directly in the notebook, you're at risk of exposing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0612a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_string = 'mistral/mistral-large-2411'\n",
    "model_provider_api_key_env_var = 'MISTRAL_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01542e0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T18:02:29.927627Z",
     "start_time": "2025-11-19T18:02:29.858316Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from belletrist import LLM, LLMConfig, PromptMaker, DataSampler, ResultStore\n",
    "\n",
    "llm = LLM(LLMConfig(\n",
    "    model=model_string,\n",
    "    api_key=os.environ.get(model_provider_api_key_env_var)\n",
    "))\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler(\n",
    "    data_path=(Path(os.getcwd()) / \"data\" / \"russell\").resolve()\n",
    ")\n",
    "store = ResultStore(Path(f\"{os.getcwd()}/belletrist_storage.db\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce383212",
   "metadata": {},
   "source": [
    "In case a clean run is wanted, the old contents of the database are discarded with a result store reset. Do **not** run this reset if content should be preserved from previous runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5663b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "store.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2485939a",
   "metadata": {},
   "source": [
    "## Generate and Store Text Samples to be Analyzed\n",
    "\n",
    "The `DataSampler` retrieves paragraphs from the corpus of text. The retrieval can be a random sample of consecutive paragraphs (via the method `sample_segment`) or a specific file and paragraph range (via the method `get_paragraph_chunk`).\n",
    "\n",
    "As illustration of the process, a random four-paragraph long segment is sampled below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9fbd16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text source: /Users/andersohrn/PycharmProjects/russell_writes/data/russell/education_and_the_good_life.txt\n",
      "Paragraph range: 207 - 211\n",
      "\n",
      "CHAPTER VIII\n",
      "\n",
      "TRUTHFULNESS\n",
      "\n",
      "To produce the habit of truthfulness should be one of the major aims\n",
      "of moral education. I do not mean truthfulness in speech only, but\n",
      "also in thought; indeed, of the two, the latter seems to me the more\n",
      "important. I prefer a person who lies with full consciousness of what\n",
      "he is doing to a person who first subconsciously deceives himself\n",
      "and then imagines that he is being virtuous and truthful. Indeed, no\n",
      "man who thinks truthfully can believe that it is _always_ wrong to\n",
      "speak untruthfully. Those who hold that a lie is always wrong have to\n",
      "supplement this view by a great deal of casuistry and considerable\n",
      "practice in misleading ambiguities, by means of which they deceive\n",
      "without admitting to themselves that they are lying. Nevertheless, I\n",
      "hold that the occasions when lying is justifiable are few--much fewer\n",
      "than would be inferred from the practice of high-minded men. And almost\n",
      "all the occasions which justify lying are occasions where power is\n",
      "being used tyrannically, or where people are engaged in some harmful\n",
      "activity such as war; therefore in a good social system they would be\n",
      "even rarer than they are now.\n",
      "\n",
      "Untruthfulness, as a practice, is almost always a product of fear. The\n",
      "child brought up without fear will be truthful, not in virtue of a\n",
      "moral effort, but because it will never occur to him to be otherwise.\n",
      "The child who has been treated wisely and kindly has a frank look in\n",
      "the eyes, and a fearless demeanour even with strangers; whereas the\n",
      "child that has been subject to nagging or severity is in perpetual\n",
      "terror of incurring reproof, and terrified of having transgressed some\n",
      "rule whenever he has behaved in a natural manner. It does not at first\n",
      "occur to a young child that it is possible to lie. The possibility of\n",
      "lying is a discovery, due to observation of grown-ups quickened by\n",
      "terror. The child discovers that grown-ups lie to him, and that it is\n",
      "dangerous to tell them the truth; under these circumstances he takes to\n",
      "lying. Avoid these incentives, and he will not think of lying.\n"
     ]
    }
   ],
   "source": [
    "text_sample = sampler.sample_segment(p_length=4)\n",
    "print(f'Text source: {text_sample.file_path}')\n",
    "print(f'Paragraph range: {text_sample.paragraph_start} - {text_sample.paragraph_end}')\n",
    "print(f'\\n{text_sample.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84180101",
   "metadata": {},
   "source": [
    "A number of text samples are retrieved (**set `n_sample` to the desired number**) and each sample comprises a set number of paragraphs (**set `m_paragraphs_per_sample` to the desired number**). These text samples are stored in the project result stored with keys like `sample_001`, `sample_002` and so on; these keys are henceforth referring to specific text samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "882fb8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 5\n",
    "m_paragraphs_per_sample = 5\n",
    "\n",
    "for _ in range(n_sample):\n",
    "    sample_id = f'sample_{len(store.list_samples()) + 1:03d}'\n",
    "    segment = sampler.sample_segment(p_length=m_paragraphs_per_sample)\n",
    "    store.save_segment(sample_id, segment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7df88824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample keys:\n",
      "============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sample_001', 'sample_002', 'sample_003', 'sample_004', 'sample_005']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Sample keys:\\n============')\n",
    "store.list_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d571313",
   "metadata": {},
   "source": [
    "## Step 1: Construct the Analyst Agents and Analyze Text Samples\n",
    "\n",
    "Send the text samples through each specialist analyst. Each produces an independent analysis from their domain expertise.\n",
    "\n",
    "**Prompt structure for each analyst is:**\n",
    "1. Preamble instruction of task ahead \n",
    "2. Analyst-specific instruction template\n",
    "3. Text to analyze\n",
    "\n",
    "Note that the execution of this can take time since it involves invoking LLMs, once per analyst and text sample. These are however independent analyses, and can therefore *in principle* be run in parallel, though the implementation below does not utilize that fact.\n",
    "\n",
    "Note that the agents are distinguished by their prompts, which are obtained via prompt models defined within the `bellatrist` project library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c83180eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist.models import (\n",
    "    PreambleInstructionConfig,\n",
    "    PreambleTextConfig,\n",
    "    RhetoricianConfig,\n",
    "    SyntacticianConfig,\n",
    "    LexicologistConfig,\n",
    "    InformationArchitectConfig,\n",
    "    EfficiencyAuditorConfig,\n",
    "    CrossPerspectiveIntegratorConfig,\n",
    ")\n",
    "ANALYSTS = [\"rhetorician\", \"syntactician\", \"lexicologist\", \"information_architect\", \"efficiency_auditor\"]\n",
    "ANALYST_CONFIGS = {\n",
    "    \"rhetorician\": RhetoricianConfig,\n",
    "    \"syntactician\": SyntacticianConfig,\n",
    "    \"lexicologist\": LexicologistConfig,\n",
    "    \"information_architect\": InformationArchitectConfig,\n",
    "    \"efficiency_auditor\": EfficiencyAuditorConfig,\n",
    "}\n",
    "\n",
    "def build_analyst_prompt(preamble_instruction: str, analyst_prompt: str, preamble_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to construct the full prompt for an analyst.\n",
    "    \n",
    "    \"\"\"\n",
    "    return f\"{preamble_instruction}\\n\\n{analyst_prompt}\\n\\n{preamble_text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "078b581b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 5 samples with 5 analysts each\n",
      "\n",
      "Sample: sample_001\n",
      "  Running rhetorician... ✓ (5790 chars)\n",
      "  Running syntactician... ✓ (12203 chars)\n",
      "  Running lexicologist... ✓ (5372 chars)\n",
      "  Running information_architect... ✓ (5967 chars)\n",
      "  Running efficiency_auditor... ✓ (5428 chars)\n",
      "\n",
      "Sample: sample_002\n",
      "  Running rhetorician... ✓ (5292 chars)\n",
      "  Running syntactician... ✓ (5439 chars)\n",
      "  Running lexicologist... ✓ (6028 chars)\n",
      "  Running information_architect... ✓ (6999 chars)\n",
      "  Running efficiency_auditor... ✓ (4836 chars)\n",
      "\n",
      "Sample: sample_003\n",
      "  Running rhetorician... ✓ (5983 chars)\n",
      "  Running syntactician... ✓ (10971 chars)\n",
      "  Running lexicologist... ✓ (5480 chars)\n",
      "  Running information_architect... ✓ (6501 chars)\n",
      "  Running efficiency_auditor... ✓ (6605 chars)\n",
      "\n",
      "Sample: sample_004\n",
      "  Running rhetorician... ✓ (3853 chars)\n",
      "  Running syntactician... ✓ (8259 chars)\n",
      "  Running lexicologist... ✓ (6615 chars)\n",
      "  Running information_architect... ✓ (8012 chars)\n",
      "  Running efficiency_auditor... ✓ (5426 chars)\n",
      "\n",
      "Sample: sample_005\n",
      "  Running rhetorician... ✓ (6789 chars)\n",
      "  Running syntactician... ✓ (6131 chars)\n",
      "  Running lexicologist... ✓ (4723 chars)\n",
      "  Running information_architect... ✓ (7701 chars)\n",
      "  Running efficiency_auditor... ✓ (5811 chars)\n",
      "\n",
      "All analyses complete for 5 samples\n"
     ]
    }
   ],
   "source": [
    "# Get all samples from the store\n",
    "all_samples = store.list_samples()\n",
    "print(f\"Processing {len(all_samples)} samples with {len(ANALYSTS)} analysts each\\n\")\n",
    "\n",
    "# Outer loop: iterate over each text sample\n",
    "for sample_id in all_samples:\n",
    "    print(f\"Sample: {sample_id}\")\n",
    "    \n",
    "    # Get the sample text\n",
    "    sample = store.get_sample(sample_id)\n",
    "    text = sample['text']\n",
    "    \n",
    "    # Build shared prompt components (reused across all analysts for this sample)\n",
    "    preamble_instruction = prompt_maker.render(PreambleInstructionConfig())\n",
    "    preamble_text = prompt_maker.render(PreambleTextConfig(text_to_analyze=text))\n",
    "    \n",
    "    # Inner loop: run each analyst on this sample\n",
    "    for analyst_name in ANALYSTS:\n",
    "        print(f\"  Running {analyst_name}...\", end=\" \")\n",
    "        \n",
    "        # Get analyst-specific prompt using the config class\n",
    "        analyst_config = ANALYST_CONFIGS[analyst_name]()\n",
    "        analyst_prompt = prompt_maker.render(analyst_config)\n",
    "        full_prompt = build_analyst_prompt(preamble_instruction, analyst_prompt, preamble_text)\n",
    "        \n",
    "        # Run analysis and save result\n",
    "        response = llm.complete(full_prompt)\n",
    "        store.save_analysis(sample_id, analyst_name, response.content, response.model)\n",
    "        \n",
    "        print(f\"✓ ({len(response.content)} chars)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"All analyses complete for {len(all_samples)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8672e",
   "metadata": {},
   "source": [
    "Verification that analysis was run as expected and yielded analysis results. Excerpt of one specific analysis retrieved from project database and printed for illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "219cbc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis complete: True\n",
      "\n",
      "Sample: sample_001\n",
      "Source: File 1, paragraphs 372-377\n",
      "Analyses available: ['efficiency_auditor', 'information_architect', 'lexicologist', 'rhetorician', 'syntactician']\n",
      "\n",
      "--- Rhetorician Output (first 500 chars) ---\n",
      "### RHETORICAL STRATEGY AND STANCE ANALYSIS\n",
      "\n",
      "#### 1. WRITER'S POSITION\n",
      "\n",
      "**Persona**: The writer emerges as authoritative but also somewhat conversational. The use of phrases like \"It is of course true\" and \"I do not believe\" suggests a confident yet approachable voice. The writer is not overly formal but maintains a clear authority on the subject.\n",
      "\n",
      "**Relationship to Subject Matter**: The writer positions themselves as an expert, providing insights and recommendations on the educational system an\n"
     ]
    }
   ],
   "source": [
    "sample_id = 'sample_001'\n",
    "is_complete = store.is_complete(sample_id, ANALYSTS)\n",
    "print(f\"Analysis complete: {is_complete}\")\n",
    "\n",
    "# Retrieve sample and all analyses (both are now dicts)\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "print(f\"\\nSample: {sample['sample_id']}\")\n",
    "print(f\"Source: File {sample['file_index']}, paragraphs {sample['paragraph_start']}-{sample['paragraph_end']}\")\n",
    "print(f\"Analyses available: {list(analyses.keys())}\")\n",
    "\n",
    "# Examine one analysis\n",
    "print(f\"\\n--- Rhetorician Output (first 500 chars) ---\")\n",
    "print(analyses.get(\"rhetorician\", \"Not found\")[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22708d46",
   "metadata": {},
   "source": [
    "## Step 2a: Pattern Recognition (Cross-Perspective Integration) per Text Sample\n",
    "\n",
    "Synthesize all analyst perspectives to identify interactions, tensions, and load-bearing features. This is a per-text-cross-analyst transformation. This looks to integrate multiple perspectives on each text sample and indirectly distill the information content in the assessments of the text samples. \n",
    "\n",
    "If only a subset of samples are to be analyzed, filter or slice the list `samples_to_analyze`, which is a list of sample IDs, as created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a022dc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_001', 'sample_002', 'sample_003', 'sample_004', 'sample_005']\n"
     ]
    }
   ],
   "source": [
    "samples_to_analyze = store.list_samples()\n",
    "print(samples_to_analyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "579dedd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pattern_prompt_from_(text: str, analyses: dict):\n",
    "    \"\"\"Convenience function to create the prompt, since the prompt depends on the kinds of analysts to integrate.\n",
    "    \n",
    "    \"\"\"\n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "\n",
    "    analyst_info = {}\n",
    "    for analyst_name in ANALYSTS:\n",
    "        config_class = ANALYST_CONFIGS[analyst_name]\n",
    "        analyst_info[analyst_name] = {\n",
    "            'analysis': analyses[analyst_name],\n",
    "            'analyst_descr_short': config_class.description()\n",
    "        }\n",
    "\n",
    "    pattern_config = CrossPerspectiveIntegratorConfig(\n",
    "        original_text=sample['text'],\n",
    "        analysts=analyst_info\n",
    "    )\n",
    "    return prompt_maker.render(pattern_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0deb2c",
   "metadata": {},
   "source": [
    "Note that this loop can take time to execute, since LLMs are called. Each analysis is independent and can therefore in principle be parallelized, though the implementation below does not do that.\n",
    "\n",
    "Note also that the cross-perspective per-text result are stored in the result store, keyed on the sample ID and the analyst kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db326784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pattern recognizer for sample_001... ✓ (9644 chars)\n",
      "Running pattern recognizer for sample_002... ✓ (9039 chars)\n",
      "Running pattern recognizer for sample_003... ✓ (6755 chars)\n",
      "Running pattern recognizer for sample_004... ✓ (11118 chars)\n",
      "Running pattern recognizer for sample_005... ✓ (20398 chars)\n"
     ]
    }
   ],
   "source": [
    "for sample_id in samples_to_analyze:\n",
    "    print(f\"Running Cross-Perspective Integrator agent for {sample_id}...\", end=\" \")\n",
    "    \n",
    "    sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "    pattern_prompt = build_pattern_prompt_from_(text=sample['text'], analyses=analyses)\n",
    "\n",
    "    pattern_response = llm.complete(pattern_prompt)\n",
    "    \n",
    "    # Store pattern recognition result in result store\n",
    "    store.save_analysis(\n",
    "        sample_id, \n",
    "        CrossPerspectiveIntegratorConfig.analyst_name(), \n",
    "        pattern_response.content, \n",
    "        pattern_response.model\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ ({len(pattern_response.content)} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a556aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['cross_perspective_integrator', 'efficiency_auditor', 'information_architect', 'lexicologist', 'rhetorician', 'syntactician'])\n",
      "\n",
      "--- Pattern Analyst Output (first 2000 chars) ---\n",
      "## I. Extracted Techniques\n",
      "\n",
      "### 1. **Precise Technical Definition**\n",
      "**Specification:**\n",
      "Define a technical term clearly and concisely at the beginning of a paragraph. Use a single sentence for the definition, followed by an example or explanation in subsequent sentences.\n",
      "\n",
      "**Example from text:**\n",
      "\"In such a law as 'A, B, C,... in the past, together with X now, cause Y now,' we will call A, B, C,... the mnemic cause, X the occasion or stimulus, and Y the reaction.\"\n",
      "\n",
      "**Source observations:**\n",
      "- **Lexicologist:** \"Technical terms such as 'mnemic causation,' 'psycho-physical parallelism,' and 'engram' are used frequently. They are introduced directly without extensive explanation.\"\n",
      "- **Rhetorician:** \"The writer demonstrates a deep understanding of the subject and engages with complex ideas in a clear and coherent manner.\"\n",
      "- **Information_Architect:** \"The paragraph is structured as a definition, introducing a concept and then elaborating on it.\"\n",
      "\n",
      "**Confidence:** HIGH. The term is clearly defined and used consistently throughout the text.\n",
      "\n",
      "### 2. **Complex Sentence Structure for Nuance**\n",
      "**Specification:**\n",
      "Use complex sentences with multiple subordinate clauses to convey detailed and nuanced ideas. Ensure the main clause appears at the beginning or middle, with subordinate clauses providing additional information.\n",
      "\n",
      "**Example from text:**\n",
      "\"If there is to be parallelism, it is easy to prove by mathematical logic that the causation in physical and psychical matters must be of the same sort, and it is impossible that mnemic causation should exist in psychology but not in physics.\"\n",
      "\n",
      "**Source observations:**\n",
      "- **Syntactician:** \"Subordinate clauses are frequently used to add nuance and detail to main clauses.\"\n",
      "- **Information_Architect:** \"The text exhibits a mix of sentence lengths, with a notable proportion of complex sentences, indicating a high degree of thought complexity.\"\n",
      "\n",
      "**Confidence:** HIGH. The text consistently uses complex sentences to convey nuanced arguments.\n",
      "\n",
      "### \n"
     ]
    }
   ],
   "source": [
    "sample_id = 'sample_005'\n",
    "sample, analyses = store.get_sample_with_analyses(sample_id)\n",
    "print(analyses.keys())\n",
    "\n",
    "print(f\"\\n--- Pattern Analyst Output (first 2000 chars) ---\")\n",
    "print(analyses.get(\"cross_perspective_integrator\", \"Not found\")[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f3d40d",
   "metadata": {},
   "source": [
    "## Stage 2b: Cross-Text Synthesis of Integrated Analyses\n",
    "\n",
    "Patterns that appear across multiple text analyses are synthesized. This stage takes all the cross-perspective integration outputs and identifies overaching techniques, complementary findings, and so on, in order to construct a highly specific conclusion on the techniques that are employed in the text samples. It attempts in other words a synthesis of all analysis, across perspectives and across text samples.\n",
    "\n",
    "This is a single document. In order to track provenance, the text samples and analyst types that went into its construction are gathered and included in the storage in the project database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cef87328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 cross-perspective integration results\n",
      "Sample IDs: ['sample_001', 'sample_002', 'sample_003', 'sample_004', 'sample_005']\n"
     ]
    }
   ],
   "source": [
    "from belletrist.models import CrossTextSynthesizerConfig\n",
    "\n",
    "# Get all samples that have cross-perspective integration results\n",
    "all_samples = store.list_samples()\n",
    "pattern_analyst = CrossPerspectiveIntegratorConfig.analyst_name()\n",
    "\n",
    "# Retrieve all pattern recognition analyses\n",
    "integrated_analyses = {}\n",
    "for sample_id in all_samples:\n",
    "    pattern_analysis = store.get_analysis(sample_id, pattern_analyst)\n",
    "    if pattern_analysis:\n",
    "        integrated_analyses[sample_id] = pattern_analysis\n",
    "    else:\n",
    "        print(f\"⚠ Sample {sample_id} missing cross-perspective integration results\")\n",
    "\n",
    "print(f\"Found {len(integrated_analyses)} cross-perspective integration results\")\n",
    "print(f\"Sample IDs: {list(integrated_analyses.keys())}\")\n",
    "\n",
    "if len(integrated_analyses) < 2:\n",
    "    print(f\"\\n⚠ Need at least 2 pattern recognition analyses for cross-text synthesis. Got {len(integrated_analyses)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8ccfe0",
   "metadata": {},
   "source": [
    "This is where the analysis is run. This can take time, since it requires running an LLM, however, only one LLM call in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3af6ef7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running cross-text synthesis... ✓ (11977 chars)\n",
      "Saved as: cross_text_synthesis_002\n"
     ]
    }
   ],
   "source": [
    "cross_text_config = CrossTextSynthesizerConfig(\n",
    "    integrated_analyses=integrated_analyses\n",
    ")\n",
    "cross_text_prompt = prompt_maker.render(cross_text_config)\n",
    "    \n",
    "print(\"Running Cross-Text Synthesis...\", end=\" \")\n",
    "cross_text_response = llm.complete(cross_text_prompt)\n",
    "print(f\"✓ ({len(cross_text_response.content)} chars)\")\n",
    "    \n",
    "# Save to ResultStore with auto-generated ID and full provenance\n",
    "sample_contributions = [(sid, pattern_analyst) for sid in integrated_analyses.keys()]\n",
    "cross_text_id = store.save_synthesis(\n",
    "    synthesis_type=cross_text_config.synthesis_type(),\n",
    "    output=cross_text_response.content,\n",
    "    model=cross_text_response.model,\n",
    "    sample_contributions=sample_contributions,\n",
    "    config=cross_text_config\n",
    ")\n",
    "print(f\"Saved as: {cross_text_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec90ea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Cross-Text Synthesis (first 1000 chars) ---\n",
      "## SYNTHESIS OF INTEGRATED ANALYSES\n",
      "\n",
      "### I. Stable Core Patterns\n",
      "\n",
      "#### 1. **Complex Sentence Structures for Nuance**\n",
      "   - **Frequency**: 4/5 texts\n",
      "   - **Description**: Use complex sentences with multiple subordinate clauses to convey detailed and nuanced ideas. Main clauses should be 8-12 words, with subordinate clauses extending to 15-20 words.\n",
      "   - **Examples**:\n",
      "     - \"We assume, that is to say, that we know what we mean by saying that a certain event is nearer to another than to a third, so that before making accurate measurements we can speak of the “neighborhood” of an event...\" (Sample 003)\n",
      "     - \"The Far Eastern situation is so complex that it is very difficult to guess what will be the ultimate outcome of the Washington Conference, and still more difficult to know what outcome we ought to desire.\" (Sample 004)\n",
      "   - **Centrality**: Foundational. Complex sentences allow for detailed explanations and nuanced arguments.\n",
      "   - **Mechanical specification**: Main clause (8-12 words), subordinate clauses (15-20 words), complex sentences (30-35 words).\n",
      "\n",
      "#### 2. **Precise Technical Definitions**\n",
      "   - **Frequency**: 3/5 texts\n",
      "   - **Description**: Define technical terms clearly and concisely at the beginning of a paragraph. Use a single sentence for the definition, followed by an example or explanation.\n",
      "   - **Examples**:\n",
      "     - \"In such a law as 'A, B, C,... in the past, together with X now, cause Y now,' we will call A, B, C,... the mnemic cause, X the occasion or stimulus, and Y the reaction.\" (Sample 005)\n",
      "   - **Centrality**: Foundational. Clear definitions establish key concepts for subsequent arguments.\n",
      "   - **Mechanical specification**: Single sentence definition, followed by an example or explanation.\n",
      "\n",
      "#### 3. **Logical Argumentation with Concessions**\n",
      "   - **Frequency**: 3/5 texts\n",
      "   - **Description**: Present logical arguments with clear premises and conclusions. Use concessions to acknowledge opposing views before arguing against them.\n",
      "   - **Examples**:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Cross-Text Synthesis (first 1000 chars) ---\")\n",
    "print(cross_text_response.content[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed323f",
   "metadata": {},
   "source": [
    "## Stage 3: Synthesize Prescriptive Writing Document\n",
    "\n",
    "The final stage converts the descriptive cross-text synthesis into actionable prescriptive writing principles. This generates a style guide that can be used to instruct an LLM to write in a similar style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3a844a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cross-text synthesis: cross_text_synthesis_002\n"
     ]
    }
   ],
   "source": [
    "cross_text_syntheses = store.list_syntheses('cross_text_synthesis')\n",
    "cross_text_synthesis_to_analyze_id = cross_text_syntheses[-1]\n",
    "print(f\"Using cross-text synthesis: {cross_text_synthesis_to_analyze_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c54d38",
   "metadata": {},
   "source": [
    "The following step executes the LLM and can therefore take time. The result is stored for provenance tracking in the project database alongside relevant metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fa5fce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Synthesizer of Writing Style Principles... ✓ (16915 chars)\n",
      "Saved as: principles_guide_002\n"
     ]
    }
   ],
   "source": [
    "from belletrist.models import SynthesizerOfPrinciplesConfig\n",
    "\n",
    "# Build principles guide config and prompt\n",
    "cross_text_synthesis_to_analyze = store.get_synthesis(cross_text_synthesis_to_analyze_id)\n",
    "principles_config = SynthesizerOfPrinciplesConfig(\n",
    "    synthesis_document=cross_text_synthesis_to_analyze['output']\n",
    ")\n",
    "principles_prompt = prompt_maker.render(principles_config)\n",
    "    \n",
    "# Run principles synthesis\n",
    "print(\"Running Synthesizer of Writing Style Principles...\", end=\" \")\n",
    "principles_response = llm.complete(principles_prompt)\n",
    "print(f\"✓ ({len(principles_response.content)} chars)\")\n",
    "    \n",
    "# Save to ResultStore with parent linkage (inherits provenance)\n",
    "principles_id = store.save_synthesis(\n",
    "    synthesis_type=principles_config.synthesis_type(),\n",
    "    output=principles_response.content,\n",
    "    model=principles_response.model,\n",
    "    sample_contributions=[],  # Inherited from parent\n",
    "    config=principles_config,\n",
    "    parent_synthesis_id=cross_text_synthesis_to_analyze_id\n",
    ")\n",
    "print(f\"Saved as: {principles_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c9a521b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Principles Guide (first 2000 chars) ---\n",
      "# A GUIDE TO EFFECTIVE PROSE\n",
      "## Principles Extracted from Pattern Analysis\n",
      "\n",
      "### PART I: FOUNDATIONS\n",
      "**Core Principles**\n",
      "\n",
      "#### 1. Place Core Claims in Concise Independent Clauses\n",
      "**State it clearly**: Place core claims in 8-12 word independent clauses.\n",
      "**Mechanism**: This principle works by ensuring clarity and directness in the presentation of key ideas. It allows the reader to easily grasp the main point before delving into supporting details.\n",
      "**Demonstration**:\n",
      "   - \"It is difficult to define knowledge, difficult to decide whether we have any knowledge, and difficult, even if it is conceded that we sometimes have knowledge to discover whether we can ever know that we have knowledge in this or that particular case.\"\n",
      "   - \"Believers in psycho-physical parallelism hold that psychology can theoretically be freed entirely from all dependence on physiology or physics.\"\n",
      "**Dependencies**: This principle enables the construction of complex sentences with subordinate clauses that add nuance and detail to the main claim. Without this, the text may become convoluted and difficult to follow.\n",
      "\n",
      "#### 2. Use Explicit Connectors for Coherence\n",
      "**State it clearly**: Use explicit connectors like \"Therefore,\" \"But,\" and \"Nor\" to link ideas within and between paragraphs.\n",
      "**Mechanism**: Explicit connectors ensure logical flow and coherence, helping the reader to follow the progression of ideas and arguments.\n",
      "**Demonstration**:\n",
      "   - \"Therefore, when we wish to estimate the desirability of extending the influence of the United States, we have to take account of this almost certain future loss of idealism.\"\n",
      "   - \"That is to say, they believe that every psychical event has a psychical cause and a physical concomitant.\"\n",
      "**Dependencies**: This principle supports logical argumentation and coherence, making the text easier to understand and more persuasive.\n",
      "\n",
      "#### 3. Construct Complex Sentences for Nuance\n",
      "**State it clearly**: Use complex sentences with multiple subordinate clauses to convey deta\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Principles Guide (first 2000 chars) ---\")\n",
    "print(principles_response.content[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586fb6ac",
   "metadata": {},
   "source": [
    "## Querying Synthesis Metadata\n",
    "\n",
    "The ResultStore tracks full provenance for all syntheses. Query metadata to understand what samples, analysts, and models contributed to each synthesis.\n",
    "\n",
    "**This code is for convenience and not required to generate the writing instruction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ded24137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Syntheses\n",
      "==================================================\n",
      "\n",
      "cross_text_synthesis: 2 found\n",
      "  - cross_text_synthesis_001\n",
      "  - cross_text_synthesis_002\n",
      "\n",
      "principles_guide: 2 found\n",
      "  - principles_guide_001\n",
      "  - principles_guide_002\n",
      "\n",
      "\n",
      "Detailed Metadata Example\n",
      "==================================================\n",
      "\n",
      "Synthesis ID: principles_guide_001\n",
      "Type: principles_guide\n",
      "Model: mistral-large-2411\n",
      "Created: 2025-11-24T15:40:32.461919\n",
      "Parent: cross_text_synthesis_001\n",
      "\n",
      "Metadata:\n",
      "  Samples: 0\n",
      "  Sample IDs: []\n",
      "  Model homogeneous: True\n",
      "  Models used: ['mistral-large-2411']\n",
      "\n",
      "\n",
      "Full Provenance Tree\n",
      "==================================================\n",
      "\n",
      "Principles Guide: principles_guide_001\n",
      "  Created: 2025-11-24T15:40:32.461919\n",
      "  Model: mistral-large-2411\n",
      "\n",
      "  Parent (Cross-Text): cross_text_synthesis_001\n",
      "    Sample contributions: 2\n",
      "      - sample_001 / cross_perspective_integrator\n",
      "      - sample_002 / cross_perspective_integrator\n",
      "\n",
      "Principles Guide: principles_guide_002\n",
      "  Created: 2025-11-25T13:03:58.639053\n",
      "  Model: mistral-large-2411\n",
      "\n",
      "  Parent (Cross-Text): cross_text_synthesis_002\n",
      "    Sample contributions: 5\n",
      "      - sample_001 / cross_perspective_integrator\n",
      "      - sample_002 / cross_perspective_integrator\n",
      "      - sample_003 / cross_perspective_integrator\n",
      "      ... and 2 more\n"
     ]
    }
   ],
   "source": [
    "# List all syntheses\n",
    "print(\"All Syntheses\")\n",
    "print(\"=\" * 50)\n",
    "for synth_type in ['cross_text_synthesis', 'principles_guide']:\n",
    "    syntheses = store.list_syntheses(synth_type)\n",
    "    print(f\"\\n{synth_type}: {len(syntheses)} found\")\n",
    "    for synth_id in syntheses:\n",
    "        print(f\"  - {synth_id}\")\n",
    "\n",
    "# Get detailed metadata for a synthesis\n",
    "if store.list_syntheses():\n",
    "    print(\"\\n\\nDetailed Metadata Example\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get first principles guide (or first cross-text if none)\n",
    "    principles = store.list_syntheses('principles_guide')\n",
    "    if principles:\n",
    "        synth_id = principles[0]\n",
    "    else:\n",
    "        synth_id = store.list_syntheses()[0]\n",
    "    \n",
    "    synth_with_meta = store.get_synthesis_with_metadata(synth_id)\n",
    "    \n",
    "    print(f\"\\nSynthesis ID: {synth_with_meta['synthesis_id']}\")\n",
    "    print(f\"Type: {synth_with_meta['type']}\")\n",
    "    print(f\"Model: {synth_with_meta['model']}\")\n",
    "    print(f\"Created: {synth_with_meta['created_at']}\")\n",
    "    print(f\"Parent: {synth_with_meta['parent_id']}\")\n",
    "    \n",
    "    if synth_with_meta.get('metadata'):\n",
    "        meta = synth_with_meta['metadata']\n",
    "        print(f\"\\nMetadata:\")\n",
    "        print(f\"  Samples: {meta['num_samples']}\")\n",
    "        print(f\"  Sample IDs: {meta['sample_ids']}\")\n",
    "        print(f\"  Model homogeneous: {meta['is_homogeneous_model']}\")\n",
    "        print(f\"  Models used: {meta['models_used']}\")\n",
    "\n",
    "# Get full provenance tree\n",
    "if store.list_syntheses('principles_guide'):\n",
    "    print(\"\\n\\nFull Provenance Tree\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for p_id in store.list_syntheses('principles_guide'):\n",
    "        provenance = store.get_synthesis_provenance(p_id)\n",
    "    \n",
    "        print(f\"\\nPrinciples Guide: {provenance['synthesis']['synthesis_id']}\")\n",
    "        print(f\"  Created: {provenance['synthesis']['created_at']}\")\n",
    "        print(f\"  Model: {provenance['synthesis']['model']}\")\n",
    "    \n",
    "        if provenance['parent']:\n",
    "            parent = provenance['parent']\n",
    "            print(f\"\\n  Parent (Cross-Text): {parent['synthesis']['synthesis_id']}\")\n",
    "            print(f\"    Sample contributions: {len(parent['sample_contributions'])}\")\n",
    "            for sample_id, analyst in parent['sample_contributions'][:3]:\n",
    "                print(f\"      - {sample_id} / {analyst}\")\n",
    "            if len(parent['sample_contributions']) > 3:\n",
    "                print(f\"      ... and {len(parent['sample_contributions']) - 3} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3af3c7",
   "metadata": {},
   "source": [
    "## Exporting Syntheses to Filesystem\n",
    "\n",
    "Export final syntheses to text files with YAML metadata headers for consumption by other tools or LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ff6d3ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported: outputs/cross_text_synthesis_001.txt\n",
      "Exported: outputs/cross_text_synthesis_002.txt\n",
      "Exported: outputs/principles_guide_001.txt\n",
      "Exported: outputs/principles_guide_002.txt\n",
      "Exported for style evaluation: outputs/derived_style_instructions.txt\n",
      "\n",
      "All syntheses exported to /Users/andersohrn/PycharmProjects/russell_writes/outputs\n"
     ]
    }
   ],
   "source": [
    "# Create outputs directory\n",
    "outputs_dir = Path(\"outputs\")\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Export cross-text synthesis\n",
    "cross_text_syntheses = store.list_syntheses('cross_text_synthesis')\n",
    "if cross_text_syntheses:\n",
    "    for synth_id in cross_text_syntheses:\n",
    "        output_path = outputs_dir / f\"{synth_id}.txt\"\n",
    "        store.export_synthesis(synth_id, output_path, metadata_format='yaml')\n",
    "        print(f\"Exported: {output_path}\")\n",
    "\n",
    "# Export principles guide\n",
    "principles_guides = store.list_syntheses('principles_guide')\n",
    "if principles_guides:\n",
    "    for synth_id in principles_guides:\n",
    "        output_path = outputs_dir / f\"{synth_id}.txt\"\n",
    "        store.export_synthesis(synth_id, output_path, metadata_format='yaml')\n",
    "        print(f\"Exported: {output_path}\")\n",
    "        \n",
    "        # Also create a special \"derived_style_instructions.txt\" for style_evaluation.ipynb\n",
    "        if synth_id == principles_guides[-1]:  # Use latest\n",
    "            instructions_path = outputs_dir / \"derived_style_instructions.txt\"\n",
    "            store.export_synthesis(synth_id, instructions_path, metadata_format='yaml')\n",
    "            print(f\"Exported for style evaluation: {instructions_path}\")\n",
    "\n",
    "print(f\"\\nAll syntheses exported to {outputs_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bed1d9",
   "metadata": {},
   "source": [
    "## Utilities: Working with Stored Samples\n",
    "\n",
    "Helper functions for browsing and managing stored results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ecbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all samples in the database\n",
    "all_samples = store.list_samples()\n",
    "print(f\"Total samples: {len(all_samples)}\")\n",
    "print(f\"Sample IDs: {all_samples}\")\n",
    "\n",
    "# Check completion status for each\n",
    "print(\"\\nCompletion status:\")\n",
    "for sid in all_samples:\n",
    "    complete = store.is_complete(sid, ANALYSTS)\n",
    "    status = \"✓\" if complete else \"✗\"\n",
    "    print(f\"  {status} {sid}\")\n",
    "\n",
    "# Close database connection when done\n",
    "# store.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
