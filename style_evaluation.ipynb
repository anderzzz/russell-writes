{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Instruction Evaluation Framework\n",
    "\n",
    "This notebook evaluates how well derived style instructions enable style replication, compared to alternative approaches.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each gold standard text:\n",
    "1. **Flatten**: Extract content while removing style\n",
    "2. **Reconstruct**: Generate text using 4 different methods:\n",
    "   - Generic baseline\n",
    "   - Few-shot learning\n",
    "   - Author name prompting\n",
    "   - Derived style instructions\n",
    "3. **Judge**: Compare each reconstruction against the original\n",
    "4. **Aggregate**: Analyze results across multiple texts and runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Base Objects\n",
    "The base objects part of the current project library (`belletrist`) are initialized. They are:\n",
    "* `LLM`: the LLM object.\n",
    "* `LLMConfig`: the configuration of the LLM object, such as what model to use.\n",
    "* `PromptMaker`: generates prompts from templates and variables\n",
    "* `DataSampler`: retrieves and samples text at a source directory\n",
    "\n",
    "These will implement text transformations by LLMs part of the evaluation process. They build on the third-party LLMs, which we furthermore split into LLMs for text reconstruction and text judging, the key parameters for which are set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reconstruction_string = 'together_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput'\n",
    "model_reconstruction_api_key_env_var = 'TOGETHER_AI_API_KEY'\n",
    "model_judge_string = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "model_judge_api_key_env_var = 'ANTHROPIC_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist import PromptMaker, DataSampler\n",
    "\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler(\n",
    "    data_path=(Path(os.getcwd()) / \"data\" / \"russell\").resolve()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist import LLM, LLMConfig\n",
    "\n",
    "reconstruction_llm = LLM(LLMConfig(\n",
    "    model=model_reconstruction_string,\n",
    "    api_key=os.environ.get(model_reconstruction_api_key_env_var)\n",
    "))\n",
    "judge_llm = LLM(LLMConfig(\n",
    "    model=model_judge_string,\n",
    "    api_key=os.environ.get(model_judge_api_key_env_var)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Test Data and Few-Shot Data\n",
    "The reconstruction method tests build on gold standard texts. The test also includes few-shot prompting with the gold standard texts. In order to not skew the tests, no few-shot examples can overlap with the test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "n_sample = 10\nm_paragraphs_per_sample = 5\nn_few_shot_sample = 3\n\ntest_texts = []\nfor _ in range(n_sample):\n    test_texts.append(sampler.sample_segment(p_length=m_paragraphs_per_sample))\n\nfew_shot_texts = []\nwhile len(few_shot_texts) < n_few_shot_sample:\n    p = sampler.sample_segment(p_length=m_paragraphs_per_sample)\n\n    # Check if p overlaps with any test text\n    # Two segments overlap if they're from the same file AND their paragraph ranges overlap\n    # Ranges [a, b) and [c, d) overlap if: a < d AND c < b\n    has_overlap = any(\n        p.file_index == test_seg.file_index and\n        p.paragraph_start < test_seg.paragraph_end and\n        test_seg.paragraph_start < p.paragraph_end\n        for test_seg in test_texts\n    )\n\n    if not has_overlap:\n        few_shot_texts.append(p)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Test Transformation Objects\n",
    "The combination of prompt and LLM leads to the following operators in the test chain:\n",
    "* **Style Flattener**, which given a text compresses it into its content bare bones.\n",
    "* **Reconstructor, LLM House Style**, which given a compressed content expands it into a complete text with the \"house style\" of the LLM employed for the reconstruction.\n",
    "* **Reconstructor, Few Shot**, which given a compressed content expands it into a complete text with a few text excerpts on unrelated topics as style guide.\n",
    "* **Reconstructor, LLM Author Model**, which given a compressed content expands it into a complete text with the named author's style as the LLM conceives it without any other guidance.\n",
    "* **Reconstructor, Style Instruction**, which given a compressed content expands it into a complete text following the detailed style instruction, as derived from previous analysis."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "style_instructions_path = Path(\"outputs/derived_style_instructions_1125_000.txt\")\n",
    "\n",
    "if style_instructions_path.exists():\n",
    "    style_instructions = style_instructions_path.read_text()\n",
    "    print(f\"✓ Loaded style instructions ({len(style_instructions)} chars)\")\n",
    "    print(f\"\\nFirst 200 chars:\\n{style_instructions[:200]}...\")\n",
    "else:\n",
    "    print(\"⚠ Style instructions file not found. Please provide path.\")\n",
    "    style_instructions = \"\"  # Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist.models import (\n",
    "    StyleFlatteningConfig,\n",
    "    StyleReconstructionGenericConfig,\n",
    "    StyleReconstructionFewShotConfig,\n",
    "    StyleReconstructionAuthorConfig,\n",
    "    StyleReconstructionInstructionsConfig\n",
    ")\n",
    "n_runs = 3\n",
    "RECONSTRUCTORS = [\n",
    "    'generic',\n",
    "    'fewshot',\n",
    "    'author',\n",
    "    'instructions'\n",
    "]\n",
    "RECONSTRUCTORS_CFGS = {\n",
    "    'generic': StyleReconstructionGenericConfig,\n",
    "    'fewshot': StyleReconstructionFewShotConfig,\n",
    "    'author': StyleReconstructionAuthorConfig,\n",
    "    'instructions': StyleReconstructionInstructionsConfig\n",
    "}\n",
    "RECONSTRUCTORS_KWARGS = {\n",
    "    'generic': {},\n",
    "    'fewshot': {'few_shot_examples': [seg.text for seg in few_shot_texts]},\n",
    "    'author': {'author_name': 'Bertrand Russell'},\n",
    "    'instructions': {'style_instructions': style_instructions}\n",
    "}\n",
    "\n",
    "reconstructions = {}\n",
    "for k_text, test_sample in enumerate(test_texts):\n",
    "    flatten_prompt = prompt_maker.render(\n",
    "        StyleFlatteningConfig(text=test_sample.text)\n",
    "    )\n",
    "    flattened_content = reconstruction_llm.complete(flatten_prompt)\n",
    "    for k_run in range(n_runs):\n",
    "        for reconstructor in RECONSTRUCTORS:\n",
    "            config = RECONSTRUCTORS_CFGS[reconstructor](content_summary=flattened_content.content, **RECONSTRUCTORS_KWARGS[reconstructor])\n",
    "            reconstruct_prompt = prompt_maker.render(config)\n",
    "            reconstructed_text = reconstruction_llm.complete(reconstruct_prompt)\n",
    "\n",
    "            reconstructions[(k_text, k_run, reconstructor)] = reconstructed_text.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Content Flattening\n",
    "\n",
    "Extract content from each test sample, removing stylistic elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all test samples\n",
    "for sample in test_samples:\n",
    "    print(f\"Flattening {sample['id']}...\", end=\" \")\n",
    "    \n",
    "    flattening_config = StyleFlatteningConfig(text=sample['text'])\n",
    "    flattening_prompt = prompt_maker.render(flattening_config)\n",
    "    \n",
    "    response = reconstruction_llm.complete(flattening_prompt)\n",
    "    sample['content_summary'] = response.content\n",
    "    \n",
    "    print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "print(f\"\\n✓ All samples flattened\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reconstruction\n",
    "\n",
    "Generate reconstructions using all 4 methods, with M stochastic runs each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store reconstructions\n",
    "reconstructions = []\n",
    "\n",
    "for sample in test_samples:\n",
    "    print(f\"\\nProcessing {sample['id']}:\")\n",
    "    \n",
    "    for run in range(M_RUNS):\n",
    "        print(f\"  Run {run+1}/{M_RUNS}:\")\n",
    "        \n",
    "        content_summary = sample['content_summary']\n",
    "        \n",
    "        # Method 1: Generic baseline\n",
    "        print(\"    - Generic...\", end=\" \")\n",
    "        config = StyleReconstructionGenericConfig(content_summary=content_summary)\n",
    "        prompt = prompt_maker.render(config)\n",
    "        response = reconstruction_llm.complete(prompt)\n",
    "        reconstructions.append({\n",
    "            'sample_id': sample['id'],\n",
    "            'run': run,\n",
    "            'method': 'generic',\n",
    "            'text': response.content\n",
    "        })\n",
    "        print(\"✓\")\n",
    "        \n",
    "        # Method 2: Few-shot\n",
    "        print(\"    - Few-shot...\", end=\" \")\n",
    "        config = StyleReconstructionFewShotConfig(\n",
    "            content_summary=content_summary,\n",
    "            few_shot_examples=fewshot_examples\n",
    "        )\n",
    "        prompt = prompt_maker.render(config)\n",
    "        response = reconstruction_llm.complete(prompt)\n",
    "        reconstructions.append({\n",
    "            'sample_id': sample['id'],\n",
    "            'run': run,\n",
    "            'method': 'fewshot',\n",
    "            'text': response.content\n",
    "        })\n",
    "        print(\"✓\")\n",
    "        \n",
    "        # Method 3: Author name\n",
    "        print(\"    - Author name...\", end=\" \")\n",
    "        config = StyleReconstructionAuthorConfig(\n",
    "            content_summary=content_summary,\n",
    "            author_name=AUTHOR_NAME\n",
    "        )\n",
    "        prompt = prompt_maker.render(config)\n",
    "        response = reconstruction_llm.complete(prompt)\n",
    "        reconstructions.append({\n",
    "            'sample_id': sample['id'],\n",
    "            'run': run,\n",
    "            'method': 'author',\n",
    "            'text': response.content\n",
    "        })\n",
    "        print(\"✓\")\n",
    "        \n",
    "        # Method 4: Derived instructions\n",
    "        print(\"    - Instructions...\", end=\" \")\n",
    "        config = StyleReconstructionInstructionsConfig(\n",
    "            content_summary=content_summary,\n",
    "            style_instructions=style_instructions\n",
    "        )\n",
    "        prompt = prompt_maker.render(config)\n",
    "        response = reconstruction_llm.complete(prompt)\n",
    "        reconstructions.append({\n",
    "            'sample_id': sample['id'],\n",
    "            'run': run,\n",
    "            'method': 'instructions',\n",
    "            'text': response.content\n",
    "        })\n",
    "        print(\"✓\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(reconstructions)} total reconstructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Judging\n",
    "\n",
    "Compare each reconstruction against the original using the judge LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store judgments\n",
    "judgments = []\n",
    "\n",
    "for reconstruction in reconstructions:\n",
    "    # Find corresponding original sample\n",
    "    sample = next(s for s in test_samples if s['id'] == reconstruction['sample_id'])\n",
    "    \n",
    "    print(f\"Judging {reconstruction['sample_id']}, run {reconstruction['run']}, method {reconstruction['method']}...\", end=\" \")\n",
    "    \n",
    "    # Build judge prompt\n",
    "    judge_config = StyleJudgeConfig(\n",
    "        original_text=sample['text'],\n",
    "        reconstruction_text=reconstruction['text']\n",
    "    )\n",
    "    judge_prompt = prompt_maker.render(judge_config)\n",
    "    \n",
    "    # Get structured JSON judgment\n",
    "    response = judge_llm.complete_json(judge_prompt)\n",
    "    \n",
    "    try:\n",
    "        # Parse and validate JSON\n",
    "        judgment_data = json.loads(response.content)\n",
    "        judgment = StyleJudgment(**judgment_data)\n",
    "        \n",
    "        # Store judgment\n",
    "        judgments.append({\n",
    "            'sample_id': reconstruction['sample_id'],\n",
    "            'run': reconstruction['run'],\n",
    "            'method': reconstruction['method'],\n",
    "            'ranking': judgment.ranking,\n",
    "            'confidence': judgment.confidence,\n",
    "            'reasoning': judgment.reasoning,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        print(f\"✓ {judgment.ranking}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        judgments.append({\n",
    "            'sample_id': reconstruction['sample_id'],\n",
    "            'run': reconstruction['run'],\n",
    "            'method': reconstruction['method'],\n",
    "            'ranking': 'error',\n",
    "            'confidence': 'error',\n",
    "            'reasoning': str(e),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "print(f\"\\n✓ Completed {len(judgments)} judgments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(judgments)\n",
    "\n",
    "# Remove error rows\n",
    "df_clean = df[df['ranking'] != 'error']\n",
    "\n",
    "print(f\"Total judgments: {len(df)}\")\n",
    "print(f\"Valid judgments: {len(df_clean)}\")\n",
    "print(f\"Errors: {len(df) - len(df_clean)}\")\n",
    "\n",
    "# Show summary\n",
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results by method\n",
    "print(\"\\n=== Results by Method ===\")\n",
    "print(\"\\nRanking distribution:\")\n",
    "print(df_clean.groupby(['method', 'ranking']).size().unstack(fill_value=0))\n",
    "\n",
    "print(\"\\nConfidence distribution:\")\n",
    "print(df_clean.groupby(['method', 'confidence']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate success rate (reconstruction_better)\n",
    "success_rates = df_clean[df_clean['ranking'] == 'reconstruction_better'].groupby('method').size() / df_clean.groupby('method').size()\n",
    "\n",
    "print(\"\\n=== Success Rate by Method ===\")\n",
    "print(\"(Percentage where reconstruction was judged better than original)\\n\")\n",
    "print(success_rates.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "output_file = f\"style_evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "TODO: Add analysis cells for:\n",
    "- Statistical significance testing\n",
    "- Visualization of results\n",
    "- Sample-by-sample breakdown\n",
    "- Confidence level analysis\n",
    "- Qualitative review of reasoning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
