{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Instruction Evaluation Framework\n",
    "\n",
    "This notebook evaluates how well derived style instructions enable style replication, compared to alternative approaches.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each gold standard text:\n",
    "1. **Flatten**: Extract content while removing style\n",
    "2. **Reconstruct**: Generate text using 4 different methods (M stochastic runs each):\n",
    "   - Generic baseline\n",
    "   - Few-shot learning\n",
    "   - Author name prompting\n",
    "   - Derived style instructions\n",
    "3. **Judge (Blind Comparative)**: Judge ranks all 4 reconstructions from 1-4 based on similarity to original\n",
    "   - **Blind evaluation**: Judge sees only anonymous labels (Text A, B, C, D) - no method names\n",
    "   - **Ranking**: 1 = most similar, 2 = second, 3 = third, 4 = least similar\n",
    "   - **Order randomized**: Position of methods varies across samples to eliminate bias\n",
    "4. **Aggregate**: Analyze rankings to determine which method best captures style\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Crash resilient**: All LLM responses saved to SQLite immediately\n",
    "- **Resume support**: Can restart after failures, skips completed work\n",
    "- **Blind evaluation**: Eliminates judge bias by hiding method names\n",
    "- **Comparative ranking**: More informative than binary comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries and Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm==1.79.3 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.79.3)\n",
      "Requirement already satisfied: pydantic==2.7.4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (2.7.4)\n",
      "Requirement already satisfied: jinja2>=3.1.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (3.10.11)\n",
      "Requirement already satisfied: click in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.27.0)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (8.5.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (4.23.0)\n",
      "Requirement already satisfied: openai>=1.99.5 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (2.2.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (1.0.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: tokenizers in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from litellm==1.79.3->-r requirements.txt (line 1)) (0.9.4)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from pydantic==2.7.4->-r requirements.txt (line 2)) (4.13.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jinja2>=3.1.0->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (4.0.2)\n",
      "Requirement already satisfied: anyio in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.7.1)\n",
      "Requirement already satisfied: certifi in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.0.5)\n",
      "Requirement already satisfied: idna in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.10)\n",
      "Requirement already satisfied: sniffio in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=6.8.0->litellm==1.79.3->-r requirements.txt (line 1)) (3.20.2)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (5.10.2)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (2023.12.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.79.3->-r requirements.txt (line 1)) (0.20.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (0.9.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from openai>=1.99.5->litellm==1.79.3->-r requirements.txt (line 1)) (4.64.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2022.4.24)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.27.1)\n",
      "Requirement already satisfied: exceptiongroup in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from anyio->httpx>=0.23.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.1.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm==1.79.3->-r requirements.txt (line 1)) (2.0.12)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/andersohrn/opt/anaconda3/lib/python3.8/site-packages (from yarl<2.0,>=1.12.0->aiohttp>=3.10->litellm==1.79.3->-r requirements.txt (line 1)) (0.2.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providers\n",
      "=========\n",
      "* openai\n",
      "* openai_like\n",
      "* bytez\n",
      "* xai\n",
      "* custom_openai\n",
      "* text-completion-openai\n",
      "* cohere\n",
      "* cohere_chat\n",
      "* clarifai\n",
      "* anthropic\n",
      "* anthropic_text\n",
      "* replicate\n",
      "* huggingface\n",
      "* together_ai\n",
      "* datarobot\n",
      "* openrouter\n",
      "* cometapi\n",
      "* vertex_ai\n",
      "* vertex_ai_beta\n",
      "* gemini\n",
      "* ai21\n",
      "* baseten\n",
      "* azure\n",
      "* azure_text\n",
      "* azure_ai\n",
      "* sagemaker\n",
      "* sagemaker_chat\n",
      "* bedrock\n",
      "* vllm\n",
      "* nlp_cloud\n",
      "* petals\n",
      "* oobabooga\n",
      "* ollama\n",
      "* ollama_chat\n",
      "* deepinfra\n",
      "* perplexity\n",
      "* mistral\n",
      "* groq\n",
      "* nvidia_nim\n",
      "* cerebras\n",
      "* baseten\n",
      "* ai21_chat\n",
      "* volcengine\n",
      "* codestral\n",
      "* text-completion-codestral\n",
      "* deepseek\n",
      "* sambanova\n",
      "* maritalk\n",
      "* cloudflare\n",
      "* fireworks_ai\n",
      "* friendliai\n",
      "* watsonx\n",
      "* watsonx_text\n",
      "* triton\n",
      "* predibase\n",
      "* databricks\n",
      "* empower\n",
      "* github\n",
      "* custom\n",
      "* litellm_proxy\n",
      "* hosted_vllm\n",
      "* llamafile\n",
      "* lm_studio\n",
      "* galadriel\n",
      "* gradient_ai\n",
      "* github_copilot\n",
      "* novita\n",
      "* meta_llama\n",
      "* featherless_ai\n",
      "* nscale\n",
      "* nebius\n",
      "* dashscope\n",
      "* moonshot\n",
      "* v0\n",
      "* heroku\n",
      "* oci\n",
      "* morph\n",
      "* lambda_ai\n",
      "* vercel_ai_gateway\n",
      "* wandb\n",
      "* ovhcloud\n",
      "* lemonade\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Base Objects\n",
    "The base objects part of the current project library (`belletrist`) are initialized. They are:\n",
    "* `LLM`: the LLM object.\n",
    "* `LLMConfig`: the configuration of the LLM object, such as what model to use.\n",
    "* `PromptMaker`: generates prompts from templates and variables\n",
    "* `DataSampler`: retrieves and samples text at a source directory\n",
    "\n",
    "These will implement text transformations by LLMs part of the evaluation process. They build on the third-party LLMs, which we furthermore split into LLMs for text reconstruction and text judging, the key parameters for which are set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reconstruction_string = 'together_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput'\n",
    "model_reconstruction_api_key_env_var = 'TOGETHER_AI_API_KEY'\n",
    "#model_reconstruction_string = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "#model_reconstruction_api_key_env_var = 'ANTHROPIC_API_KEY'\n",
    "model_judge_string = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "model_judge_api_key_env_var = 'ANTHROPIC_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist import PromptMaker, DataSampler, StyleEvaluationStore\n",
    "\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler(\n",
    "    data_path=(Path(os.getcwd()) / \"data\" / \"russell\").resolve()\n",
    ")\n",
    "store = StyleEvaluationStore(Path(\"style_evaluation_results_mistral_qwen_sonnet_sparse_fewshot.db\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.reset('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist import LLM, LLMConfig\n",
    "\n",
    "reconstruction_llm = LLM(LLMConfig(\n",
    "    model=model_reconstruction_string,\n",
    "    api_key=os.environ.get(model_reconstruction_api_key_env_var)\n",
    "))\n",
    "judge_llm = LLM(LLMConfig(\n",
    "    model=model_judge_string,\n",
    "    api_key=os.environ.get(model_judge_api_key_env_var)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Test Data and Few-Shot Data\n",
    "The reconstruction method tests build on gold standard texts. The test also includes few-shot prompting with the gold standard texts. In order to not skew the tests, no few-shot examples can overlap with the test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 5\n",
    "m_paragraphs_per_sample = 5\n",
    "n_few_shot_sample = 2\n",
    "\n",
    "test_texts = []\n",
    "for _ in range(n_sample):\n",
    "    test_texts.append(sampler.sample_segment(p_length=m_paragraphs_per_sample))\n",
    "\n",
    "few_shot_texts = []\n",
    "while len(few_shot_texts) < n_few_shot_sample:\n",
    "    p = sampler.sample_segment(p_length=m_paragraphs_per_sample)\n",
    "\n",
    "    # Check if p overlaps with any test text\n",
    "    # Two segments overlap if they're from the same file AND their paragraph ranges overlap\n",
    "    # Ranges [a, b) and [c, d) overlap if: a < d AND c < b\n",
    "    has_overlap = any(\n",
    "        p.file_index == test_seg.file_index and\n",
    "        p.paragraph_start < test_seg.paragraph_end and\n",
    "        test_seg.paragraph_start < p.paragraph_end\n",
    "        for test_seg in test_texts\n",
    "    )\n",
    "\n",
    "    if not has_overlap:\n",
    "        few_shot_texts.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Test Transformation Objects\n",
    "The combination of prompt and LLM leads to the following operators in the test chain:\n",
    "* **Style Flattener**, which given a text compresses it into its content bare bones.\n",
    "* **Reconstructor, LLM House Style**, which given a compressed content expands it into a complete text with the \"house style\" of the LLM employed for the reconstruction.\n",
    "* **Reconstructor, Few Shot**, which given a compressed content expands it into a complete text with a few text excerpts on unrelated topics as style guide.\n",
    "* **Reconstructor, LLM Author Model**, which given a compressed content expands it into a complete text with the named author's style as the LLM conceives it without any other guidance.\n",
    "* **Reconstructor, Style Instruction**, which given a compressed content expands it into a complete text following the detailed style instruction, as derived from previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded style instructions (24952 chars)\n",
      "\n",
      "First 200 chars:\n",
      "---\n",
      "synthesis_id: principles_guide_001\n",
      "synthesis_type: principles_guide\n",
      "model: mistral-large-2411\n",
      "created_at: 2025-11-27T18:18:13.356530\n",
      "parent_synthesis_id: cross_text_synthesis_001\n",
      "num_samples: 5\n",
      "sample_ids:\n",
      "  - sample_005\n",
      "  - sample_002\n",
      "  - sample_001\n",
      "  - sample_004\n",
      "  - sample_003\n",
      "is_homogeneous_model: true\n",
      "models_used:\n",
      "  - mistral-large-2411\n",
      "---\n",
      "\n",
      "# THE SYNTHESIST: A STYLE GUIDE FOR WRITERS\n",
      "\n",
      "## SECTION 1: FOUNDATIONAL PRINCIPLES\n",
      "\n",
      "### Subordinate Complexity to Simple Frames\n",
      "\n",
      "Place your core cl...\n"
     ]
    }
   ],
   "source": [
    "style_instructions_path = Path(\"outputs/derived_style_instructions_mistral_1127.txt\")\n",
    "\n",
    "if style_instructions_path.exists():\n",
    "    style_instructions = style_instructions_path.read_text()\n",
    "    print(f\"✓ Loaded style instructions ({len(style_instructions)} chars)\")\n",
    "    print(f\"\\nFirst 200 chars:\\n{style_instructions[:500]}...\")\n",
    "else:\n",
    "    print(\"⚠ Style instructions file not found. Please provide path.\")\n",
    "    style_instructions = \"\"  # Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Store initialized at style_evaluation_results_mistral_qwen_sonnet_sparse_fewshot.db\n",
      "✓ Configuration: 3 runs per sample, 4 reconstruction methods\n"
     ]
    }
   ],
   "source": [
    "from belletrist.models import (\n",
    "    StyleFlatteningConfig,\n",
    "    StyleReconstructionGenericConfig,\n",
    "    StyleReconstructionFewShotConfig,\n",
    "    StyleReconstructionAuthorConfig,\n",
    "    StyleReconstructionInstructionsConfig,\n",
    "    MethodMapping,\n",
    "    StyleJudgeComparativeConfig\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "n_runs = 3\n",
    "AUTHOR_NAME = \"Bertrand Russell\"\n",
    "\n",
    "# Reconstructor configs\n",
    "RECONSTRUCTORS_CFGS = {\n",
    "    'generic': StyleReconstructionGenericConfig,\n",
    "    'fewshot': StyleReconstructionFewShotConfig,\n",
    "    'author': StyleReconstructionAuthorConfig,\n",
    "    'instructions': StyleReconstructionInstructionsConfig\n",
    "}\n",
    "\n",
    "# Reconstructor kwargs\n",
    "RECONSTRUCTORS_KWARGS = {\n",
    "    'generic': {},\n",
    "    'fewshot': {'few_shot_examples': [seg.text for seg in few_shot_texts]},\n",
    "    'author': {'author_name': AUTHOR_NAME},\n",
    "    'instructions': {'style_instructions': style_instructions}\n",
    "}\n",
    "\n",
    "print(f\"✓ Store initialized at {store.filepath}\")\n",
    "print(f\"✓ Configuration: {n_runs} runs per sample, 4 reconstruction methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Content Flattening\n",
    "\n",
    "Extract content from each test sample, removing stylistic elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 1: Flattening and Saving Samples ===\n",
      "\n",
      "✓ sample_000 already flattened (skipping)\n",
      "✓ sample_001 already flattened (skipping)\n",
      "✓ sample_002 already flattened (skipping)\n",
      "✓ sample_003 already flattened (skipping)\n",
      "✓ sample_004 already flattened (skipping)\n",
      "\n",
      "✓ All samples flattened and saved to store\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Save samples and flatten content\n",
    "print(\"=== Step 1: Flattening and Saving Samples ===\\n\")\n",
    "\n",
    "for k_text, test_sample in enumerate(test_texts):\n",
    "    sample_id = f\"sample_{k_text:03d}\"\n",
    "    \n",
    "    # Skip if already saved\n",
    "    if store.get_sample(sample_id):\n",
    "        print(f\"✓ {sample_id} already flattened (skipping)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Flattening {sample_id}...\", end=\" \")\n",
    "    \n",
    "    # Flatten content\n",
    "    flatten_prompt = prompt_maker.render(\n",
    "        StyleFlatteningConfig(text=test_sample.text)\n",
    "    )\n",
    "    flattened = reconstruction_llm.complete(flatten_prompt)\n",
    "    \n",
    "    # Save to store with provenance\n",
    "    source_info = f\"File {test_sample.file_index}, para {test_sample.paragraph_start}-{test_sample.paragraph_end}\"\n",
    "    store.save_sample(\n",
    "        sample_id=sample_id,\n",
    "        original_text=test_sample.text,\n",
    "        flattened_content=flattened.content,\n",
    "        flattening_model=flattened.model,\n",
    "        source_info=source_info\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ ({len(flattened.content)} chars)\")\n",
    "\n",
    "print(f\"\\n✓ All samples flattened and saved to store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====ORIGINAL===\n",
      "The really difficult effort required for solving this problem was in\n",
      "regard to time. It was necessary to introduce the notion of “proper”\n",
      "time which we have already considered, and to abandon the old belief in\n",
      "one universal time. The quantitative laws of electromagnetic phenomena\n",
      "are expressed in Maxwell’s equations, and these equations are found\n",
      "to be true for any observer, however he may be moving.[3] It is a\n",
      "straight-forward mathematical problem to find out what differences\n",
      "there must be between the measures applied by one observer and the\n",
      "measures applied by another, if, in spite of their relative motion,\n",
      "they are to find the same equations verified. The answer is contained\n",
      "in the “Lorentz transformation,” found as a formula by Lorentz, but\n",
      "interpreted and made intelligible by Einstein.\n",
      "\n",
      "The Lorentz transformation tells us what estimate of distances and\n",
      "periods of time will be made by an observer whose relative motion is\n",
      "known, when we are given those of another observer. We may suppose that\n",
      "you are in a train on a railway which travels due east. You have been\n",
      "traveling for a time which, by the clocks at the station from which\n",
      "you started, is _t_. At a distance _x_ from your starting point, as\n",
      "measured by the people on the line, an event occurs at this moment—say\n",
      "the line is struck by lightning. You have been traveling all the time\n",
      "with a uniform velocity _v_. The question is: How far from you will you\n",
      "judge that this event has taken place, and how long after you started\n",
      "will it be by your watch, assuming that your watch is correct from the\n",
      "point of view of an observer on the train?\n",
      "\n",
      "[3] So long as he has no considerable acceleration. The treatment of\n",
      "acceleration belongs to the _general_ theory of relativity.\n",
      "\n",
      "Our solution of this problem has to satisfy certain conditions. It has\n",
      "to bring out the result that the velocity of light is the same for all\n",
      "observers, however they may be moving. And it has to make physical\n",
      "phenomena—in particular, those of electromagnetism—obey the same\n",
      "laws for different observers, however they may find their measures of\n",
      "distances and times affected by their motion. And it has to make all\n",
      "such effects on measurement reciprocal. That is to say, if you are in\n",
      "a train and your motion affects your estimate of distances outside the\n",
      "train, there must be an exactly similar change in the estimate which\n",
      "people outside the train make of distances inside it. These conditions\n",
      "are sufficient to determine the solution of the problem, but the\n",
      "method of obtaining the solution cannot be explained without more\n",
      "mathematics than is possible in the present work.\n",
      "\n",
      "Before dealing with the matter in general terms, let us take an\n",
      "example. Let us suppose that you are in a train on a long straight\n",
      "railway, and that you are traveling at three-fifths of the velocity\n",
      "of light. Suppose that you measure the length of your train, and find\n",
      "that it is a hundred yards. Suppose that the people who catch a glimpse\n",
      "of you as you pass succeed, by skilful scientific methods, in taking\n",
      "observations which enable them to calculate the length of your train.\n",
      "If they do their work correctly, they will find that it is eighty\n",
      "yards long. Everything in the train will seem to them shorter in the\n",
      "direction of the train than it does to you. Dinner plates, which you\n",
      "see as ordinary circular plates, will look to the outsider as if they\n",
      "were oval: they will seem only four-fifths as broad in the direction\n",
      "in which the train is moving as in the direction of the breadth of the\n",
      "train. And all this is reciprocal. Suppose you see out of the window a\n",
      "man carrying a fishing rod which, by his measurement, is fifteen feet\n",
      "long. If he is holding it upright, you will see it as he does; so you\n",
      "will if he is holding it horizontally at right angles to the railway.\n",
      "But if he is pointing it along the railway, it will seem to you to\n",
      "be only twelve feet long. All lengths in the direction of motion are\n",
      "diminished by twenty per cent, both for those who look into the train\n",
      "from outside and for those who look out of the train from inside.\n",
      "\n",
      "\n",
      "====FLATTENED====\n",
      "- The primary challenge in solving the problem involves the concept of time.\n",
      "- It was necessary to introduce the concept of \"proper\" time.\n",
      "- The idea of a single universal time had to be abandoned.\n",
      "- The laws governing electromagnetic phenomena are described by Maxwell’s equations.\n",
      "- Maxwell’s equations hold true for any observer, regardless of their state of motion, provided they are not undergoing significant acceleration.\n",
      "- The treatment of acceleration is part of the general theory of relativity, not the current discussion.\n",
      "- A mathematical problem arises: determining how measurements of space and time must differ between observers in relative motion so that both observe the same physical laws.\n",
      "- The solution to this problem is given by the Lorentz transformation.\n",
      "- The Lorentz transformation was originally derived as a formula by Lorentz.\n",
      "- Einstein provided the interpretation and physical understanding of the transformation.\n",
      "- The Lorentz transformation specifies how an observer in motion will measure distances and time intervals, given the measurements of another observer and their relative velocity.\n",
      "- A scenario is presented: an observer is on a train moving east at a constant velocity v.\n",
      "- The train has been traveling for a time t, as measured by clocks at the starting station.\n",
      "- An event occurs at a distance x from the starting point, as measured by observers on the ground (e.g., lightning striking the track).\n",
      "- The question is: what distance and time will the observer on the train assign to this event, based on their own measurements?\n",
      "- The solution must satisfy several conditions:\n",
      "  - The speed of light must be the same for all observers, regardless of their motion.\n",
      "  - The laws of physics, especially those of electromagnetism, must be the same for all observers.\n",
      "  - The effects of motion on measurements of space and time must be reciprocal between observers.\n",
      "- Reciprocity means that if an observer on the train measures lengths outside the train as altered due to motion, then observers outside must measure lengths inside the train as altered in the same way.\n",
      "- These conditions are sufficient to determine the correct transformation between observers.\n",
      "- The mathematical derivation of the solution is not provided due to its complexity.\n",
      "- An example is used to illustrate the effects.\n",
      "- An observer is on a train moving at 3/5 the speed of light relative to the ground.\n",
      "- The observer on the train measures the train’s length as 100 yards.\n",
      "- Ground observers, using correct methods, calculate the train’s length as 80 yards.\n",
      "- Objects inside the train appear shorter in the direction of motion to outside observers.\n",
      "- A circular dinner plate on the train appears to outside observers as oval, with its dimension in the direction of motion reduced to 4/5 of its rest width.\n",
      "- The shortening factor is 4/5 (a 20% reduction) for lengths in the direction of motion.\n",
      "- This effect is reciprocal.\n",
      "- If a person on the ground holds a fishing rod 15 feet long:\n",
      "  - If held vertically or perpendicular to the direction of motion, its length appears unchanged to the observer on the train.\n",
      "  - If held parallel to the track (along the direction of motion), it appears to the observer on the train as 12 feet long.\n",
      "- Thus, lengths in the direction of motion are reduced by 20% for both:\n",
      "  - Observers measuring objects in a moving frame from outside.\n",
      "  - Observers in the moving frame measuring objects at rest outside.\n"
     ]
    }
   ],
   "source": [
    "print('====ORIGINAL===')\n",
    "print(store.get_sample('sample_001')['original_text'])\n",
    "print('\\n\\n====FLATTENED====')\n",
    "print(store.get_sample('sample_001')['flattened_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reconstruction\n",
    "\n",
    "Generate reconstructions using all 4 methods, with M stochastic runs each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 2: Generating Reconstructions ===\n",
      "\n",
      "\n",
      "sample_000:\n",
      "  Run 0:\n",
      "    ✓ generic      (already done)\n",
      "    ✓ fewshot      (11121 chars)\n",
      "    ✓ author       (10280 chars)\n",
      "    ✓ instructions (8513 chars)\n",
      "  Run 1:\n",
      "    ✓ generic      (9579 chars)\n",
      "    ✓ fewshot      (9511 chars)\n",
      "    ✓ author       (10557 chars)\n",
      "    ✓ instructions (7058 chars)\n",
      "  Run 2:\n",
      "    ✓ generic      (8416 chars)\n",
      "    ✓ fewshot      (9076 chars)\n",
      "    ✓ author       (9830 chars)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Generate reconstructions (with crash resume)\n",
    "print(\"=== Step 2: Generating Reconstructions ===\\n\")\n",
    "\n",
    "for sample_id in store.list_samples():\n",
    "    sample = store.get_sample(sample_id)\n",
    "    print(f\"\\n{sample_id}:\")\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        print(f\"  Run {run}:\")\n",
    "        \n",
    "        # Check which methods need reconstruction\n",
    "        for method in ['generic', 'fewshot', 'author', 'instructions']:\n",
    "            if store.has_reconstruction(sample_id, run, method):\n",
    "                print(f\"    ✓ {method:12s} (already done)\")\n",
    "                continue\n",
    "            \n",
    "            # Generate reconstruction\n",
    "            config = RECONSTRUCTORS_CFGS[method](\n",
    "                content_summary=sample['flattened_content'],\n",
    "                **RECONSTRUCTORS_KWARGS[method]\n",
    "            )\n",
    "            prompt = prompt_maker.render(config)\n",
    "            response = reconstruction_llm.complete(prompt)\n",
    "            \n",
    "            # Save immediately (crash resilient!)\n",
    "            store.save_reconstruction(\n",
    "                sample_id=sample_id,\n",
    "                run=run,\n",
    "                method=method,\n",
    "                reconstructed_text=response.content,\n",
    "                model=response.model\n",
    "            )\n",
    "            print(f\"    ✓ {method:12s} ({len(response.content)} chars)\")\n",
    "\n",
    "stats = store.get_stats()\n",
    "print(f\"\\n✓ Generated {stats['n_reconstructions']} total reconstructions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RECONSTRUCTORS_CFGS['fewshot'](\n",
    "                content_summary=sample['flattened_content'],\n",
    "                **RECONSTRUCTORS_KWARGS['fewshot']\n",
    "            )\n",
    "prompt = prompt_maker.render(config)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = store.get_reconstructions('sample_001', 0)\n",
    "for reconstructor in reconstructions.keys():\n",
    "    print(f\"{reconstructor.upper()}\\n===================\")\n",
    "    print(f\"\\n{reconstructions.get(reconstructor)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Judging\n",
    "\n",
    "Compare each reconstruction against the original using the judge LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist.models import StyleJudgeComparativeConfig, StyleJudgmentComparative\n",
    "# Step 3: Comparative blind judging (with crash resume)\n",
    "print(\"=== Step 3: Comparative Blind Judging ===\\n\")\n",
    "\n",
    "for sample_id in store.list_samples():\n",
    "    sample = store.get_sample(sample_id)\n",
    "    print(f\"\\n{sample_id}:\")\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        if store.has_judgment(sample_id, run):\n",
    "            print(f\"  Run {run}: ✓ Already judged (skipping)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Run {run}: Judging...\", end=\" \")\n",
    "        \n",
    "        # Get all 4 reconstructions\n",
    "        reconstructions = store.get_reconstructions(sample_id, run)\n",
    "        if len(reconstructions) != 4:\n",
    "            print(f\"✗ Missing reconstructions (found {len(reconstructions)}/4)\")\n",
    "            continue\n",
    "        \n",
    "        # Create randomized mapping for blind evaluation\n",
    "        # Using deterministic seed per (sample, run) for reproducibility\n",
    "        mapping = store.create_random_mapping(seed=hash(f\"{sample_id}_{run}\"))\n",
    "        \n",
    "        # Build prompt with anonymous labels (BLIND EVALUATION)\n",
    "        judge_config = StyleJudgeComparativeConfig(\n",
    "            original_text=sample['original_text'],\n",
    "            reconstruction_text_a=reconstructions[mapping.text_a],\n",
    "            reconstruction_text_b=reconstructions[mapping.text_b],\n",
    "            reconstruction_text_c=reconstructions[mapping.text_c],\n",
    "            reconstruction_text_d=reconstructions[mapping.text_d]\n",
    "        )\n",
    "        judge_prompt = prompt_maker.render(judge_config)\n",
    "        \n",
    "        # Get structured JSON judgment\n",
    "        try:\n",
    "            response = judge_llm.complete_json(judge_prompt)\n",
    "            judgment_data = json.loads(response.content)\n",
    "            judgment = StyleJudgmentComparative(**judgment_data)\n",
    "            \n",
    "            # Save judgment with mapping (crash resilient!)\n",
    "            store.save_judgment(sample_id, run, judgment, mapping, response.model)\n",
    "            print(f\"✓ (confidence: {judgment.confidence})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error: {e}\")\n",
    "\n",
    "stats = store.get_stats()\n",
    "print(f\"\\n✓ Completed {stats['n_judgments']} judgments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to DataFrame\n",
    "print(\"=== Exporting Results ===\\n\")\n",
    "\n",
    "# Export from store (resolves anonymous rankings to methods)\n",
    "df = store.to_dataframe()\n",
    "\n",
    "print(f\"Total judgments: {len(df)}\")\n",
    "print(f\"Samples: {df['sample_id'].nunique()}\")\n",
    "print(f\"Runs per sample: {df.groupby('sample_id')['run'].nunique().mean():.1f}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"\\n=== Sample Results ===\\n\")\n",
    "display_cols = ['sample_id', 'run', 'ranking_generic', 'ranking_fewshot', \n",
    "                'ranking_author', 'ranking_instructions', 'confidence']\n",
    "print(df[display_cols].head(15))\n",
    "\n",
    "# Export to CSV\n",
    "output_file = f\"style_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[3,'reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ranking distributions\n",
    "print(\"=== Ranking Distribution by Method ===\\n\")\n",
    "\n",
    "for method in ['generic', 'fewshot', 'author', 'instructions']:\n",
    "    col = f'ranking_{method}'\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    ranking_counts = df[col].value_counts().sort_index()\n",
    "    for rank in [1, 2, 3, 4]:\n",
    "        count = ranking_counts.get(rank, 0)\n",
    "        pct = (count / len(df) * 100) if len(df) > 0 else 0\n",
    "        print(f\"  Rank {rank}: {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n=== Confidence Distribution ===\\n\")\n",
    "print(df['confidence'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate method performance metrics\n",
    "print(\"=== Method Performance Metrics ===\\n\")\n",
    "\n",
    "# Calculate mean ranking for each method (lower is better: 1 = best, 4 = worst)\n",
    "mean_rankings = {}\n",
    "for method in ['generic', 'fewshot', 'author', 'instructions']:\n",
    "    col = f'ranking_{method}'\n",
    "    mean_rankings[method] = df[col].mean()\n",
    "\n",
    "# Sort by mean ranking (best first)\n",
    "sorted_methods = sorted(mean_rankings.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"Average Ranking (lower is better):\")\n",
    "for i, (method, mean_rank) in enumerate(sorted_methods, 1):\n",
    "    # Count how often this method ranked 1st\n",
    "    first_place = (df[f'ranking_{method}'] == 1).sum()\n",
    "    first_place_pct = (first_place / len(df) * 100) if len(df) > 0 else 0\n",
    "    \n",
    "    print(f\"{i}. {method:12s}: {mean_rank:.2f} (1st place: {first_place}/{len(df)} = {first_place_pct:.1f}%)\")\n",
    "\n",
    "# Win rate (percentage of times ranked 1st or 2nd)\n",
    "print(\"\\nTop-2 Rate (ranked 1st or 2nd):\")\n",
    "for method in ['generic', 'fewshot', 'author', 'instructions']:\n",
    "    col = f'ranking_{method}'\n",
    "    top2 = ((df[col] == 1) | (df[col] == 2)).sum()\n",
    "    top2_pct = (top2 / len(df) * 100) if len(df) > 0 else 0\n",
    "    print(f\"  {method:12s}: {top2}/{len(df)} = {top2_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "output_file = f\"style_evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "TODO: Add analysis cells for:\n",
    "- Statistical significance testing\n",
    "- Visualization of results\n",
    "- Sample-by-sample breakdown\n",
    "- Confidence level analysis\n",
    "- Qualitative review of reasoning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
