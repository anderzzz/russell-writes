{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Instruction Evaluation Framework\n",
    "\n",
    "This notebook evaluates how well derived style instructions enable style replication, compared to alternative approaches.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each gold standard text:\n",
    "1. **Flatten**: Extract content while removing style\n",
    "2. **Reconstruct**: Generate text using 4 different methods:\n",
    "   - Generic baseline\n",
    "   - Few-shot learning\n",
    "   - Author name prompting\n",
    "   - Derived style instructions\n",
    "3. **Judge**: Compare each reconstruction against the original\n",
    "4. **Aggregate**: Analyze results across multiple texts and runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from belletrist import (\n",
    "    LLM, LLMConfig, PromptMaker, DataSampler,\n",
    "    StyleFlatteningConfig,\n",
    "    StyleReconstructionGenericConfig,\n",
    "    StyleReconstructionFewShotConfig,\n",
    "    StyleReconstructionAuthorConfig,\n",
    "    StyleReconstructionInstructionsConfig,\n",
    "    StyleJudgeConfig,\n",
    "    StyleJudgment\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Parameters\n",
    "\n",
    "Configure the evaluation settings here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation parameters\n",
    "N_SAMPLES = 5              # Number of gold standard texts to test\n",
    "M_RUNS = 3                 # Stochastic runs per sample\n",
    "AUTHOR_NAME = \"Bertrand Russell\"\n",
    "N_FEWSHOT_EXAMPLES = 3\n",
    "\n",
    "# Model configuration\n",
    "# TODO: Adjust models and API keys for your setup\n",
    "RECONSTRUCTION_MODEL = \"mistral/mistral-large-2411\"  # For generating reconstructions\n",
    "JUDGE_MODEL = \"openai/gpt-4o\"                        # Best model for judging\n",
    "RECONSTRUCTION_TEMPERATURE = 0.8                     # Higher for variety across runs\n",
    "JUDGE_TEMPERATURE = 0.3                              # Lower for consistent judgments\n",
    "\n",
    "# API keys (set in environment)\n",
    "RECONSTRUCTION_API_KEY = os.environ.get('MISTRAL_API_KEY')\n",
    "JUDGE_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "\n",
    "# Sample selection (TODO: Specify text indices manually)\n",
    "# Split your corpus into:\n",
    "# - INSTRUCTION_SAMPLES: Used to derive style instructions (already done)\n",
    "# - TEST_SAMPLES: Used for compression and reconstruction (this evaluation)\n",
    "# - FEWSHOT_SAMPLES: Used as few-shot examples (different from test samples)\n",
    "TEST_SAMPLE_INDICES = [10, 15, 20, 25, 30]  # Example indices\n",
    "FEWSHOT_SAMPLE_INDICES = [5, 12, 18]        # Example indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize components\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler()\n",
    "\n",
    "# Initialize LLMs\n",
    "reconstruction_llm = LLM(LLMConfig(\n",
    "    model=RECONSTRUCTION_MODEL,\n",
    "    api_key=RECONSTRUCTION_API_KEY,\n",
    "    temperature=RECONSTRUCTION_TEMPERATURE,\n",
    "    max_tokens=2000\n",
    "))\n",
    "\n",
    "judge_llm = LLM(LLMConfig(\n",
    "    model=JUDGE_MODEL,\n",
    "    api_key=JUDGE_API_KEY,\n",
    "    temperature=JUDGE_TEMPERATURE\n",
    "))\n",
    "\n",
    "print(\"✓ LLMs initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load derived style instructions\n",
    "# This should be the output from SynthesizerOfPrinciplesConfig\n",
    "style_instructions_path = Path(\"derived_style_instructions.txt\")\n",
    "\n",
    "if style_instructions_path.exists():\n",
    "    style_instructions = style_instructions_path.read_text()\n",
    "    print(f\"✓ Loaded style instructions ({len(style_instructions)} chars)\")\n",
    "    print(f\"\\nFirst 200 chars:\\n{style_instructions[:200]}...\")\n",
    "else:\n",
    "    print(\"⚠ Style instructions file not found. Please provide path.\")\n",
    "    style_instructions = \"\"  # Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test samples and few-shot examples\n",
    "# TODO: Implement sample selection based on your corpus structure\n",
    "\n",
    "test_samples = []\n",
    "for idx in TEST_SAMPLE_INDICES[:N_SAMPLES]:\n",
    "    segment = sampler.get_paragraph_chunk(file_index=0, start_para=idx*10, length=5)\n",
    "    test_samples.append({\n",
    "        'id': f'test_{idx}',\n",
    "        'text': segment.text,\n",
    "        'source': f'File {segment.file_index}, para {segment.paragraph_start}-{segment.paragraph_end}'\n",
    "    })\n",
    "\n",
    "fewshot_examples = []\n",
    "for idx in FEWSHOT_SAMPLE_INDICES[:N_FEWSHOT_EXAMPLES]:\n",
    "    segment = sampler.get_paragraph_chunk(file_index=0, start_para=idx*10, length=5)\n",
    "    fewshot_examples.append(segment.text)\n",
    "\n",
    "print(f\"✓ Loaded {len(test_samples)} test samples\")\n",
    "print(f\"✓ Loaded {len(fewshot_examples)} few-shot examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Content Flattening\n",
    "\n",
    "Extract content from each test sample, removing stylistic elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all test samples\n",
    "for sample in test_samples:\n",
    "    print(f\"Flattening {sample['id']}...\", end=\" \")\n",
    "    \n",
    "    flattening_config = StyleFlatteningConfig(text=sample['text'])\n",
    "    flattening_prompt = prompt_maker.render(flattening_config)\n",
    "    \n",
    "    response = reconstruction_llm.complete(flattening_prompt)\n",
    "    sample['content_summary'] = response.content\n",
    "    \n",
    "    print(f\"✓ ({len(response.content)} chars)\")\n",
    "\n",
    "print(f\"\\n✓ All samples flattened\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reconstruction\n",
    "\n",
    "Generate reconstructions using all 4 methods, with M stochastic runs each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store reconstructions\n",
    "reconstructions = []\n",
    "\n",
    "for sample in test_samples:\n",
    "    print(f\"\\nProcessing {sample['id']}:\")\n",
    "    \n",
    "    for run in range(M_RUNS):\n",
    "        print(f\"  Run {run+1}/{M_RUNS}:\")\n",
    "        \n",
    "        content_summary = sample['content_summary']\n",
    "        \n",
    "        # Method 1: Generic baseline\n",
    "        print(\"    - Generic...\", end=\" \")\n",
    "        config = StyleReconstructionGenericConfig(content_summary=content_summary)\n",
    "        prompt = prompt_maker.render(config)\n",
    "        response = reconstruction_llm.complete(prompt)\n",
    "        reconstructions.append({\n",
    "            'sample_id': sample['id'],\n",
    "            'run': run,\n",
    "            'method': 'generic',\n",
    "            'text': response.content\n",
    "        })\n",
    "        print(\"✓\")\n",
    "        \n",
    "        # Method 2: Few-shot\n",
    "        print(\"    - Few-shot...\", end=\" \")\n",
    "        config = StyleReconstructionFewShotConfig(\n",
    "            content_summary=content_summary,\n",
    "            few_shot_examples=fewshot_examples\n",
    "        )\n",
    "        prompt = prompt_maker.render(config)\n",
    "        response = reconstruction_llm.complete(prompt)\n",
    "        reconstructions.append({\n",
    "            'sample_id': sample['id'],\n",
    "            'run': run,\n",
    "            'method': 'fewshot',\n",
    "            'text': response.content\n",
    "        })\n",
    "        print(\"✓\")\n",
    "        \n",
    "        # Method 3: Author name\n",
    "        print(\"    - Author name...\", end=\" \")\n",
    "        config = StyleReconstructionAuthorConfig(\n",
    "            content_summary=content_summary,\n",
    "            author_name=AUTHOR_NAME\n",
    "        )\n",
    "        prompt = prompt_maker.render(config)\n",
    "        response = reconstruction_llm.complete(prompt)\n",
    "        reconstructions.append({\n",
    "            'sample_id': sample['id'],\n",
    "            'run': run,\n",
    "            'method': 'author',\n",
    "            'text': response.content\n",
    "        })\n",
    "        print(\"✓\")\n",
    "        \n",
    "        # Method 4: Derived instructions\n",
    "        print(\"    - Instructions...\", end=\" \")\n",
    "        config = StyleReconstructionInstructionsConfig(\n",
    "            content_summary=content_summary,\n",
    "            style_instructions=style_instructions\n",
    "        )\n",
    "        prompt = prompt_maker.render(config)\n",
    "        response = reconstruction_llm.complete(prompt)\n",
    "        reconstructions.append({\n",
    "            'sample_id': sample['id'],\n",
    "            'run': run,\n",
    "            'method': 'instructions',\n",
    "            'text': response.content\n",
    "        })\n",
    "        print(\"✓\")\n",
    "\n",
    "print(f\"\\n✓ Generated {len(reconstructions)} total reconstructions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Judging\n",
    "\n",
    "Compare each reconstruction against the original using the judge LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store judgments\n",
    "judgments = []\n",
    "\n",
    "for reconstruction in reconstructions:\n",
    "    # Find corresponding original sample\n",
    "    sample = next(s for s in test_samples if s['id'] == reconstruction['sample_id'])\n",
    "    \n",
    "    print(f\"Judging {reconstruction['sample_id']}, run {reconstruction['run']}, method {reconstruction['method']}...\", end=\" \")\n",
    "    \n",
    "    # Build judge prompt\n",
    "    judge_config = StyleJudgeConfig(\n",
    "        original_text=sample['text'],\n",
    "        reconstruction_text=reconstruction['text']\n",
    "    )\n",
    "    judge_prompt = prompt_maker.render(judge_config)\n",
    "    \n",
    "    # Get structured JSON judgment\n",
    "    response = judge_llm.complete_json(judge_prompt)\n",
    "    \n",
    "    try:\n",
    "        # Parse and validate JSON\n",
    "        judgment_data = json.loads(response.content)\n",
    "        judgment = StyleJudgment(**judgment_data)\n",
    "        \n",
    "        # Store judgment\n",
    "        judgments.append({\n",
    "            'sample_id': reconstruction['sample_id'],\n",
    "            'run': reconstruction['run'],\n",
    "            'method': reconstruction['method'],\n",
    "            'ranking': judgment.ranking,\n",
    "            'confidence': judgment.confidence,\n",
    "            'reasoning': judgment.reasoning,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "        print(f\"✓ {judgment.ranking}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error: {e}\")\n",
    "        judgments.append({\n",
    "            'sample_id': reconstruction['sample_id'],\n",
    "            'run': reconstruction['run'],\n",
    "            'method': reconstruction['method'],\n",
    "            'ranking': 'error',\n",
    "            'confidence': 'error',\n",
    "            'reasoning': str(e),\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "\n",
    "print(f\"\\n✓ Completed {len(judgments)} judgments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = pd.DataFrame(judgments)\n",
    "\n",
    "# Remove error rows\n",
    "df_clean = df[df['ranking'] != 'error']\n",
    "\n",
    "print(f\"Total judgments: {len(df)}\")\n",
    "print(f\"Valid judgments: {len(df_clean)}\")\n",
    "print(f\"Errors: {len(df) - len(df_clean)}\")\n",
    "\n",
    "# Show summary\n",
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results by method\n",
    "print(\"\\n=== Results by Method ===\")\n",
    "print(\"\\nRanking distribution:\")\n",
    "print(df_clean.groupby(['method', 'ranking']).size().unstack(fill_value=0))\n",
    "\n",
    "print(\"\\nConfidence distribution:\")\n",
    "print(df_clean.groupby(['method', 'confidence']).size().unstack(fill_value=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate success rate (reconstruction_better)\n",
    "success_rates = df_clean[df_clean['ranking'] == 'reconstruction_better'].groupby('method').size() / df_clean.groupby('method').size()\n",
    "\n",
    "print(\"\\n=== Success Rate by Method ===\")\n",
    "print(\"(Percentage where reconstruction was judged better than original)\\n\")\n",
    "print(success_rates.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "output_file = f\"style_evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "TODO: Add analysis cells for:\n",
    "- Statistical significance testing\n",
    "- Visualization of results\n",
    "- Sample-by-sample breakdown\n",
    "- Confidence level analysis\n",
    "- Qualitative review of reasoning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
