{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Style Instruction Evaluation Framework\n\nThis notebook evaluates how well derived style instructions enable style replication, compared to alternative approaches.\n\n## Methodology\n\nFor each gold standard text:\n1. **Flatten**: Extract content while removing style\n2. **Reconstruct**: Generate text using 4 different methods (M stochastic runs each):\n   - Generic baseline\n   - Few-shot learning\n   - Author name prompting\n   - Derived style instructions\n3. **Judge (Blind Comparative)**: Judge ranks all 4 reconstructions from 1-4 based on similarity to original\n   - **Blind evaluation**: Judge sees only anonymous labels (Text A, B, C, D) - no method names\n   - **Ranking**: 1 = most similar, 2 = second, 3 = third, 4 = least similar\n   - **Order randomized**: Position of methods varies across samples to eliminate bias\n4. **Aggregate**: Analyze rankings to determine which method best captures style\n\n## Key Features\n\n- **Crash resilient**: All LLM responses saved to SQLite immediately\n- **Resume support**: Can restart after failures, skips completed work\n- **Blind evaluation**: Eliminates judge bias by hiding method names\n- **Comparative ranking**: More informative than binary comparisons"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Base Objects\n",
    "The base objects part of the current project library (`belletrist`) are initialized. They are:\n",
    "* `LLM`: the LLM object.\n",
    "* `LLMConfig`: the configuration of the LLM object, such as what model to use.\n",
    "* `PromptMaker`: generates prompts from templates and variables\n",
    "* `DataSampler`: retrieves and samples text at a source directory\n",
    "\n",
    "These will implement text transformations by LLMs part of the evaluation process. They build on the third-party LLMs, which we furthermore split into LLMs for text reconstruction and text judging, the key parameters for which are set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reconstruction_string = 'together_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput'\n",
    "model_reconstruction_api_key_env_var = 'TOGETHER_AI_API_KEY'\n",
    "model_judge_string = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "model_judge_api_key_env_var = 'ANTHROPIC_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist import PromptMaker, DataSampler\n",
    "\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler(\n",
    "    data_path=(Path(os.getcwd()) / \"data\" / \"russell\").resolve()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist import LLM, LLMConfig\n",
    "\n",
    "reconstruction_llm = LLM(LLMConfig(\n",
    "    model=model_reconstruction_string,\n",
    "    api_key=os.environ.get(model_reconstruction_api_key_env_var)\n",
    "))\n",
    "judge_llm = LLM(LLMConfig(\n",
    "    model=model_judge_string,\n",
    "    api_key=os.environ.get(model_judge_api_key_env_var)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Test Data and Few-Shot Data\n",
    "The reconstruction method tests build on gold standard texts. The test also includes few-shot prompting with the gold standard texts. In order to not skew the tests, no few-shot examples can overlap with the test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "n_sample = 10\nm_paragraphs_per_sample = 5\nn_few_shot_sample = 3\n\ntest_texts = []\nfor _ in range(n_sample):\n    test_texts.append(sampler.sample_segment(p_length=m_paragraphs_per_sample))\n\nfew_shot_texts = []\nwhile len(few_shot_texts) < n_few_shot_sample:\n    p = sampler.sample_segment(p_length=m_paragraphs_per_sample)\n\n    # Check if p overlaps with any test text\n    # Two segments overlap if they're from the same file AND their paragraph ranges overlap\n    # Ranges [a, b) and [c, d) overlap if: a < d AND c < b\n    has_overlap = any(\n        p.file_index == test_seg.file_index and\n        p.paragraph_start < test_seg.paragraph_end and\n        test_seg.paragraph_start < p.paragraph_end\n        for test_seg in test_texts\n    )\n\n    if not has_overlap:\n        few_shot_texts.append(p)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Test Transformation Objects\n",
    "The combination of prompt and LLM leads to the following operators in the test chain:\n",
    "* **Style Flattener**, which given a text compresses it into its content bare bones.\n",
    "* **Reconstructor, LLM House Style**, which given a compressed content expands it into a complete text with the \"house style\" of the LLM employed for the reconstruction.\n",
    "* **Reconstructor, Few Shot**, which given a compressed content expands it into a complete text with a few text excerpts on unrelated topics as style guide.\n",
    "* **Reconstructor, LLM Author Model**, which given a compressed content expands it into a complete text with the named author's style as the LLM conceives it without any other guidance.\n",
    "* **Reconstructor, Style Instruction**, which given a compressed content expands it into a complete text following the detailed style instruction, as derived from previous analysis."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "style_instructions_path = Path(\"outputs/derived_style_instructions_1125_000.txt\")\n",
    "\n",
    "if style_instructions_path.exists():\n",
    "    style_instructions = style_instructions_path.read_text()\n",
    "    print(f\"✓ Loaded style instructions ({len(style_instructions)} chars)\")\n",
    "    print(f\"\\nFirst 200 chars:\\n{style_instructions[:200]}...\")\n",
    "else:\n",
    "    print(\"⚠ Style instructions file not found. Please provide path.\")\n",
    "    style_instructions = \"\"  # Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import store and comparative judging models\nfrom belletrist import StyleEvaluationStore\nfrom belletrist.models import (\n    StyleFlatteningConfig,\n    StyleReconstructionGenericConfig,\n    StyleReconstructionFewShotConfig,\n    StyleReconstructionAuthorConfig,\n    StyleReconstructionInstructionsConfig,\n    StyleJudgeComparativeConfig,\n    MethodMapping,\n    StyleJudgmentComparative\n)\n\n# Initialize crash-resilient store\nstore = StyleEvaluationStore(Path(\"style_evaluation_results.db\"))\n\n# Configuration\nn_runs = 3\nAUTHOR_NAME = \"Bertrand Russell\"\n\n# Reconstructor configs\nRECONSTRUCTORS_CFGS = {\n    'generic': StyleReconstructionGenericConfig,\n    'fewshot': StyleReconstructionFewShotConfig,\n    'author': StyleReconstructionAuthorConfig,\n    'instructions': StyleReconstructionInstructionsConfig\n}\n\n# Reconstructor kwargs\nRECONSTRUCTORS_KWARGS = {\n    'generic': {},\n    'fewshot': {'few_shot_examples': [seg.text for seg in few_shot_texts]},\n    'author': {'author_name': AUTHOR_NAME},\n    'instructions': {'style_instructions': style_instructions}\n}\n\nprint(f\"✓ Store initialized at {store.filepath}\")\nprint(f\"✓ Configuration: {n_runs} runs per sample, 4 reconstruction methods\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Content Flattening\n",
    "\n",
    "Extract content from each test sample, removing stylistic elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Save samples and flatten content\nprint(\"=== Step 1: Flattening and Saving Samples ===\\n\")\n\nfor k_text, test_sample in enumerate(test_texts):\n    sample_id = f\"sample_{k_text:03d}\"\n    \n    # Skip if already saved\n    if store.get_sample(sample_id):\n        print(f\"✓ {sample_id} already flattened (skipping)\")\n        continue\n    \n    print(f\"Flattening {sample_id}...\", end=\" \")\n    \n    # Flatten content\n    flatten_prompt = prompt_maker.render(\n        StyleFlatteningConfig(text=test_sample.text)\n    )\n    flattened = reconstruction_llm.complete(flatten_prompt)\n    \n    # Save to store with provenance\n    source_info = f\"File {test_sample.file_index}, para {test_sample.paragraph_start}-{test_sample.paragraph_end}\"\n    store.save_sample(\n        sample_id=sample_id,\n        original_text=test_sample.text,\n        flattened_content=flattened.content,\n        source_info=source_info\n    )\n    \n    print(f\"✓ ({len(flattened.content)} chars)\")\n\nprint(f\"\\n✓ All samples flattened and saved to store\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reconstruction\n",
    "\n",
    "Generate reconstructions using all 4 methods, with M stochastic runs each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Generate reconstructions (with crash resume)\nprint(\"=== Step 2: Generating Reconstructions ===\\n\")\n\nfor sample_id in store.list_samples():\n    sample = store.get_sample(sample_id)\n    print(f\"\\n{sample_id}:\")\n    \n    for run in range(n_runs):\n        print(f\"  Run {run}:\")\n        \n        # Check which methods need reconstruction\n        for method in ['generic', 'fewshot', 'author', 'instructions']:\n            if store.has_reconstruction(sample_id, run, method):\n                print(f\"    ✓ {method:12s} (already done)\")\n                continue\n            \n            # Generate reconstruction\n            config = RECONSTRUCTORS_CFGS[method](\n                content_summary=sample['flattened_content'],\n                **RECONSTRUCTORS_KWARGS[method]\n            )\n            prompt = prompt_maker.render(config)\n            response = reconstruction_llm.complete(prompt)\n            \n            # Save immediately (crash resilient!)\n            store.save_reconstruction(\n                sample_id=sample_id,\n                run=run,\n                method=method,\n                reconstructed_text=response.content,\n                model=response.model\n            )\n            print(f\"    ✓ {method:12s} ({len(response.content)} chars)\")\n\nstats = store.get_stats()\nprint(f\"\\n✓ Generated {stats['n_reconstructions']} total reconstructions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Judging\n",
    "\n",
    "Compare each reconstruction against the original using the judge LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: Comparative blind judging (with crash resume)\nprint(\"=== Step 3: Comparative Blind Judging ===\\n\")\n\nfor sample_id in store.list_samples():\n    sample = store.get_sample(sample_id)\n    print(f\"\\n{sample_id}:\")\n    \n    for run in range(n_runs):\n        if store.has_judgment(sample_id, run):\n            print(f\"  Run {run}: ✓ Already judged (skipping)\")\n            continue\n        \n        print(f\"  Run {run}: Judging...\", end=\" \")\n        \n        # Get all 4 reconstructions\n        reconstructions = store.get_reconstructions(sample_id, run)\n        if len(reconstructions) != 4:\n            print(f\"✗ Missing reconstructions (found {len(reconstructions)}/4)\")\n            continue\n        \n        # Create randomized mapping for blind evaluation\n        # Using deterministic seed per (sample, run) for reproducibility\n        mapping = store.create_random_mapping(seed=hash(f\"{sample_id}_{run}\"))\n        \n        # Build prompt with anonymous labels (BLIND EVALUATION)\n        judge_config = StyleJudgeComparativeConfig(\n            original_text=sample['original_text'],\n            reconstruction_text_a=reconstructions[mapping.text_a],\n            reconstruction_text_b=reconstructions[mapping.text_b],\n            reconstruction_text_c=reconstructions[mapping.text_c],\n            reconstruction_text_d=reconstructions[mapping.text_d]\n        )\n        judge_prompt = prompt_maker.render(judge_config)\n        \n        # Get structured JSON judgment\n        try:\n            response = judge_llm.complete_json(judge_prompt)\n            judgment_data = json.loads(response.content)\n            judgment = StyleJudgmentComparative(**judgment_data)\n            \n            # Save judgment with mapping (crash resilient!)\n            store.save_judgment(sample_id, run, judgment, mapping, response.model)\n            print(f\"✓ (confidence: {judgment.confidence})\")\n            \n        except Exception as e:\n            print(f\"✗ Error: {e}\")\n\nstats = store.get_stats()\nprint(f\"\\n✓ Completed {stats['n_judgments']} judgments\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Export results to DataFrame\nprint(\"=== Exporting Results ===\\n\")\n\n# Export from store (resolves anonymous rankings to methods)\ndf = store.to_dataframe()\n\nprint(f\"Total judgments: {len(df)}\")\nprint(f\"Samples: {df['sample_id'].nunique()}\")\nprint(f\"Runs per sample: {df.groupby('sample_id')['run'].nunique().mean():.1f}\")\n\n# Show first few rows\nprint(f\"\\n=== Sample Results ===\\n\")\ndisplay_cols = ['sample_id', 'run', 'ranking_generic', 'ranking_fewshot', \n                'ranking_author', 'ranking_instructions', 'confidence']\nprint(df[display_cols].head(10))\n\n# Export to CSV\noutput_file = f\"style_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\ndf.to_csv(output_file, index=False)\nprint(f\"\\n✓ Results saved to {output_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze ranking distributions\nprint(\"=== Ranking Distribution by Method ===\\n\")\n\nfor method in ['generic', 'fewshot', 'author', 'instructions']:\n    col = f'ranking_{method}'\n    print(f\"\\n{method.upper()}:\")\n    ranking_counts = df[col].value_counts().sort_index()\n    for rank in [1, 2, 3, 4]:\n        count = ranking_counts.get(rank, 0)\n        pct = (count / len(df) * 100) if len(df) > 0 else 0\n        print(f\"  Rank {rank}: {count:3d} ({pct:5.1f}%)\")\n\nprint(\"\\n=== Confidence Distribution ===\\n\")\nprint(df['confidence'].value_counts())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate method performance metrics\nprint(\"=== Method Performance Metrics ===\\n\")\n\n# Calculate mean ranking for each method (lower is better: 1 = best, 4 = worst)\nmean_rankings = {}\nfor method in ['generic', 'fewshot', 'author', 'instructions']:\n    col = f'ranking_{method}'\n    mean_rankings[method] = df[col].mean()\n\n# Sort by mean ranking (best first)\nsorted_methods = sorted(mean_rankings.items(), key=lambda x: x[1])\n\nprint(\"Average Ranking (lower is better):\")\nfor i, (method, mean_rank) in enumerate(sorted_methods, 1):\n    # Count how often this method ranked 1st\n    first_place = (df[f'ranking_{method}'] == 1).sum()\n    first_place_pct = (first_place / len(df) * 100) if len(df) > 0 else 0\n    \n    print(f\"{i}. {method:12s}: {mean_rank:.2f} (1st place: {first_place}/{len(df)} = {first_place_pct:.1f}%)\")\n\n# Win rate (percentage of times ranked 1st or 2nd)\nprint(\"\\nTop-2 Rate (ranked 1st or 2nd):\")\nfor method in ['generic', 'fewshot', 'author', 'instructions']:\n    col = f'ranking_{method}'\n    top2 = ((df[col] == 1) | (df[col] == 2)).sum()\n    top2_pct = (top2 / len(df) * 100) if len(df) > 0 else 0\n    print(f\"  {method:12s}: {top2}/{len(df)} = {top2_pct:.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "output_file = f\"style_evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "TODO: Add analysis cells for:\n",
    "- Statistical significance testing\n",
    "- Visualization of results\n",
    "- Sample-by-sample breakdown\n",
    "- Confidence level analysis\n",
    "- Qualitative review of reasoning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}